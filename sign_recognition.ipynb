{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"./model/kp_classifier.csv\"\n",
    "model_save_path = \"./model/kp_classifier.hdf5\"\n",
    "tflite_save_path = \"./model/kp_classifier.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt(dataset, delimiter=\",\", dtype='float32', skiprows=1, usecols=list(range(1, (21 * 3) + 1)))\n",
    "y = np.loadtxt(dataset, delimiter=\",\", dtype='int32', skiprows=1, usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout (Dropout)           (None, 63)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                1280      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 36)                396       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1886 (7.37 KB)\n",
      "Trainable params: 1886 (7.37 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 3, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False, save_best_only=True, monitor='val_accuracy', mode='max'\n",
    ")\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 1/12 [=>............................] - ETA: 12s - loss: 3.5793 - accuracy: 0.0312\n",
      "Epoch 1: val_accuracy improved from -inf to 0.02395, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 1s 31ms/step - loss: 3.5743 - accuracy: 0.0340 - val_loss: 3.5491 - val_accuracy: 0.0240\n",
      "Epoch 2/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.5683 - accuracy: 0.0312\n",
      "Epoch 2: val_accuracy improved from 0.02395 to 0.04990, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.5531 - accuracy: 0.0380 - val_loss: 3.5268 - val_accuracy: 0.0499\n",
      "Epoch 3/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.5583 - accuracy: 0.0391\n",
      "Epoch 3: val_accuracy improved from 0.04990 to 0.08184, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.5340 - accuracy: 0.0520 - val_loss: 3.5040 - val_accuracy: 0.0818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.5132 - accuracy: 0.0547\n",
      "Epoch 4: val_accuracy improved from 0.08184 to 0.09581, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.5112 - accuracy: 0.0553 - val_loss: 3.4784 - val_accuracy: 0.0958\n",
      "Epoch 5/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.5059 - accuracy: 0.0469\n",
      "Epoch 5: val_accuracy improved from 0.09581 to 0.12575, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.4950 - accuracy: 0.0620 - val_loss: 3.4481 - val_accuracy: 0.1257\n",
      "Epoch 6/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.4683 - accuracy: 0.0234\n",
      "Epoch 6: val_accuracy did not improve from 0.12575\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3.4577 - accuracy: 0.0713 - val_loss: 3.4110 - val_accuracy: 0.1257\n",
      "Epoch 7/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.4985 - accuracy: 0.0469\n",
      "Epoch 7: val_accuracy improved from 0.12575 to 0.14571, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.4255 - accuracy: 0.0713 - val_loss: 3.3651 - val_accuracy: 0.1457\n",
      "Epoch 8/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.3916 - accuracy: 0.0625\n",
      "Epoch 8: val_accuracy improved from 0.14571 to 0.15170, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.4001 - accuracy: 0.0806 - val_loss: 3.3111 - val_accuracy: 0.1517\n",
      "Epoch 9/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.4116 - accuracy: 0.0703\n",
      "Epoch 9: val_accuracy improved from 0.15170 to 0.17964, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.3529 - accuracy: 0.0946 - val_loss: 3.2520 - val_accuracy: 0.1796\n",
      "Epoch 10/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.3046 - accuracy: 0.0938\n",
      "Epoch 10: val_accuracy improved from 0.17964 to 0.20758, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.3019 - accuracy: 0.1053 - val_loss: 3.1862 - val_accuracy: 0.2076\n",
      "Epoch 11/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.2210 - accuracy: 0.1094\n",
      "Epoch 11: val_accuracy improved from 0.20758 to 0.23952, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.2367 - accuracy: 0.1093 - val_loss: 3.1106 - val_accuracy: 0.2395\n",
      "Epoch 12/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.2162 - accuracy: 0.0781\n",
      "Epoch 12: val_accuracy improved from 0.23952 to 0.25749, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.2014 - accuracy: 0.1139 - val_loss: 3.0300 - val_accuracy: 0.2575\n",
      "Epoch 13/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.1245 - accuracy: 0.1484\n",
      "Epoch 13: val_accuracy improved from 0.25749 to 0.28942, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.1375 - accuracy: 0.1319 - val_loss: 2.9445 - val_accuracy: 0.2894\n",
      "Epoch 14/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.0641 - accuracy: 0.1172\n",
      "Epoch 14: val_accuracy improved from 0.28942 to 0.32934, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.0613 - accuracy: 0.1312 - val_loss: 2.8578 - val_accuracy: 0.3293\n",
      "Epoch 15/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.0737 - accuracy: 0.1484\n",
      "Epoch 15: val_accuracy improved from 0.32934 to 0.35928, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.0135 - accuracy: 0.1392 - val_loss: 2.7791 - val_accuracy: 0.3593\n",
      "Epoch 16/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.9185 - accuracy: 0.1719\n",
      "Epoch 16: val_accuracy improved from 0.35928 to 0.36926, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2.9451 - accuracy: 0.1566 - val_loss: 2.7011 - val_accuracy: 0.3693\n",
      "Epoch 17/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.9138 - accuracy: 0.1484\n",
      "Epoch 17: val_accuracy did not improve from 0.36926\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.8785 - accuracy: 0.1672 - val_loss: 2.6243 - val_accuracy: 0.3493\n",
      "Epoch 18/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.8146 - accuracy: 0.1875\n",
      "Epoch 18: val_accuracy improved from 0.36926 to 0.40120, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.8078 - accuracy: 0.1925 - val_loss: 2.5438 - val_accuracy: 0.4012\n",
      "Epoch 19/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.7517 - accuracy: 0.2188\n",
      "Epoch 19: val_accuracy improved from 0.40120 to 0.41916, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.7466 - accuracy: 0.1965 - val_loss: 2.4678 - val_accuracy: 0.4192\n",
      "Epoch 20/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.7642 - accuracy: 0.1953\n",
      "Epoch 20: val_accuracy improved from 0.41916 to 0.45709, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.7067 - accuracy: 0.2039 - val_loss: 2.3917 - val_accuracy: 0.4571\n",
      "Epoch 21/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.6001 - accuracy: 0.2266\n",
      "Epoch 21: val_accuracy improved from 0.45709 to 0.50100, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.6340 - accuracy: 0.2225 - val_loss: 2.3230 - val_accuracy: 0.5010\n",
      "Epoch 22/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.6187 - accuracy: 0.3047\n",
      "Epoch 22: val_accuracy did not improve from 0.50100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.5975 - accuracy: 0.2352 - val_loss: 2.2548 - val_accuracy: 0.4830\n",
      "Epoch 23/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.5186 - accuracy: 0.2422\n",
      "Epoch 23: val_accuracy improved from 0.50100 to 0.51497, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.5437 - accuracy: 0.2478 - val_loss: 2.2000 - val_accuracy: 0.5150\n",
      "Epoch 24/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.6082 - accuracy: 0.2109\n",
      "Epoch 24: val_accuracy improved from 0.51497 to 0.51697, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.5123 - accuracy: 0.2338 - val_loss: 2.1421 - val_accuracy: 0.5170\n",
      "Epoch 25/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.4789 - accuracy: 0.2734\n",
      "Epoch 25: val_accuracy improved from 0.51697 to 0.54291, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.4482 - accuracy: 0.2885 - val_loss: 2.0819 - val_accuracy: 0.5429\n",
      "Epoch 26/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.4395 - accuracy: 0.2891\n",
      "Epoch 26: val_accuracy improved from 0.54291 to 0.55888, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.4531 - accuracy: 0.2672 - val_loss: 2.0245 - val_accuracy: 0.5589\n",
      "Epoch 27/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.4371 - accuracy: 0.2578\n",
      "Epoch 27: val_accuracy improved from 0.55888 to 0.57884, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 2.3751 - accuracy: 0.2838 - val_loss: 1.9686 - val_accuracy: 0.5788\n",
      "Epoch 28/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.4357 - accuracy: 0.3047\n",
      "Epoch 28: val_accuracy improved from 0.57884 to 0.58683, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.3477 - accuracy: 0.3038 - val_loss: 1.9160 - val_accuracy: 0.5868\n",
      "Epoch 29/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.3635 - accuracy: 0.2891\n",
      "Epoch 29: val_accuracy did not improve from 0.58683\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.2992 - accuracy: 0.2898 - val_loss: 1.8726 - val_accuracy: 0.5868\n",
      "Epoch 30/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.4102 - accuracy: 0.3203\n",
      "Epoch 30: val_accuracy improved from 0.58683 to 0.61677, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.2676 - accuracy: 0.2965 - val_loss: 1.8298 - val_accuracy: 0.6168\n",
      "Epoch 31/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1807 - accuracy: 0.2734\n",
      "Epoch 31: val_accuracy did not improve from 0.61677\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.2221 - accuracy: 0.3045 - val_loss: 1.7855 - val_accuracy: 0.5908\n",
      "Epoch 32/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1979 - accuracy: 0.2812\n",
      "Epoch 32: val_accuracy did not improve from 0.61677\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.2100 - accuracy: 0.3178 - val_loss: 1.7477 - val_accuracy: 0.6008\n",
      "Epoch 33/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1297 - accuracy: 0.2891\n",
      "Epoch 33: val_accuracy did not improve from 0.61677\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.2031 - accuracy: 0.2951 - val_loss: 1.7162 - val_accuracy: 0.5968\n",
      "Epoch 34/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0127 - accuracy: 0.3672\n",
      "Epoch 34: val_accuracy improved from 0.61677 to 0.64072, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2.1366 - accuracy: 0.3291 - val_loss: 1.6804 - val_accuracy: 0.6407\n",
      "Epoch 35/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1453 - accuracy: 0.3359\n",
      "Epoch 35: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.1262 - accuracy: 0.3245 - val_loss: 1.6445 - val_accuracy: 0.6367\n",
      "Epoch 36/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0040 - accuracy: 0.3516\n",
      "Epoch 36: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.0719 - accuracy: 0.3284 - val_loss: 1.6117 - val_accuracy: 0.6387\n",
      "Epoch 37/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1442 - accuracy: 0.2969\n",
      "Epoch 37: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.0702 - accuracy: 0.3598 - val_loss: 1.5824 - val_accuracy: 0.6287\n",
      "Epoch 38/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0560 - accuracy: 0.3438\n",
      "Epoch 38: val_accuracy improved from 0.64072 to 0.64671, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.0846 - accuracy: 0.3264 - val_loss: 1.5551 - val_accuracy: 0.6467\n",
      "Epoch 39/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9025 - accuracy: 0.4141\n",
      "Epoch 39: val_accuracy improved from 0.64671 to 0.65868, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2.0450 - accuracy: 0.3458 - val_loss: 1.5330 - val_accuracy: 0.6587\n",
      "Epoch 40/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0433 - accuracy: 0.3125\n",
      "Epoch 40: val_accuracy improved from 0.65868 to 0.66267, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2.0134 - accuracy: 0.3544 - val_loss: 1.5167 - val_accuracy: 0.6627\n",
      "Epoch 41/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9483 - accuracy: 0.3594\n",
      "Epoch 41: val_accuracy did not improve from 0.66267\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9998 - accuracy: 0.3598 - val_loss: 1.4876 - val_accuracy: 0.6447\n",
      "Epoch 42/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8690 - accuracy: 0.4141\n",
      "Epoch 42: val_accuracy did not improve from 0.66267\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9839 - accuracy: 0.3511 - val_loss: 1.4683 - val_accuracy: 0.6427\n",
      "Epoch 43/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0387 - accuracy: 0.3750\n",
      "Epoch 43: val_accuracy did not improve from 0.66267\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9969 - accuracy: 0.3551 - val_loss: 1.4484 - val_accuracy: 0.6607\n",
      "Epoch 44/1000\n",
      " 8/12 [===================>..........] - ETA: 0s - loss: 1.9574 - accuracy: 0.3633\n",
      "Epoch 44: val_accuracy improved from 0.66267 to 0.66667, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1.9579 - accuracy: 0.3671 - val_loss: 1.4273 - val_accuracy: 0.6667\n",
      "Epoch 45/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8988 - accuracy: 0.3828\n",
      "Epoch 45: val_accuracy did not improve from 0.66667\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.9274 - accuracy: 0.3611 - val_loss: 1.4092 - val_accuracy: 0.6647\n",
      "Epoch 46/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9353 - accuracy: 0.3750\n",
      "Epoch 46: val_accuracy improved from 0.66667 to 0.68463, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.9419 - accuracy: 0.3731 - val_loss: 1.3908 - val_accuracy: 0.6846\n",
      "Epoch 47/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8211 - accuracy: 0.4375\n",
      "Epoch 47: val_accuracy improved from 0.68463 to 0.70259, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.9240 - accuracy: 0.3711 - val_loss: 1.3732 - val_accuracy: 0.7026\n",
      "Epoch 48/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7993 - accuracy: 0.4609\n",
      "Epoch 48: val_accuracy improved from 0.70259 to 0.70659, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.9073 - accuracy: 0.3784 - val_loss: 1.3512 - val_accuracy: 0.7066\n",
      "Epoch 49/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8195 - accuracy: 0.3984\n",
      "Epoch 49: val_accuracy did not improve from 0.70659\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.8949 - accuracy: 0.3897 - val_loss: 1.3341 - val_accuracy: 0.7046\n",
      "Epoch 50/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8108 - accuracy: 0.4219\n",
      "Epoch 50: val_accuracy improved from 0.70659 to 0.73253, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8757 - accuracy: 0.3931 - val_loss: 1.3236 - val_accuracy: 0.7325\n",
      "Epoch 51/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9289 - accuracy: 0.3516\n",
      "Epoch 51: val_accuracy improved from 0.73253 to 0.74251, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8434 - accuracy: 0.3904 - val_loss: 1.3098 - val_accuracy: 0.7425\n",
      "Epoch 52/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7530 - accuracy: 0.3984\n",
      "Epoch 52: val_accuracy did not improve from 0.74251\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8349 - accuracy: 0.3997 - val_loss: 1.2909 - val_accuracy: 0.7166\n",
      "Epoch 53/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9363 - accuracy: 0.4375\n",
      "Epoch 53: val_accuracy did not improve from 0.74251\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.8229 - accuracy: 0.4151 - val_loss: 1.2737 - val_accuracy: 0.7026\n",
      "Epoch 54/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7360 - accuracy: 0.4141\n",
      "Epoch 54: val_accuracy did not improve from 0.74251\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8268 - accuracy: 0.3951 - val_loss: 1.2629 - val_accuracy: 0.7126\n",
      "Epoch 55/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8454 - accuracy: 0.4062\n",
      "Epoch 55: val_accuracy did not improve from 0.74251\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8758 - accuracy: 0.3811 - val_loss: 1.2527 - val_accuracy: 0.7086\n",
      "Epoch 56/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8625 - accuracy: 0.4141\n",
      "Epoch 56: val_accuracy did not improve from 0.74251\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8285 - accuracy: 0.3904 - val_loss: 1.2429 - val_accuracy: 0.7146\n",
      "Epoch 57/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7398 - accuracy: 0.4844\n",
      "Epoch 57: val_accuracy did not improve from 0.74251\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7950 - accuracy: 0.4144 - val_loss: 1.2291 - val_accuracy: 0.7186\n",
      "Epoch 58/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9210 - accuracy: 0.3984\n",
      "Epoch 58: val_accuracy did not improve from 0.74251\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.8153 - accuracy: 0.4051 - val_loss: 1.2157 - val_accuracy: 0.7405\n",
      "Epoch 59/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6937 - accuracy: 0.4609\n",
      "Epoch 59: val_accuracy improved from 0.74251 to 0.75449, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.7785 - accuracy: 0.4197 - val_loss: 1.2048 - val_accuracy: 0.7545\n",
      "Epoch 60/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8364 - accuracy: 0.3906\n",
      "Epoch 60: val_accuracy did not improve from 0.75449\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7530 - accuracy: 0.4264 - val_loss: 1.1936 - val_accuracy: 0.7285\n",
      "Epoch 61/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7565 - accuracy: 0.4219\n",
      "Epoch 61: val_accuracy improved from 0.75449 to 0.75848, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.7783 - accuracy: 0.4231 - val_loss: 1.1819 - val_accuracy: 0.7585\n",
      "Epoch 62/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9107 - accuracy: 0.4297\n",
      "Epoch 62: val_accuracy did not improve from 0.75848\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7919 - accuracy: 0.4211 - val_loss: 1.1768 - val_accuracy: 0.7425\n",
      "Epoch 63/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6697 - accuracy: 0.4062\n",
      "Epoch 63: val_accuracy did not improve from 0.75848\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7455 - accuracy: 0.4197 - val_loss: 1.1696 - val_accuracy: 0.7405\n",
      "Epoch 64/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7680 - accuracy: 0.4375\n",
      "Epoch 64: val_accuracy did not improve from 0.75848\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7940 - accuracy: 0.4151 - val_loss: 1.1645 - val_accuracy: 0.7385\n",
      "Epoch 65/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7986 - accuracy: 0.4141\n",
      "Epoch 65: val_accuracy did not improve from 0.75848\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7210 - accuracy: 0.4430 - val_loss: 1.1518 - val_accuracy: 0.7405\n",
      "Epoch 66/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8428 - accuracy: 0.4141\n",
      "Epoch 66: val_accuracy did not improve from 0.75848\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7323 - accuracy: 0.4364 - val_loss: 1.1362 - val_accuracy: 0.7585\n",
      "Epoch 67/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7825 - accuracy: 0.3984\n",
      "Epoch 67: val_accuracy did not improve from 0.75848\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7363 - accuracy: 0.4284 - val_loss: 1.1274 - val_accuracy: 0.7545\n",
      "Epoch 68/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6742 - accuracy: 0.4297\n",
      "Epoch 68: val_accuracy did not improve from 0.75848\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7273 - accuracy: 0.4304 - val_loss: 1.1199 - val_accuracy: 0.7565\n",
      "Epoch 69/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6814 - accuracy: 0.4609\n",
      "Epoch 69: val_accuracy improved from 0.75848 to 0.76248, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7325 - accuracy: 0.4317 - val_loss: 1.1090 - val_accuracy: 0.7625\n",
      "Epoch 70/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6335 - accuracy: 0.4609\n",
      "Epoch 70: val_accuracy did not improve from 0.76248\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7269 - accuracy: 0.4264 - val_loss: 1.1047 - val_accuracy: 0.7625\n",
      "Epoch 71/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6638 - accuracy: 0.4375\n",
      "Epoch 71: val_accuracy improved from 0.76248 to 0.76647, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.7108 - accuracy: 0.4357 - val_loss: 1.0961 - val_accuracy: 0.7665\n",
      "Epoch 72/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6259 - accuracy: 0.4609\n",
      "Epoch 72: val_accuracy did not improve from 0.76647\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6918 - accuracy: 0.4250 - val_loss: 1.0930 - val_accuracy: 0.7645\n",
      "Epoch 73/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7482 - accuracy: 0.4141\n",
      "Epoch 73: val_accuracy did not improve from 0.76647\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7147 - accuracy: 0.4284 - val_loss: 1.0862 - val_accuracy: 0.7465\n",
      "Epoch 74/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6189 - accuracy: 0.4219\n",
      "Epoch 74: val_accuracy did not improve from 0.76647\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6969 - accuracy: 0.4310 - val_loss: 1.0782 - val_accuracy: 0.7525\n",
      "Epoch 75/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5153 - accuracy: 0.5000\n",
      "Epoch 75: val_accuracy did not improve from 0.76647\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6618 - accuracy: 0.4590 - val_loss: 1.0675 - val_accuracy: 0.7525\n",
      "Epoch 76/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7738 - accuracy: 0.3750\n",
      "Epoch 76: val_accuracy did not improve from 0.76647\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6794 - accuracy: 0.4390 - val_loss: 1.0617 - val_accuracy: 0.7625\n",
      "Epoch 77/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5996 - accuracy: 0.4609\n",
      "Epoch 77: val_accuracy improved from 0.76647 to 0.78044, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6703 - accuracy: 0.4384 - val_loss: 1.0565 - val_accuracy: 0.7804\n",
      "Epoch 78/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5976 - accuracy: 0.4766\n",
      "Epoch 78: val_accuracy did not improve from 0.78044\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6634 - accuracy: 0.4350 - val_loss: 1.0498 - val_accuracy: 0.7804\n",
      "Epoch 79/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5922 - accuracy: 0.4922\n",
      "Epoch 79: val_accuracy did not improve from 0.78044\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6110 - accuracy: 0.4710 - val_loss: 1.0428 - val_accuracy: 0.7804\n",
      "Epoch 80/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5750 - accuracy: 0.4531\n",
      "Epoch 80: val_accuracy did not improve from 0.78044\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6611 - accuracy: 0.4610 - val_loss: 1.0354 - val_accuracy: 0.7804\n",
      "Epoch 81/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6074 - accuracy: 0.4844\n",
      "Epoch 81: val_accuracy did not improve from 0.78044\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6889 - accuracy: 0.4484 - val_loss: 1.0323 - val_accuracy: 0.7645\n",
      "Epoch 82/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5516 - accuracy: 0.5312\n",
      "Epoch 82: val_accuracy did not improve from 0.78044\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6227 - accuracy: 0.4817 - val_loss: 1.0227 - val_accuracy: 0.7585\n",
      "Epoch 83/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7668 - accuracy: 0.3750\n",
      "Epoch 83: val_accuracy did not improve from 0.78044\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6359 - accuracy: 0.4464 - val_loss: 1.0199 - val_accuracy: 0.7465\n",
      "Epoch 84/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6494 - accuracy: 0.4584\n",
      "Epoch 84: val_accuracy improved from 0.78044 to 0.78842, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 1.6494 - accuracy: 0.4584 - val_loss: 1.0102 - val_accuracy: 0.7884\n",
      "Epoch 85/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7413 - accuracy: 0.4297\n",
      "Epoch 85: val_accuracy did not improve from 0.78842\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6320 - accuracy: 0.4597 - val_loss: 1.0057 - val_accuracy: 0.7685\n",
      "Epoch 86/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4650 - accuracy: 0.5312\n",
      "Epoch 86: val_accuracy did not improve from 0.78842\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6491 - accuracy: 0.4397 - val_loss: 1.0016 - val_accuracy: 0.7764\n",
      "Epoch 87/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7683 - accuracy: 0.4688\n",
      "Epoch 87: val_accuracy improved from 0.78842 to 0.80838, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6240 - accuracy: 0.4624 - val_loss: 0.9986 - val_accuracy: 0.8084\n",
      "Epoch 88/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4778 - accuracy: 0.4531\n",
      "Epoch 88: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6216 - accuracy: 0.4704 - val_loss: 0.9951 - val_accuracy: 0.8004\n",
      "Epoch 89/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6149 - accuracy: 0.4531\n",
      "Epoch 89: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5758 - accuracy: 0.4630 - val_loss: 0.9897 - val_accuracy: 0.7545\n",
      "Epoch 90/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6464 - accuracy: 0.4219\n",
      "Epoch 90: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6493 - accuracy: 0.4564 - val_loss: 0.9827 - val_accuracy: 0.7625\n",
      "Epoch 91/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6499 - accuracy: 0.4609\n",
      "Epoch 91: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5876 - accuracy: 0.4637 - val_loss: 0.9761 - val_accuracy: 0.7645\n",
      "Epoch 92/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6627 - accuracy: 0.4453\n",
      "Epoch 92: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6225 - accuracy: 0.4524 - val_loss: 0.9753 - val_accuracy: 0.7605\n",
      "Epoch 93/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4968 - accuracy: 0.4453\n",
      "Epoch 93: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6013 - accuracy: 0.4817 - val_loss: 0.9667 - val_accuracy: 0.7984\n",
      "Epoch 94/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5134 - accuracy: 0.4922\n",
      "Epoch 94: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6187 - accuracy: 0.4477 - val_loss: 0.9618 - val_accuracy: 0.7944\n",
      "Epoch 95/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3844 - accuracy: 0.5469\n",
      "Epoch 95: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5663 - accuracy: 0.4717 - val_loss: 0.9567 - val_accuracy: 0.7964\n",
      "Epoch 96/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5673 - accuracy: 0.4062\n",
      "Epoch 96: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5507 - accuracy: 0.4724 - val_loss: 0.9557 - val_accuracy: 0.7924\n",
      "Epoch 97/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6825 - accuracy: 0.4375\n",
      "Epoch 97: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5651 - accuracy: 0.4650 - val_loss: 0.9541 - val_accuracy: 0.7904\n",
      "Epoch 98/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4365 - accuracy: 0.5234\n",
      "Epoch 98: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5936 - accuracy: 0.4717 - val_loss: 0.9475 - val_accuracy: 0.8044\n",
      "Epoch 99/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5889 - accuracy: 0.5156\n",
      "Epoch 99: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5922 - accuracy: 0.4704 - val_loss: 0.9453 - val_accuracy: 0.7804\n",
      "Epoch 100/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6138 - accuracy: 0.4375\n",
      "Epoch 100: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6136 - accuracy: 0.4617 - val_loss: 0.9456 - val_accuracy: 0.7864\n",
      "Epoch 101/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4991 - accuracy: 0.5625\n",
      "Epoch 101: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6054 - accuracy: 0.4664 - val_loss: 0.9411 - val_accuracy: 0.7884\n",
      "Epoch 102/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6864 - accuracy: 0.4453\n",
      "Epoch 102: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5621 - accuracy: 0.4677 - val_loss: 0.9416 - val_accuracy: 0.7984\n",
      "Epoch 103/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5750 - accuracy: 0.4922\n",
      "Epoch 103: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5124 - accuracy: 0.4823 - val_loss: 0.9338 - val_accuracy: 0.7904\n",
      "Epoch 104/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5038 - accuracy: 0.4375\n",
      "Epoch 104: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5330 - accuracy: 0.4843 - val_loss: 0.9254 - val_accuracy: 0.7964\n",
      "Epoch 105/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4736 - accuracy: 0.5156\n",
      "Epoch 105: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5711 - accuracy: 0.4750 - val_loss: 0.9215 - val_accuracy: 0.7984\n",
      "Epoch 106/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3690 - accuracy: 0.5312\n",
      "Epoch 106: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5520 - accuracy: 0.4657 - val_loss: 0.9185 - val_accuracy: 0.7904\n",
      "Epoch 107/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6905 - accuracy: 0.4453\n",
      "Epoch 107: val_accuracy did not improve from 0.80838\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5312 - accuracy: 0.4890 - val_loss: 0.9148 - val_accuracy: 0.7984\n",
      "Epoch 108/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5316 - accuracy: 0.4375\n",
      "Epoch 108: val_accuracy improved from 0.80838 to 0.81238, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5277 - accuracy: 0.4783 - val_loss: 0.9070 - val_accuracy: 0.8124\n",
      "Epoch 109/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4801 - accuracy: 0.4844\n",
      "Epoch 109: val_accuracy did not improve from 0.81238\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5433 - accuracy: 0.4783 - val_loss: 0.9022 - val_accuracy: 0.8044\n",
      "Epoch 110/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5825 - accuracy: 0.4062\n",
      "Epoch 110: val_accuracy did not improve from 0.81238\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5504 - accuracy: 0.4797 - val_loss: 0.8998 - val_accuracy: 0.8104\n",
      "Epoch 111/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5616 - accuracy: 0.4453\n",
      "Epoch 111: val_accuracy improved from 0.81238 to 0.84431, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5137 - accuracy: 0.4943 - val_loss: 0.9010 - val_accuracy: 0.8443\n",
      "Epoch 112/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4798 - accuracy: 0.5469\n",
      "Epoch 112: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5159 - accuracy: 0.5037 - val_loss: 0.8958 - val_accuracy: 0.8044\n",
      "Epoch 113/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4027 - accuracy: 0.5078\n",
      "Epoch 113: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4838 - accuracy: 0.4897 - val_loss: 0.8941 - val_accuracy: 0.7864\n",
      "Epoch 114/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3828 - accuracy: 0.5469\n",
      "Epoch 114: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4818 - accuracy: 0.4877 - val_loss: 0.8855 - val_accuracy: 0.8184\n",
      "Epoch 115/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6325 - accuracy: 0.5078\n",
      "Epoch 115: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5435 - accuracy: 0.4750 - val_loss: 0.8836 - val_accuracy: 0.8184\n",
      "Epoch 116/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5528 - accuracy: 0.4375\n",
      "Epoch 116: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5495 - accuracy: 0.4783 - val_loss: 0.8805 - val_accuracy: 0.8244\n",
      "Epoch 117/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4395 - accuracy: 0.5156\n",
      "Epoch 117: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5352 - accuracy: 0.4963 - val_loss: 0.8812 - val_accuracy: 0.8184\n",
      "Epoch 118/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4648 - accuracy: 0.6094\n",
      "Epoch 118: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5227 - accuracy: 0.5070 - val_loss: 0.8816 - val_accuracy: 0.8104\n",
      "Epoch 119/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3623 - accuracy: 0.4844\n",
      "Epoch 119: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4846 - accuracy: 0.5083 - val_loss: 0.8740 - val_accuracy: 0.8283\n",
      "Epoch 120/1000\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 1.4858 - accuracy: 0.4822\n",
      "Epoch 120: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.4988 - accuracy: 0.4777 - val_loss: 0.8685 - val_accuracy: 0.8044\n",
      "Epoch 121/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4442 - accuracy: 0.5000\n",
      "Epoch 121: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4849 - accuracy: 0.4950 - val_loss: 0.8687 - val_accuracy: 0.8144\n",
      "Epoch 122/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5129 - accuracy: 0.4766\n",
      "Epoch 122: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5285 - accuracy: 0.4970 - val_loss: 0.8690 - val_accuracy: 0.8263\n",
      "Epoch 123/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5745 - accuracy: 0.4531\n",
      "Epoch 123: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5285 - accuracy: 0.4817 - val_loss: 0.8698 - val_accuracy: 0.8104\n",
      "Epoch 124/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3791 - accuracy: 0.5781\n",
      "Epoch 124: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5306 - accuracy: 0.4790 - val_loss: 0.8673 - val_accuracy: 0.8343\n",
      "Epoch 125/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4749 - accuracy: 0.5234\n",
      "Epoch 125: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5450 - accuracy: 0.4763 - val_loss: 0.8672 - val_accuracy: 0.8323\n",
      "Epoch 126/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2338 - accuracy: 0.5781\n",
      "Epoch 126: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4765 - accuracy: 0.5157 - val_loss: 0.8649 - val_accuracy: 0.8064\n",
      "Epoch 127/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4635 - accuracy: 0.5234\n",
      "Epoch 127: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5432 - accuracy: 0.4837 - val_loss: 0.8628 - val_accuracy: 0.8124\n",
      "Epoch 128/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4074 - accuracy: 0.5312\n",
      "Epoch 128: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5090 - accuracy: 0.4937 - val_loss: 0.8641 - val_accuracy: 0.8124\n",
      "Epoch 129/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6531 - accuracy: 0.4375\n",
      "Epoch 129: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5117 - accuracy: 0.4983 - val_loss: 0.8610 - val_accuracy: 0.8224\n",
      "Epoch 130/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7277 - accuracy: 0.4531\n",
      "Epoch 130: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5394 - accuracy: 0.4837 - val_loss: 0.8590 - val_accuracy: 0.8204\n",
      "Epoch 131/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3644 - accuracy: 0.5859\n",
      "Epoch 131: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4751 - accuracy: 0.5050 - val_loss: 0.8551 - val_accuracy: 0.8124\n",
      "Epoch 132/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5452 - accuracy: 0.4688\n",
      "Epoch 132: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5076 - accuracy: 0.4890 - val_loss: 0.8564 - val_accuracy: 0.8204\n",
      "Epoch 133/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3914 - accuracy: 0.5703\n",
      "Epoch 133: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4568 - accuracy: 0.5103 - val_loss: 0.8556 - val_accuracy: 0.8104\n",
      "Epoch 134/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5113 - accuracy: 0.4766\n",
      "Epoch 134: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5022 - accuracy: 0.4943 - val_loss: 0.8544 - val_accuracy: 0.7924\n",
      "Epoch 135/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5318 - accuracy: 0.4766\n",
      "Epoch 135: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4500 - accuracy: 0.5077 - val_loss: 0.8508 - val_accuracy: 0.7944\n",
      "Epoch 136/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5177 - accuracy: 0.4688\n",
      "Epoch 136: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4561 - accuracy: 0.4870 - val_loss: 0.8431 - val_accuracy: 0.8343\n",
      "Epoch 137/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5224 - accuracy: 0.5156\n",
      "Epoch 137: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4601 - accuracy: 0.5117 - val_loss: 0.8393 - val_accuracy: 0.8383\n",
      "Epoch 138/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5817 - accuracy: 0.4453\n",
      "Epoch 138: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4506 - accuracy: 0.5003 - val_loss: 0.8354 - val_accuracy: 0.8323\n",
      "Epoch 139/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3896 - accuracy: 0.5391\n",
      "Epoch 139: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4934 - accuracy: 0.4770 - val_loss: 0.8370 - val_accuracy: 0.8064\n",
      "Epoch 140/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5028 - accuracy: 0.5312\n",
      "Epoch 140: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4502 - accuracy: 0.5010 - val_loss: 0.8319 - val_accuracy: 0.8303\n",
      "Epoch 141/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3532 - accuracy: 0.5469\n",
      "Epoch 141: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4310 - accuracy: 0.5250 - val_loss: 0.8295 - val_accuracy: 0.8383\n",
      "Epoch 142/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5010 - accuracy: 0.5000\n",
      "Epoch 142: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4767 - accuracy: 0.5017 - val_loss: 0.8284 - val_accuracy: 0.8283\n",
      "Epoch 143/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4578 - accuracy: 0.4766\n",
      "Epoch 143: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4941 - accuracy: 0.4977 - val_loss: 0.8329 - val_accuracy: 0.7924\n",
      "Epoch 144/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3816 - accuracy: 0.4688\n",
      "Epoch 144: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4866 - accuracy: 0.4983 - val_loss: 0.8252 - val_accuracy: 0.8204\n",
      "Epoch 145/1000\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 1.5212 - accuracy: 0.4815\n",
      "Epoch 145: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.5078 - accuracy: 0.4837 - val_loss: 0.8241 - val_accuracy: 0.8323\n",
      "Epoch 146/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4399 - accuracy: 0.5547\n",
      "Epoch 146: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4526 - accuracy: 0.5017 - val_loss: 0.8225 - val_accuracy: 0.8423\n",
      "Epoch 147/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4473 - accuracy: 0.5625\n",
      "Epoch 147: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4671 - accuracy: 0.5203 - val_loss: 0.8235 - val_accuracy: 0.8104\n",
      "Epoch 148/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5531 - accuracy: 0.5312\n",
      "Epoch 148: val_accuracy did not improve from 0.84431\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4615 - accuracy: 0.4877 - val_loss: 0.8194 - val_accuracy: 0.8124\n",
      "Epoch 149/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4612 - accuracy: 0.4922\n",
      "Epoch 149: val_accuracy improved from 0.84431 to 0.85429, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.4048 - accuracy: 0.5250 - val_loss: 0.8099 - val_accuracy: 0.8543\n",
      "Epoch 150/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5126 - accuracy: 0.5156\n",
      "Epoch 150: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4957 - accuracy: 0.5050 - val_loss: 0.8093 - val_accuracy: 0.8363\n",
      "Epoch 151/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5433 - accuracy: 0.4766\n",
      "Epoch 151: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4882 - accuracy: 0.4790 - val_loss: 0.8161 - val_accuracy: 0.8423\n",
      "Epoch 152/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5103 - accuracy: 0.5078\n",
      "Epoch 152: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4212 - accuracy: 0.5276 - val_loss: 0.8156 - val_accuracy: 0.8244\n",
      "Epoch 153/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3507 - accuracy: 0.6172\n",
      "Epoch 153: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4123 - accuracy: 0.5150 - val_loss: 0.8048 - val_accuracy: 0.8283\n",
      "Epoch 154/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4909 - accuracy: 0.4609\n",
      "Epoch 154: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5053 - accuracy: 0.4797 - val_loss: 0.8061 - val_accuracy: 0.8483\n",
      "Epoch 155/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5152 - accuracy: 0.4531\n",
      "Epoch 155: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4064 - accuracy: 0.5270 - val_loss: 0.8066 - val_accuracy: 0.8503\n",
      "Epoch 156/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4829 - accuracy: 0.5078\n",
      "Epoch 156: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4248 - accuracy: 0.5143 - val_loss: 0.8035 - val_accuracy: 0.8503\n",
      "Epoch 157/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5185 - accuracy: 0.5000\n",
      "Epoch 157: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4526 - accuracy: 0.5083 - val_loss: 0.8001 - val_accuracy: 0.8323\n",
      "Epoch 158/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6995 - accuracy: 0.4375\n",
      "Epoch 158: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4387 - accuracy: 0.5050 - val_loss: 0.7937 - val_accuracy: 0.8204\n",
      "Epoch 159/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3070 - accuracy: 0.5547\n",
      "Epoch 159: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4130 - accuracy: 0.5330 - val_loss: 0.7878 - val_accuracy: 0.8303\n",
      "Epoch 160/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5583 - accuracy: 0.4531\n",
      "Epoch 160: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4799 - accuracy: 0.5083 - val_loss: 0.7924 - val_accuracy: 0.8383\n",
      "Epoch 161/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4334 - accuracy: 0.5312\n",
      "Epoch 161: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4414 - accuracy: 0.5110 - val_loss: 0.7973 - val_accuracy: 0.8363\n",
      "Epoch 162/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4361 - accuracy: 0.5703\n",
      "Epoch 162: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4014 - accuracy: 0.5323 - val_loss: 0.8018 - val_accuracy: 0.8283\n",
      "Epoch 163/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3667 - accuracy: 0.5078\n",
      "Epoch 163: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4419 - accuracy: 0.5210 - val_loss: 0.7959 - val_accuracy: 0.8283\n",
      "Epoch 164/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2605 - accuracy: 0.6484\n",
      "Epoch 164: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4872 - accuracy: 0.5003 - val_loss: 0.7974 - val_accuracy: 0.8403\n",
      "Epoch 165/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4385 - accuracy: 0.5156\n",
      "Epoch 165: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4180 - accuracy: 0.5243 - val_loss: 0.7941 - val_accuracy: 0.8403\n",
      "Epoch 166/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2585 - accuracy: 0.6016\n",
      "Epoch 166: val_accuracy did not improve from 0.85429\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4304 - accuracy: 0.5150 - val_loss: 0.7928 - val_accuracy: 0.8383\n",
      "Epoch 167/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5835 - accuracy: 0.5547\n",
      "Epoch 167: val_accuracy improved from 0.85429 to 0.85828, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.4061 - accuracy: 0.5316 - val_loss: 0.7885 - val_accuracy: 0.8583\n",
      "Epoch 168/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3790 - accuracy: 0.5938\n",
      "Epoch 168: val_accuracy did not improve from 0.85828\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3723 - accuracy: 0.5436 - val_loss: 0.7789 - val_accuracy: 0.8503\n",
      "Epoch 169/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3617 - accuracy: 0.5391\n",
      "Epoch 169: val_accuracy did not improve from 0.85828\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3924 - accuracy: 0.5290 - val_loss: 0.7747 - val_accuracy: 0.8503\n",
      "Epoch 170/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5052 - accuracy: 0.5469\n",
      "Epoch 170: val_accuracy improved from 0.85828 to 0.86228, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.4060 - accuracy: 0.5250 - val_loss: 0.7730 - val_accuracy: 0.8623\n",
      "Epoch 171/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4666 - accuracy: 0.5156\n",
      "Epoch 171: val_accuracy improved from 0.86228 to 0.86826, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.4197 - accuracy: 0.5170 - val_loss: 0.7750 - val_accuracy: 0.8683\n",
      "Epoch 172/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2966 - accuracy: 0.5625\n",
      "Epoch 172: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3918 - accuracy: 0.5137 - val_loss: 0.7804 - val_accuracy: 0.8443\n",
      "Epoch 173/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2112 - accuracy: 0.6250\n",
      "Epoch 173: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4287 - accuracy: 0.5170 - val_loss: 0.7764 - val_accuracy: 0.8503\n",
      "Epoch 174/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6071 - accuracy: 0.5000\n",
      "Epoch 174: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4399 - accuracy: 0.5090 - val_loss: 0.7759 - val_accuracy: 0.8583\n",
      "Epoch 175/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2101 - accuracy: 0.6016\n",
      "Epoch 175: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3354 - accuracy: 0.5390 - val_loss: 0.7737 - val_accuracy: 0.8503\n",
      "Epoch 176/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3432 - accuracy: 0.5000\n",
      "Epoch 176: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3720 - accuracy: 0.5323 - val_loss: 0.7699 - val_accuracy: 0.8583\n",
      "Epoch 177/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2922 - accuracy: 0.5156\n",
      "Epoch 177: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4010 - accuracy: 0.5203 - val_loss: 0.7680 - val_accuracy: 0.8463\n",
      "Epoch 178/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4239 - accuracy: 0.4141\n",
      "Epoch 178: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.4253 - accuracy: 0.5090 - val_loss: 0.7639 - val_accuracy: 0.8423\n",
      "Epoch 179/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3859 - accuracy: 0.5234\n",
      "Epoch 179: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4143 - accuracy: 0.5123 - val_loss: 0.7638 - val_accuracy: 0.8244\n",
      "Epoch 180/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3988 - accuracy: 0.5312\n",
      "Epoch 180: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3857 - accuracy: 0.5283 - val_loss: 0.7626 - val_accuracy: 0.8204\n",
      "Epoch 181/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2976 - accuracy: 0.5078\n",
      "Epoch 181: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3686 - accuracy: 0.5243 - val_loss: 0.7619 - val_accuracy: 0.8224\n",
      "Epoch 182/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3814 - accuracy: 0.5703\n",
      "Epoch 182: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4091 - accuracy: 0.5323 - val_loss: 0.7606 - val_accuracy: 0.8283\n",
      "Epoch 183/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4276 - accuracy: 0.5625\n",
      "Epoch 183: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3813 - accuracy: 0.5183 - val_loss: 0.7553 - val_accuracy: 0.8503\n",
      "Epoch 184/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3048 - accuracy: 0.5625\n",
      "Epoch 184: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.3939 - accuracy: 0.5203 - val_loss: 0.7505 - val_accuracy: 0.8423\n",
      "Epoch 185/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5466 - accuracy: 0.4844\n",
      "Epoch 185: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.3999 - accuracy: 0.5190 - val_loss: 0.7534 - val_accuracy: 0.8523\n",
      "Epoch 186/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3530 - accuracy: 0.5156\n",
      "Epoch 186: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3991 - accuracy: 0.5290 - val_loss: 0.7558 - val_accuracy: 0.8323\n",
      "Epoch 187/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2543 - accuracy: 0.5391\n",
      "Epoch 187: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3664 - accuracy: 0.5256 - val_loss: 0.7540 - val_accuracy: 0.8363\n",
      "Epoch 188/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5553 - accuracy: 0.4219\n",
      "Epoch 188: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3972 - accuracy: 0.5237 - val_loss: 0.7541 - val_accuracy: 0.8403\n",
      "Epoch 189/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3876 - accuracy: 0.5703\n",
      "Epoch 189: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4088 - accuracy: 0.5203 - val_loss: 0.7525 - val_accuracy: 0.8363\n",
      "Epoch 190/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3833 - accuracy: 0.5312\n",
      "Epoch 190: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3376 - accuracy: 0.5490 - val_loss: 0.7472 - val_accuracy: 0.8483\n",
      "Epoch 191/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4266 - accuracy: 0.5156\n",
      "Epoch 191: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3793 - accuracy: 0.5330 - val_loss: 0.7383 - val_accuracy: 0.8363\n",
      "Epoch 192/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6680 - accuracy: 0.5234\n",
      "Epoch 192: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4086 - accuracy: 0.5376 - val_loss: 0.7376 - val_accuracy: 0.8463\n",
      "Epoch 193/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3921 - accuracy: 0.5312\n",
      "Epoch 193: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3622 - accuracy: 0.5270 - val_loss: 0.7381 - val_accuracy: 0.8583\n",
      "Epoch 194/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4537 - accuracy: 0.5234\n",
      "Epoch 194: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3544 - accuracy: 0.5383 - val_loss: 0.7345 - val_accuracy: 0.8603\n",
      "Epoch 195/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3832 - accuracy: 0.5234\n",
      "Epoch 195: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3393 - accuracy: 0.5436 - val_loss: 0.7381 - val_accuracy: 0.8423\n",
      "Epoch 196/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3795 - accuracy: 0.5391\n",
      "Epoch 196: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3982 - accuracy: 0.5256 - val_loss: 0.7403 - val_accuracy: 0.8463\n",
      "Epoch 197/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2514 - accuracy: 0.5625\n",
      "Epoch 197: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3955 - accuracy: 0.5063 - val_loss: 0.7419 - val_accuracy: 0.8383\n",
      "Epoch 198/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4938 - accuracy: 0.5391\n",
      "Epoch 198: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3767 - accuracy: 0.5463 - val_loss: 0.7398 - val_accuracy: 0.8463\n",
      "Epoch 199/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1576 - accuracy: 0.5547\n",
      "Epoch 199: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3273 - accuracy: 0.5376 - val_loss: 0.7310 - val_accuracy: 0.8523\n",
      "Epoch 200/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1145 - accuracy: 0.6172\n",
      "Epoch 200: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3480 - accuracy: 0.5390 - val_loss: 0.7309 - val_accuracy: 0.8483\n",
      "Epoch 201/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4043 - accuracy: 0.5312\n",
      "Epoch 201: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3950 - accuracy: 0.5323 - val_loss: 0.7290 - val_accuracy: 0.8443\n",
      "Epoch 202/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3897 - accuracy: 0.5469\n",
      "Epoch 202: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3794 - accuracy: 0.5383 - val_loss: 0.7332 - val_accuracy: 0.8443\n",
      "Epoch 203/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3383 - accuracy: 0.5078\n",
      "Epoch 203: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.3742 - accuracy: 0.5256 - val_loss: 0.7375 - val_accuracy: 0.8363\n",
      "Epoch 204/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3970 - accuracy: 0.5078\n",
      "Epoch 204: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3519 - accuracy: 0.5410 - val_loss: 0.7337 - val_accuracy: 0.8443\n",
      "Epoch 205/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3862 - accuracy: 0.4922\n",
      "Epoch 205: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3695 - accuracy: 0.5330 - val_loss: 0.7298 - val_accuracy: 0.8323\n",
      "Epoch 206/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2487 - accuracy: 0.5938\n",
      "Epoch 206: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3431 - accuracy: 0.5623 - val_loss: 0.7278 - val_accuracy: 0.8523\n",
      "Epoch 207/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4057 - accuracy: 0.5234\n",
      "Epoch 207: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3902 - accuracy: 0.5243 - val_loss: 0.7268 - val_accuracy: 0.8503\n",
      "Epoch 208/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4743 - accuracy: 0.5938\n",
      "Epoch 208: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3369 - accuracy: 0.5596 - val_loss: 0.7251 - val_accuracy: 0.8583\n",
      "Epoch 209/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6059 - accuracy: 0.5000\n",
      "Epoch 209: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3626 - accuracy: 0.5510 - val_loss: 0.7211 - val_accuracy: 0.8523\n",
      "Epoch 210/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3604 - accuracy: 0.4688\n",
      "Epoch 210: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3756 - accuracy: 0.5376 - val_loss: 0.7201 - val_accuracy: 0.8523\n",
      "Epoch 211/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3160 - accuracy: 0.5156\n",
      "Epoch 211: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3615 - accuracy: 0.5443 - val_loss: 0.7165 - val_accuracy: 0.8603\n",
      "Epoch 212/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1661 - accuracy: 0.5625\n",
      "Epoch 212: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3082 - accuracy: 0.5523 - val_loss: 0.7145 - val_accuracy: 0.8643\n",
      "Epoch 213/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4564 - accuracy: 0.5234\n",
      "Epoch 213: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3301 - accuracy: 0.5550 - val_loss: 0.7165 - val_accuracy: 0.8343\n",
      "Epoch 214/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5795 - accuracy: 0.5078\n",
      "Epoch 214: val_accuracy did not improve from 0.86826\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3870 - accuracy: 0.5150 - val_loss: 0.7122 - val_accuracy: 0.8583\n",
      "Epoch 215/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4160 - accuracy: 0.5078\n",
      "Epoch 215: val_accuracy improved from 0.86826 to 0.87026, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.3519 - accuracy: 0.5450 - val_loss: 0.7154 - val_accuracy: 0.8703\n",
      "Epoch 216/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3609 - accuracy: 0.5703\n",
      "Epoch 216: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3578 - accuracy: 0.5390 - val_loss: 0.7159 - val_accuracy: 0.8623\n",
      "Epoch 217/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2367 - accuracy: 0.6250\n",
      "Epoch 217: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3166 - accuracy: 0.5536 - val_loss: 0.7122 - val_accuracy: 0.8523\n",
      "Epoch 218/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5308 - accuracy: 0.5078\n",
      "Epoch 218: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3758 - accuracy: 0.5563 - val_loss: 0.7101 - val_accuracy: 0.8343\n",
      "Epoch 219/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2558 - accuracy: 0.5625\n",
      "Epoch 219: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3327 - accuracy: 0.5410 - val_loss: 0.7072 - val_accuracy: 0.8323\n",
      "Epoch 220/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1877 - accuracy: 0.6875\n",
      "Epoch 220: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3131 - accuracy: 0.5676 - val_loss: 0.7029 - val_accuracy: 0.8383\n",
      "Epoch 221/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2302 - accuracy: 0.5703\n",
      "Epoch 221: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3005 - accuracy: 0.5570 - val_loss: 0.6952 - val_accuracy: 0.8443\n",
      "Epoch 222/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3278 - accuracy: 0.5000\n",
      "Epoch 222: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3372 - accuracy: 0.5217 - val_loss: 0.6951 - val_accuracy: 0.8643\n",
      "Epoch 223/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3770 - accuracy: 0.5234\n",
      "Epoch 223: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.3258 - accuracy: 0.5603 - val_loss: 0.6955 - val_accuracy: 0.8683\n",
      "Epoch 224/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6496 - accuracy: 0.4297\n",
      "Epoch 224: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4206 - accuracy: 0.5250 - val_loss: 0.6997 - val_accuracy: 0.8443\n",
      "Epoch 225/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3063 - accuracy: 0.5234\n",
      "Epoch 225: val_accuracy did not improve from 0.87026\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3816 - accuracy: 0.5276 - val_loss: 0.7077 - val_accuracy: 0.8623\n",
      "Epoch 226/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3186 - accuracy: 0.5156\n",
      "Epoch 226: val_accuracy improved from 0.87026 to 0.87226, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.3079 - accuracy: 0.5476 - val_loss: 0.7052 - val_accuracy: 0.8723\n",
      "Epoch 227/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3255 - accuracy: 0.5234\n",
      "Epoch 227: val_accuracy did not improve from 0.87226\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3346 - accuracy: 0.5456 - val_loss: 0.7036 - val_accuracy: 0.8423\n",
      "Epoch 228/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4388 - accuracy: 0.4922\n",
      "Epoch 228: val_accuracy did not improve from 0.87226\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3298 - accuracy: 0.5443 - val_loss: 0.6996 - val_accuracy: 0.8463\n",
      "Epoch 229/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3221 - accuracy: 0.4922\n",
      "Epoch 229: val_accuracy did not improve from 0.87226\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3289 - accuracy: 0.5403 - val_loss: 0.7022 - val_accuracy: 0.8363\n",
      "Epoch 230/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5995 - accuracy: 0.5156\n",
      "Epoch 230: val_accuracy did not improve from 0.87226\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3458 - accuracy: 0.5556 - val_loss: 0.7019 - val_accuracy: 0.8483\n",
      "Epoch 231/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3417 - accuracy: 0.5312\n",
      "Epoch 231: val_accuracy did not improve from 0.87226\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3123 - accuracy: 0.5576 - val_loss: 0.6992 - val_accuracy: 0.8623\n",
      "Epoch 232/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3043 - accuracy: 0.5859\n",
      "Epoch 232: val_accuracy did not improve from 0.87226\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3179 - accuracy: 0.5616 - val_loss: 0.6960 - val_accuracy: 0.8703\n",
      "Epoch 233/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3242 - accuracy: 0.5703\n",
      "Epoch 233: val_accuracy did not improve from 0.87226\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3562 - accuracy: 0.5416 - val_loss: 0.6985 - val_accuracy: 0.8663\n",
      "Epoch 234/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4055 - accuracy: 0.5547\n",
      "Epoch 234: val_accuracy improved from 0.87226 to 0.87824, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.3585 - accuracy: 0.5450 - val_loss: 0.7012 - val_accuracy: 0.8782\n",
      "Epoch 235/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3024 - accuracy: 0.5469\n",
      "Epoch 235: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2824 - accuracy: 0.5670 - val_loss: 0.6983 - val_accuracy: 0.8643\n",
      "Epoch 236/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2164 - accuracy: 0.5859\n",
      "Epoch 236: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3462 - accuracy: 0.5516 - val_loss: 0.6959 - val_accuracy: 0.8523\n",
      "Epoch 237/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2661 - accuracy: 0.5312\n",
      "Epoch 237: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3248 - accuracy: 0.5623 - val_loss: 0.6963 - val_accuracy: 0.8563\n",
      "Epoch 238/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4904 - accuracy: 0.5781\n",
      "Epoch 238: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3397 - accuracy: 0.5483 - val_loss: 0.6980 - val_accuracy: 0.8423\n",
      "Epoch 239/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2459 - accuracy: 0.5938\n",
      "Epoch 239: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2493 - accuracy: 0.5723 - val_loss: 0.6943 - val_accuracy: 0.8643\n",
      "Epoch 240/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2825 - accuracy: 0.5625\n",
      "Epoch 240: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3246 - accuracy: 0.5583 - val_loss: 0.6887 - val_accuracy: 0.8503\n",
      "Epoch 241/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2166 - accuracy: 0.6094\n",
      "Epoch 241: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3340 - accuracy: 0.5476 - val_loss: 0.6893 - val_accuracy: 0.8443\n",
      "Epoch 242/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2381 - accuracy: 0.5625\n",
      "Epoch 242: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3410 - accuracy: 0.5396 - val_loss: 0.6929 - val_accuracy: 0.8663\n",
      "Epoch 243/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2547 - accuracy: 0.5625\n",
      "Epoch 243: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.3492 - accuracy: 0.5490 - val_loss: 0.6934 - val_accuracy: 0.8663\n",
      "Epoch 244/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1997 - accuracy: 0.5078\n",
      "Epoch 244: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3511 - accuracy: 0.5336 - val_loss: 0.6917 - val_accuracy: 0.8643\n",
      "Epoch 245/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2273 - accuracy: 0.5859\n",
      "Epoch 245: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3352 - accuracy: 0.5516 - val_loss: 0.6894 - val_accuracy: 0.8483\n",
      "Epoch 246/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2987 - accuracy: 0.5703\n",
      "Epoch 246: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3297 - accuracy: 0.5610 - val_loss: 0.6854 - val_accuracy: 0.8623\n",
      "Epoch 247/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4818 - accuracy: 0.5078\n",
      "Epoch 247: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2823 - accuracy: 0.5536 - val_loss: 0.6807 - val_accuracy: 0.8523\n",
      "Epoch 248/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4142 - accuracy: 0.5781\n",
      "Epoch 248: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3074 - accuracy: 0.5536 - val_loss: 0.6845 - val_accuracy: 0.8583\n",
      "Epoch 249/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3379 - accuracy: 0.5312\n",
      "Epoch 249: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3461 - accuracy: 0.5383 - val_loss: 0.6841 - val_accuracy: 0.8643\n",
      "Epoch 250/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2966 - accuracy: 0.5547\n",
      "Epoch 250: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3520 - accuracy: 0.5430 - val_loss: 0.6827 - val_accuracy: 0.8603\n",
      "Epoch 251/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3240 - accuracy: 0.5391\n",
      "Epoch 251: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3323 - accuracy: 0.5656 - val_loss: 0.6805 - val_accuracy: 0.8543\n",
      "Epoch 252/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2240 - accuracy: 0.5938\n",
      "Epoch 252: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3213 - accuracy: 0.5636 - val_loss: 0.6823 - val_accuracy: 0.8383\n",
      "Epoch 253/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2895 - accuracy: 0.5469\n",
      "Epoch 253: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3430 - accuracy: 0.5476 - val_loss: 0.6786 - val_accuracy: 0.8663\n",
      "Epoch 254/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2669 - accuracy: 0.5625\n",
      "Epoch 254: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3185 - accuracy: 0.5576 - val_loss: 0.6791 - val_accuracy: 0.8563\n",
      "Epoch 255/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4593 - accuracy: 0.5469\n",
      "Epoch 255: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3044 - accuracy: 0.5503 - val_loss: 0.6835 - val_accuracy: 0.8503\n",
      "Epoch 256/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6126 - accuracy: 0.5391\n",
      "Epoch 256: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3371 - accuracy: 0.5416 - val_loss: 0.6845 - val_accuracy: 0.8583\n",
      "Epoch 257/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1374 - accuracy: 0.6094\n",
      "Epoch 257: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3230 - accuracy: 0.5410 - val_loss: 0.6873 - val_accuracy: 0.8563\n",
      "Epoch 258/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3383 - accuracy: 0.5469\n",
      "Epoch 258: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3145 - accuracy: 0.5470 - val_loss: 0.6827 - val_accuracy: 0.8643\n",
      "Epoch 259/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3194 - accuracy: 0.6016\n",
      "Epoch 259: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.3068 - accuracy: 0.5610 - val_loss: 0.6829 - val_accuracy: 0.8683\n",
      "Epoch 260/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4578 - accuracy: 0.4766\n",
      "Epoch 260: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3599 - accuracy: 0.5223 - val_loss: 0.6832 - val_accuracy: 0.8782\n",
      "Epoch 261/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3510 - accuracy: 0.5234\n",
      "Epoch 261: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3389 - accuracy: 0.5223 - val_loss: 0.6825 - val_accuracy: 0.8703\n",
      "Epoch 262/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1560 - accuracy: 0.5781\n",
      "Epoch 262: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2863 - accuracy: 0.5490 - val_loss: 0.6802 - val_accuracy: 0.8563\n",
      "Epoch 263/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2172 - accuracy: 0.6016\n",
      "Epoch 263: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3076 - accuracy: 0.5403 - val_loss: 0.6755 - val_accuracy: 0.8703\n",
      "Epoch 264/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1728 - accuracy: 0.6016\n",
      "Epoch 264: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3067 - accuracy: 0.5416 - val_loss: 0.6738 - val_accuracy: 0.8643\n",
      "Epoch 265/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1592 - accuracy: 0.6016\n",
      "Epoch 265: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3076 - accuracy: 0.5630 - val_loss: 0.6787 - val_accuracy: 0.8643\n",
      "Epoch 266/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5620 - accuracy: 0.5625\n",
      "Epoch 266: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2852 - accuracy: 0.5556 - val_loss: 0.6757 - val_accuracy: 0.8603\n",
      "Epoch 267/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3176 - accuracy: 0.6016\n",
      "Epoch 267: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3026 - accuracy: 0.5650 - val_loss: 0.6702 - val_accuracy: 0.8782\n",
      "Epoch 268/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2648 - accuracy: 0.6016\n",
      "Epoch 268: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3204 - accuracy: 0.5436 - val_loss: 0.6740 - val_accuracy: 0.8723\n",
      "Epoch 269/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2607 - accuracy: 0.5703\n",
      "Epoch 269: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2858 - accuracy: 0.5730 - val_loss: 0.6781 - val_accuracy: 0.8543\n",
      "Epoch 270/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3333 - accuracy: 0.5312\n",
      "Epoch 270: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3379 - accuracy: 0.5423 - val_loss: 0.6780 - val_accuracy: 0.8603\n",
      "Epoch 271/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4706 - accuracy: 0.5234\n",
      "Epoch 271: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2995 - accuracy: 0.5410 - val_loss: 0.6755 - val_accuracy: 0.8443\n",
      "Epoch 272/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3329 - accuracy: 0.5078\n",
      "Epoch 272: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2994 - accuracy: 0.5456 - val_loss: 0.6715 - val_accuracy: 0.8523\n",
      "Epoch 273/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3350 - accuracy: 0.5547\n",
      "Epoch 273: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3382 - accuracy: 0.5503 - val_loss: 0.6692 - val_accuracy: 0.8583\n",
      "Epoch 274/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1188 - accuracy: 0.5781\n",
      "Epoch 274: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3158 - accuracy: 0.5456 - val_loss: 0.6708 - val_accuracy: 0.8663\n",
      "Epoch 275/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2638 - accuracy: 0.5703\n",
      "Epoch 275: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3250 - accuracy: 0.5336 - val_loss: 0.6710 - val_accuracy: 0.8663\n",
      "Epoch 276/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2318 - accuracy: 0.5625\n",
      "Epoch 276: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3483 - accuracy: 0.5376 - val_loss: 0.6739 - val_accuracy: 0.8703\n",
      "Epoch 277/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3266 - accuracy: 0.5312\n",
      "Epoch 277: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2910 - accuracy: 0.5516 - val_loss: 0.6713 - val_accuracy: 0.8583\n",
      "Epoch 278/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1283 - accuracy: 0.5938\n",
      "Epoch 278: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2794 - accuracy: 0.5590 - val_loss: 0.6716 - val_accuracy: 0.8463\n",
      "Epoch 279/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3375 - accuracy: 0.5234\n",
      "Epoch 279: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2842 - accuracy: 0.5670 - val_loss: 0.6699 - val_accuracy: 0.8703\n",
      "Epoch 280/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2349 - accuracy: 0.5859\n",
      "Epoch 280: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2856 - accuracy: 0.5616 - val_loss: 0.6637 - val_accuracy: 0.8683\n",
      "Epoch 281/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2158 - accuracy: 0.6016\n",
      "Epoch 281: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2872 - accuracy: 0.5716 - val_loss: 0.6636 - val_accuracy: 0.8663\n",
      "Epoch 282/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3647 - accuracy: 0.5312\n",
      "Epoch 282: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2878 - accuracy: 0.5516 - val_loss: 0.6632 - val_accuracy: 0.8603\n",
      "Epoch 283/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4068 - accuracy: 0.6016\n",
      "Epoch 283: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2820 - accuracy: 0.5750 - val_loss: 0.6642 - val_accuracy: 0.8703\n",
      "Epoch 284/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1841 - accuracy: 0.6016\n",
      "Epoch 284: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2800 - accuracy: 0.5823 - val_loss: 0.6650 - val_accuracy: 0.8583\n",
      "Epoch 285/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3792 - accuracy: 0.5234\n",
      "Epoch 285: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2728 - accuracy: 0.5510 - val_loss: 0.6635 - val_accuracy: 0.8723\n",
      "Epoch 286/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4596 - accuracy: 0.5234\n",
      "Epoch 286: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3200 - accuracy: 0.5530 - val_loss: 0.6639 - val_accuracy: 0.8383\n",
      "Epoch 287/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2750 - accuracy: 0.5469\n",
      "Epoch 287: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2822 - accuracy: 0.5636 - val_loss: 0.6631 - val_accuracy: 0.8343\n",
      "Epoch 288/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2867 - accuracy: 0.5703\n",
      "Epoch 288: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.3203 - accuracy: 0.5570 - val_loss: 0.6602 - val_accuracy: 0.8683\n",
      "Epoch 289/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0962 - accuracy: 0.5938\n",
      "Epoch 289: val_accuracy did not improve from 0.87824\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2976 - accuracy: 0.5443 - val_loss: 0.6570 - val_accuracy: 0.8643\n",
      "Epoch 290/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4714 - accuracy: 0.5703\n",
      "Epoch 290: val_accuracy improved from 0.87824 to 0.88423, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2967 - accuracy: 0.5610 - val_loss: 0.6547 - val_accuracy: 0.8842\n",
      "Epoch 291/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2798 - accuracy: 0.5469\n",
      "Epoch 291: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2446 - accuracy: 0.5903 - val_loss: 0.6552 - val_accuracy: 0.8623\n",
      "Epoch 292/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2526 - accuracy: 0.5859\n",
      "Epoch 292: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3232 - accuracy: 0.5670 - val_loss: 0.6556 - val_accuracy: 0.8483\n",
      "Epoch 293/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3097 - accuracy: 0.5547\n",
      "Epoch 293: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2967 - accuracy: 0.5576 - val_loss: 0.6571 - val_accuracy: 0.8623\n",
      "Epoch 294/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2444 - accuracy: 0.5781\n",
      "Epoch 294: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2898 - accuracy: 0.5696 - val_loss: 0.6521 - val_accuracy: 0.8782\n",
      "Epoch 295/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3531 - accuracy: 0.5625\n",
      "Epoch 295: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2676 - accuracy: 0.5503 - val_loss: 0.6499 - val_accuracy: 0.8623\n",
      "Epoch 296/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4581 - accuracy: 0.5469\n",
      "Epoch 296: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3020 - accuracy: 0.5663 - val_loss: 0.6509 - val_accuracy: 0.8703\n",
      "Epoch 297/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1878 - accuracy: 0.5781\n",
      "Epoch 297: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2429 - accuracy: 0.5703 - val_loss: 0.6508 - val_accuracy: 0.8723\n",
      "Epoch 298/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4349 - accuracy: 0.5703\n",
      "Epoch 298: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2699 - accuracy: 0.5763 - val_loss: 0.6500 - val_accuracy: 0.8703\n",
      "Epoch 299/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3156 - accuracy: 0.5156\n",
      "Epoch 299: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3188 - accuracy: 0.5536 - val_loss: 0.6543 - val_accuracy: 0.8583\n",
      "Epoch 300/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1811 - accuracy: 0.5781\n",
      "Epoch 300: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2537 - accuracy: 0.5883 - val_loss: 0.6483 - val_accuracy: 0.8683\n",
      "Epoch 301/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4046 - accuracy: 0.5000\n",
      "Epoch 301: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2685 - accuracy: 0.5556 - val_loss: 0.6458 - val_accuracy: 0.8723\n",
      "Epoch 302/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3047 - accuracy: 0.5703\n",
      "Epoch 302: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2820 - accuracy: 0.5583 - val_loss: 0.6473 - val_accuracy: 0.8762\n",
      "Epoch 303/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3924 - accuracy: 0.5312\n",
      "Epoch 303: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2764 - accuracy: 0.5650 - val_loss: 0.6510 - val_accuracy: 0.8723\n",
      "Epoch 304/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5379 - accuracy: 0.5234\n",
      "Epoch 304: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2999 - accuracy: 0.5503 - val_loss: 0.6598 - val_accuracy: 0.8503\n",
      "Epoch 305/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4342 - accuracy: 0.4609\n",
      "Epoch 305: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2861 - accuracy: 0.5583 - val_loss: 0.6570 - val_accuracy: 0.8563\n",
      "Epoch 306/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1936 - accuracy: 0.6094\n",
      "Epoch 306: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2642 - accuracy: 0.5663 - val_loss: 0.6496 - val_accuracy: 0.8623\n",
      "Epoch 307/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2519 - accuracy: 0.5547\n",
      "Epoch 307: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2700 - accuracy: 0.5816 - val_loss: 0.6436 - val_accuracy: 0.8663\n",
      "Epoch 308/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5364 - accuracy: 0.4688\n",
      "Epoch 308: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2974 - accuracy: 0.5443 - val_loss: 0.6438 - val_accuracy: 0.8802\n",
      "Epoch 309/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1871 - accuracy: 0.5859\n",
      "Epoch 309: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2420 - accuracy: 0.5756 - val_loss: 0.6434 - val_accuracy: 0.8822\n",
      "Epoch 310/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2594 - accuracy: 0.5469\n",
      "Epoch 310: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2934 - accuracy: 0.5563 - val_loss: 0.6450 - val_accuracy: 0.8663\n",
      "Epoch 311/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1583 - accuracy: 0.5859\n",
      "Epoch 311: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2831 - accuracy: 0.5576 - val_loss: 0.6453 - val_accuracy: 0.8563\n",
      "Epoch 312/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4837 - accuracy: 0.5312\n",
      "Epoch 312: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2843 - accuracy: 0.5750 - val_loss: 0.6448 - val_accuracy: 0.8363\n",
      "Epoch 313/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2726 - accuracy: 0.5156\n",
      "Epoch 313: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3112 - accuracy: 0.5410 - val_loss: 0.6460 - val_accuracy: 0.8703\n",
      "Epoch 314/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3337 - accuracy: 0.5547\n",
      "Epoch 314: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2737 - accuracy: 0.5656 - val_loss: 0.6507 - val_accuracy: 0.8762\n",
      "Epoch 315/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2143 - accuracy: 0.5781\n",
      "Epoch 315: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3489 - accuracy: 0.5430 - val_loss: 0.6493 - val_accuracy: 0.8822\n",
      "Epoch 316/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2107 - accuracy: 0.6250\n",
      "Epoch 316: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2969 - accuracy: 0.5610 - val_loss: 0.6547 - val_accuracy: 0.8603\n",
      "Epoch 317/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1793 - accuracy: 0.6328\n",
      "Epoch 317: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2761 - accuracy: 0.5610 - val_loss: 0.6510 - val_accuracy: 0.8703\n",
      "Epoch 318/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3280 - accuracy: 0.5391\n",
      "Epoch 318: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2495 - accuracy: 0.5716 - val_loss: 0.6439 - val_accuracy: 0.8782\n",
      "Epoch 319/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1954 - accuracy: 0.5469\n",
      "Epoch 319: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2917 - accuracy: 0.5583 - val_loss: 0.6385 - val_accuracy: 0.8603\n",
      "Epoch 320/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2688 - accuracy: 0.5391\n",
      "Epoch 320: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2982 - accuracy: 0.5730 - val_loss: 0.6415 - val_accuracy: 0.8822\n",
      "Epoch 321/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3666 - accuracy: 0.5391\n",
      "Epoch 321: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3268 - accuracy: 0.5616 - val_loss: 0.6427 - val_accuracy: 0.8762\n",
      "Epoch 322/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3038 - accuracy: 0.5469\n",
      "Epoch 322: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2919 - accuracy: 0.5736 - val_loss: 0.6486 - val_accuracy: 0.8643\n",
      "Epoch 323/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3223 - accuracy: 0.5781\n",
      "Epoch 323: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2340 - accuracy: 0.5643 - val_loss: 0.6451 - val_accuracy: 0.8443\n",
      "Epoch 324/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3618 - accuracy: 0.5469\n",
      "Epoch 324: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3199 - accuracy: 0.5623 - val_loss: 0.6482 - val_accuracy: 0.8403\n",
      "Epoch 325/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3176 - accuracy: 0.5469\n",
      "Epoch 325: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3394 - accuracy: 0.5696 - val_loss: 0.6502 - val_accuracy: 0.8663\n",
      "Epoch 326/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0972 - accuracy: 0.6094\n",
      "Epoch 326: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2281 - accuracy: 0.5756 - val_loss: 0.6487 - val_accuracy: 0.8802\n",
      "Epoch 327/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1406 - accuracy: 0.5547\n",
      "Epoch 327: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2699 - accuracy: 0.5630 - val_loss: 0.6459 - val_accuracy: 0.8683\n",
      "Epoch 328/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2544 - accuracy: 0.5703\n",
      "Epoch 328: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2551 - accuracy: 0.5703 - val_loss: 0.6467 - val_accuracy: 0.8403\n",
      "Epoch 329/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2289 - accuracy: 0.5859\n",
      "Epoch 329: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2687 - accuracy: 0.5596 - val_loss: 0.6441 - val_accuracy: 0.8503\n",
      "Epoch 330/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1800 - accuracy: 0.6172\n",
      "Epoch 330: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2732 - accuracy: 0.5636 - val_loss: 0.6411 - val_accuracy: 0.8643\n",
      "Epoch 331/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2169 - accuracy: 0.6172\n",
      "Epoch 331: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2533 - accuracy: 0.5783 - val_loss: 0.6411 - val_accuracy: 0.8603\n",
      "Epoch 332/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2364 - accuracy: 0.5781\n",
      "Epoch 332: val_accuracy did not improve from 0.88423\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2597 - accuracy: 0.5656 - val_loss: 0.6420 - val_accuracy: 0.8523\n",
      "Epoch 333/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2980 - accuracy: 0.5547\n",
      "Epoch 333: val_accuracy improved from 0.88423 to 0.89022, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.2645 - accuracy: 0.5563 - val_loss: 0.6375 - val_accuracy: 0.8902\n",
      "Epoch 334/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0879 - accuracy: 0.5703\n",
      "Epoch 334: val_accuracy did not improve from 0.89022\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2488 - accuracy: 0.5723 - val_loss: 0.6336 - val_accuracy: 0.8902\n",
      "Epoch 335/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4288 - accuracy: 0.5625\n",
      "Epoch 335: val_accuracy did not improve from 0.89022\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2750 - accuracy: 0.5783 - val_loss: 0.6331 - val_accuracy: 0.8723\n",
      "Epoch 336/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2497 - accuracy: 0.5859\n",
      "Epoch 336: val_accuracy did not improve from 0.89022\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2385 - accuracy: 0.5916 - val_loss: 0.6326 - val_accuracy: 0.8623\n",
      "Epoch 337/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2303 - accuracy: 0.5625\n",
      "Epoch 337: val_accuracy did not improve from 0.89022\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2703 - accuracy: 0.5623 - val_loss: 0.6270 - val_accuracy: 0.8862\n",
      "Epoch 338/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2402 - accuracy: 0.5391\n",
      "Epoch 338: val_accuracy improved from 0.89022 to 0.89222, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.2263 - accuracy: 0.5763 - val_loss: 0.6276 - val_accuracy: 0.8922\n",
      "Epoch 339/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1995 - accuracy: 0.5938\n",
      "Epoch 339: val_accuracy did not improve from 0.89222\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2648 - accuracy: 0.5823 - val_loss: 0.6278 - val_accuracy: 0.8822\n",
      "Epoch 340/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0882 - accuracy: 0.6172\n",
      "Epoch 340: val_accuracy did not improve from 0.89222\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2251 - accuracy: 0.5803 - val_loss: 0.6251 - val_accuracy: 0.8723\n",
      "Epoch 341/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2675 - accuracy: 0.5938\n",
      "Epoch 341: val_accuracy did not improve from 0.89222\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2577 - accuracy: 0.5803 - val_loss: 0.6277 - val_accuracy: 0.8862\n",
      "Epoch 342/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1818 - accuracy: 0.5625\n",
      "Epoch 342: val_accuracy did not improve from 0.89222\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2487 - accuracy: 0.5663 - val_loss: 0.6297 - val_accuracy: 0.8703\n",
      "Epoch 343/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0932 - accuracy: 0.6875\n",
      "Epoch 343: val_accuracy improved from 0.89222 to 0.89421, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.2980 - accuracy: 0.5590 - val_loss: 0.6291 - val_accuracy: 0.8942\n",
      "Epoch 344/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2712 - accuracy: 0.5312\n",
      "Epoch 344: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2757 - accuracy: 0.5616 - val_loss: 0.6299 - val_accuracy: 0.8942\n",
      "Epoch 345/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1714 - accuracy: 0.6641\n",
      "Epoch 345: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2533 - accuracy: 0.5703 - val_loss: 0.6379 - val_accuracy: 0.8762\n",
      "Epoch 346/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3363 - accuracy: 0.5000\n",
      "Epoch 346: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2110 - accuracy: 0.5783 - val_loss: 0.6390 - val_accuracy: 0.8723\n",
      "Epoch 347/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2677 - accuracy: 0.5703\n",
      "Epoch 347: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2394 - accuracy: 0.5690 - val_loss: 0.6365 - val_accuracy: 0.8663\n",
      "Epoch 348/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1642 - accuracy: 0.6406\n",
      "Epoch 348: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2632 - accuracy: 0.5803 - val_loss: 0.6304 - val_accuracy: 0.8862\n",
      "Epoch 349/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3634 - accuracy: 0.4766\n",
      "Epoch 349: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2690 - accuracy: 0.5610 - val_loss: 0.6279 - val_accuracy: 0.8743\n",
      "Epoch 350/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1672 - accuracy: 0.5625\n",
      "Epoch 350: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.2722 - accuracy: 0.5663 - val_loss: 0.6267 - val_accuracy: 0.8563\n",
      "Epoch 351/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3774 - accuracy: 0.6016\n",
      "Epoch 351: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2521 - accuracy: 0.5723 - val_loss: 0.6267 - val_accuracy: 0.8683\n",
      "Epoch 352/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3632 - accuracy: 0.5469\n",
      "Epoch 352: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2742 - accuracy: 0.5590 - val_loss: 0.6280 - val_accuracy: 0.8842\n",
      "Epoch 353/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0676 - accuracy: 0.6250\n",
      "Epoch 353: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2974 - accuracy: 0.5616 - val_loss: 0.6253 - val_accuracy: 0.8862\n",
      "Epoch 354/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1351 - accuracy: 0.6250\n",
      "Epoch 354: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2239 - accuracy: 0.5869 - val_loss: 0.6275 - val_accuracy: 0.8743\n",
      "Epoch 355/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2723 - accuracy: 0.5703\n",
      "Epoch 355: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2305 - accuracy: 0.5783 - val_loss: 0.6285 - val_accuracy: 0.8802\n",
      "Epoch 356/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1975 - accuracy: 0.6172\n",
      "Epoch 356: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2345 - accuracy: 0.5750 - val_loss: 0.6268 - val_accuracy: 0.8643\n",
      "Epoch 357/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3294 - accuracy: 0.5000\n",
      "Epoch 357: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2556 - accuracy: 0.5676 - val_loss: 0.6244 - val_accuracy: 0.8543\n",
      "Epoch 358/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0591 - accuracy: 0.6406\n",
      "Epoch 358: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2118 - accuracy: 0.5796 - val_loss: 0.6255 - val_accuracy: 0.8443\n",
      "Epoch 359/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6149 - accuracy: 0.5312\n",
      "Epoch 359: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2555 - accuracy: 0.5783 - val_loss: 0.6254 - val_accuracy: 0.8683\n",
      "Epoch 360/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1966 - accuracy: 0.5703\n",
      "Epoch 360: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2932 - accuracy: 0.5596 - val_loss: 0.6250 - val_accuracy: 0.8683\n",
      "Epoch 361/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2490 - accuracy: 0.5781\n",
      "Epoch 361: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2481 - accuracy: 0.5636 - val_loss: 0.6236 - val_accuracy: 0.8743\n",
      "Epoch 362/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4780 - accuracy: 0.4922\n",
      "Epoch 362: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2370 - accuracy: 0.5763 - val_loss: 0.6208 - val_accuracy: 0.8802\n",
      "Epoch 363/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0780 - accuracy: 0.6328\n",
      "Epoch 363: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2189 - accuracy: 0.5889 - val_loss: 0.6203 - val_accuracy: 0.8782\n",
      "Epoch 364/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2036 - accuracy: 0.5781\n",
      "Epoch 364: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2084 - accuracy: 0.5789 - val_loss: 0.6153 - val_accuracy: 0.8643\n",
      "Epoch 365/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1257 - accuracy: 0.5781\n",
      "Epoch 365: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.2407 - accuracy: 0.5710 - val_loss: 0.6205 - val_accuracy: 0.8643\n",
      "Epoch 366/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1764 - accuracy: 0.6250\n",
      "Epoch 366: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2246 - accuracy: 0.5923 - val_loss: 0.6198 - val_accuracy: 0.8643\n",
      "Epoch 367/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2377 - accuracy: 0.5547\n",
      "Epoch 367: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2344 - accuracy: 0.5650 - val_loss: 0.6161 - val_accuracy: 0.8762\n",
      "Epoch 368/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4453 - accuracy: 0.5547\n",
      "Epoch 368: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2593 - accuracy: 0.5683 - val_loss: 0.6105 - val_accuracy: 0.8623\n",
      "Epoch 369/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2215 - accuracy: 0.5703\n",
      "Epoch 369: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3051 - accuracy: 0.5643 - val_loss: 0.6133 - val_accuracy: 0.8782\n",
      "Epoch 370/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2674 - accuracy: 0.5703\n",
      "Epoch 370: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2343 - accuracy: 0.5849 - val_loss: 0.6122 - val_accuracy: 0.8882\n",
      "Epoch 371/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1227 - accuracy: 0.5625\n",
      "Epoch 371: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2311 - accuracy: 0.5803 - val_loss: 0.6137 - val_accuracy: 0.8703\n",
      "Epoch 372/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0327 - accuracy: 0.6250\n",
      "Epoch 372: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2243 - accuracy: 0.5789 - val_loss: 0.6197 - val_accuracy: 0.8583\n",
      "Epoch 373/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3995 - accuracy: 0.5234\n",
      "Epoch 373: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2809 - accuracy: 0.5523 - val_loss: 0.6232 - val_accuracy: 0.8583\n",
      "Epoch 374/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2735 - accuracy: 0.6016\n",
      "Epoch 374: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2605 - accuracy: 0.5710 - val_loss: 0.6202 - val_accuracy: 0.8802\n",
      "Epoch 375/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2625 - accuracy: 0.6016\n",
      "Epoch 375: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2754 - accuracy: 0.5716 - val_loss: 0.6175 - val_accuracy: 0.8782\n",
      "Epoch 376/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2127 - accuracy: 0.5938\n",
      "Epoch 376: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1999 - accuracy: 0.5849 - val_loss: 0.6119 - val_accuracy: 0.8743\n",
      "Epoch 377/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5365 - accuracy: 0.5625\n",
      "Epoch 377: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2446 - accuracy: 0.5636 - val_loss: 0.6083 - val_accuracy: 0.8862\n",
      "Epoch 378/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.2588 - accuracy: 0.5530\n",
      "Epoch 378: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2588 - accuracy: 0.5530 - val_loss: 0.6165 - val_accuracy: 0.8703\n",
      "Epoch 379/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3867 - accuracy: 0.5078\n",
      "Epoch 379: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3091 - accuracy: 0.5503 - val_loss: 0.6221 - val_accuracy: 0.8782\n",
      "Epoch 380/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3593 - accuracy: 0.5703\n",
      "Epoch 380: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2603 - accuracy: 0.5783 - val_loss: 0.6183 - val_accuracy: 0.8623\n",
      "Epoch 381/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2053 - accuracy: 0.5781\n",
      "Epoch 381: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2200 - accuracy: 0.5690 - val_loss: 0.6137 - val_accuracy: 0.8663\n",
      "Epoch 382/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1830 - accuracy: 0.5547\n",
      "Epoch 382: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2543 - accuracy: 0.5849 - val_loss: 0.6117 - val_accuracy: 0.8443\n",
      "Epoch 383/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1896 - accuracy: 0.5781\n",
      "Epoch 383: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2776 - accuracy: 0.5676 - val_loss: 0.6060 - val_accuracy: 0.8882\n",
      "Epoch 384/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2336 - accuracy: 0.5781\n",
      "Epoch 384: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2610 - accuracy: 0.5730 - val_loss: 0.6091 - val_accuracy: 0.8723\n",
      "Epoch 385/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1043 - accuracy: 0.6719\n",
      "Epoch 385: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2273 - accuracy: 0.5829 - val_loss: 0.6118 - val_accuracy: 0.8663\n",
      "Epoch 386/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4152 - accuracy: 0.5469\n",
      "Epoch 386: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2484 - accuracy: 0.5756 - val_loss: 0.6151 - val_accuracy: 0.8683\n",
      "Epoch 387/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2374 - accuracy: 0.5781\n",
      "Epoch 387: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2459 - accuracy: 0.5623 - val_loss: 0.6152 - val_accuracy: 0.8603\n",
      "Epoch 388/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3303 - accuracy: 0.5391\n",
      "Epoch 388: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2448 - accuracy: 0.5776 - val_loss: 0.6118 - val_accuracy: 0.8723\n",
      "Epoch 389/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2227 - accuracy: 0.6250\n",
      "Epoch 389: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2365 - accuracy: 0.5716 - val_loss: 0.6080 - val_accuracy: 0.8902\n",
      "Epoch 390/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3009 - accuracy: 0.5781\n",
      "Epoch 390: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2198 - accuracy: 0.5823 - val_loss: 0.6051 - val_accuracy: 0.8663\n",
      "Epoch 391/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1897 - accuracy: 0.5781\n",
      "Epoch 391: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2537 - accuracy: 0.5776 - val_loss: 0.6058 - val_accuracy: 0.8802\n",
      "Epoch 392/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2545 - accuracy: 0.4922\n",
      "Epoch 392: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2593 - accuracy: 0.5876 - val_loss: 0.6077 - val_accuracy: 0.8543\n",
      "Epoch 393/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3244 - accuracy: 0.5078\n",
      "Epoch 393: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2487 - accuracy: 0.5776 - val_loss: 0.6050 - val_accuracy: 0.8623\n",
      "Epoch 394/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2509 - accuracy: 0.6406\n",
      "Epoch 394: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2317 - accuracy: 0.5763 - val_loss: 0.6015 - val_accuracy: 0.8862\n",
      "Epoch 395/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2978 - accuracy: 0.6484\n",
      "Epoch 395: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2668 - accuracy: 0.5816 - val_loss: 0.5998 - val_accuracy: 0.8723\n",
      "Epoch 396/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0914 - accuracy: 0.6797\n",
      "Epoch 396: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2103 - accuracy: 0.6029 - val_loss: 0.6023 - val_accuracy: 0.8882\n",
      "Epoch 397/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0565 - accuracy: 0.6406\n",
      "Epoch 397: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2037 - accuracy: 0.5889 - val_loss: 0.6067 - val_accuracy: 0.8782\n",
      "Epoch 398/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3020 - accuracy: 0.5938\n",
      "Epoch 398: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2410 - accuracy: 0.5663 - val_loss: 0.6113 - val_accuracy: 0.8723\n",
      "Epoch 399/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2943 - accuracy: 0.5625\n",
      "Epoch 399: val_accuracy did not improve from 0.89421\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.3054 - accuracy: 0.5536 - val_loss: 0.6045 - val_accuracy: 0.8703\n",
      "Epoch 400/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0785 - accuracy: 0.6094\n",
      "Epoch 400: val_accuracy improved from 0.89421 to 0.89621, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.2127 - accuracy: 0.5823 - val_loss: 0.6002 - val_accuracy: 0.8962\n",
      "Epoch 401/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1908 - accuracy: 0.6016\n",
      "Epoch 401: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2226 - accuracy: 0.5769 - val_loss: 0.6008 - val_accuracy: 0.8822\n",
      "Epoch 402/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1889 - accuracy: 0.5391\n",
      "Epoch 402: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.2010 - accuracy: 0.5756 - val_loss: 0.6022 - val_accuracy: 0.8703\n",
      "Epoch 403/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2922 - accuracy: 0.5625\n",
      "Epoch 403: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2415 - accuracy: 0.5836 - val_loss: 0.6001 - val_accuracy: 0.8603\n",
      "Epoch 404/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1851 - accuracy: 0.6328\n",
      "Epoch 404: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2183 - accuracy: 0.5856 - val_loss: 0.6019 - val_accuracy: 0.8743\n",
      "Epoch 405/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4209 - accuracy: 0.5781\n",
      "Epoch 405: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2761 - accuracy: 0.5789 - val_loss: 0.6065 - val_accuracy: 0.8703\n",
      "Epoch 406/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1634 - accuracy: 0.6016\n",
      "Epoch 406: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2139 - accuracy: 0.5803 - val_loss: 0.6073 - val_accuracy: 0.8802\n",
      "Epoch 407/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4333 - accuracy: 0.5156\n",
      "Epoch 407: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2319 - accuracy: 0.5796 - val_loss: 0.6028 - val_accuracy: 0.8663\n",
      "Epoch 408/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3358 - accuracy: 0.5859\n",
      "Epoch 408: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2222 - accuracy: 0.5696 - val_loss: 0.5984 - val_accuracy: 0.8723\n",
      "Epoch 409/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2314 - accuracy: 0.5469\n",
      "Epoch 409: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2481 - accuracy: 0.5796 - val_loss: 0.5957 - val_accuracy: 0.8822\n",
      "Epoch 410/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3336 - accuracy: 0.5234\n",
      "Epoch 410: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2492 - accuracy: 0.5829 - val_loss: 0.6017 - val_accuracy: 0.8583\n",
      "Epoch 411/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3622 - accuracy: 0.5703\n",
      "Epoch 411: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2360 - accuracy: 0.5736 - val_loss: 0.6044 - val_accuracy: 0.8663\n",
      "Epoch 412/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3593 - accuracy: 0.5625\n",
      "Epoch 412: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2541 - accuracy: 0.5490 - val_loss: 0.6027 - val_accuracy: 0.8822\n",
      "Epoch 413/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2015 - accuracy: 0.6016\n",
      "Epoch 413: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2229 - accuracy: 0.5796 - val_loss: 0.6020 - val_accuracy: 0.8842\n",
      "Epoch 414/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3806 - accuracy: 0.6250\n",
      "Epoch 414: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2238 - accuracy: 0.5936 - val_loss: 0.6031 - val_accuracy: 0.8743\n",
      "Epoch 415/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4662 - accuracy: 0.5000\n",
      "Epoch 415: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2175 - accuracy: 0.5723 - val_loss: 0.6043 - val_accuracy: 0.8463\n",
      "Epoch 416/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1945 - accuracy: 0.6016\n",
      "Epoch 416: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2364 - accuracy: 0.5670 - val_loss: 0.6030 - val_accuracy: 0.8643\n",
      "Epoch 417/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3556 - accuracy: 0.5312\n",
      "Epoch 417: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2358 - accuracy: 0.5683 - val_loss: 0.5993 - val_accuracy: 0.8802\n",
      "Epoch 418/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0982 - accuracy: 0.6172\n",
      "Epoch 418: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2340 - accuracy: 0.5750 - val_loss: 0.5966 - val_accuracy: 0.8723\n",
      "Epoch 419/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2582 - accuracy: 0.5391\n",
      "Epoch 419: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2285 - accuracy: 0.5716 - val_loss: 0.5948 - val_accuracy: 0.8743\n",
      "Epoch 420/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2631 - accuracy: 0.5547\n",
      "Epoch 420: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2619 - accuracy: 0.5789 - val_loss: 0.5980 - val_accuracy: 0.8523\n",
      "Epoch 421/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2002 - accuracy: 0.5781\n",
      "Epoch 421: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2605 - accuracy: 0.5756 - val_loss: 0.6046 - val_accuracy: 0.8643\n",
      "Epoch 422/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3100 - accuracy: 0.5625\n",
      "Epoch 422: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2088 - accuracy: 0.5670 - val_loss: 0.6065 - val_accuracy: 0.8703\n",
      "Epoch 423/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2131 - accuracy: 0.6094\n",
      "Epoch 423: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2418 - accuracy: 0.5789 - val_loss: 0.6077 - val_accuracy: 0.8623\n",
      "Epoch 424/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2794 - accuracy: 0.5625\n",
      "Epoch 424: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2273 - accuracy: 0.5763 - val_loss: 0.6065 - val_accuracy: 0.8623\n",
      "Epoch 425/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1418 - accuracy: 0.5938\n",
      "Epoch 425: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2470 - accuracy: 0.5590 - val_loss: 0.6037 - val_accuracy: 0.8683\n",
      "Epoch 426/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0280 - accuracy: 0.6172\n",
      "Epoch 426: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1826 - accuracy: 0.6103 - val_loss: 0.6036 - val_accuracy: 0.8703\n",
      "Epoch 427/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2150 - accuracy: 0.5625\n",
      "Epoch 427: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2017 - accuracy: 0.5903 - val_loss: 0.5966 - val_accuracy: 0.8842\n",
      "Epoch 428/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1536 - accuracy: 0.6094\n",
      "Epoch 428: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1774 - accuracy: 0.5896 - val_loss: 0.5920 - val_accuracy: 0.8723\n",
      "Epoch 429/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3516 - accuracy: 0.5234\n",
      "Epoch 429: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2489 - accuracy: 0.5750 - val_loss: 0.5933 - val_accuracy: 0.8862\n",
      "Epoch 430/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0861 - accuracy: 0.6484\n",
      "Epoch 430: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2015 - accuracy: 0.5916 - val_loss: 0.5933 - val_accuracy: 0.8723\n",
      "Epoch 431/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3506 - accuracy: 0.5781\n",
      "Epoch 431: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2194 - accuracy: 0.5716 - val_loss: 0.5897 - val_accuracy: 0.8902\n",
      "Epoch 432/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1066 - accuracy: 0.6328\n",
      "Epoch 432: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.1958 - accuracy: 0.5909 - val_loss: 0.5860 - val_accuracy: 0.8743\n",
      "Epoch 433/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2572 - accuracy: 0.5391\n",
      "Epoch 433: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2459 - accuracy: 0.5656 - val_loss: 0.5859 - val_accuracy: 0.8802\n",
      "Epoch 434/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0993 - accuracy: 0.6250\n",
      "Epoch 434: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2250 - accuracy: 0.5923 - val_loss: 0.5849 - val_accuracy: 0.8782\n",
      "Epoch 435/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2425 - accuracy: 0.5156\n",
      "Epoch 435: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2110 - accuracy: 0.5836 - val_loss: 0.5891 - val_accuracy: 0.8483\n",
      "Epoch 436/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0626 - accuracy: 0.6250\n",
      "Epoch 436: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1919 - accuracy: 0.5909 - val_loss: 0.5846 - val_accuracy: 0.8862\n",
      "Epoch 437/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3806 - accuracy: 0.5703\n",
      "Epoch 437: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2038 - accuracy: 0.5883 - val_loss: 0.5884 - val_accuracy: 0.8703\n",
      "Epoch 438/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3203 - accuracy: 0.5625\n",
      "Epoch 438: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2014 - accuracy: 0.5829 - val_loss: 0.5922 - val_accuracy: 0.8603\n",
      "Epoch 439/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0524 - accuracy: 0.6328\n",
      "Epoch 439: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2560 - accuracy: 0.5849 - val_loss: 0.5920 - val_accuracy: 0.8723\n",
      "Epoch 440/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.9761 - accuracy: 0.6641\n",
      "Epoch 440: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.1920 - accuracy: 0.5863 - val_loss: 0.5931 - val_accuracy: 0.8563\n",
      "Epoch 441/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2026 - accuracy: 0.5859\n",
      "Epoch 441: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2076 - accuracy: 0.5843 - val_loss: 0.5958 - val_accuracy: 0.8663\n",
      "Epoch 442/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4141 - accuracy: 0.5156\n",
      "Epoch 442: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2400 - accuracy: 0.5750 - val_loss: 0.5982 - val_accuracy: 0.8743\n",
      "Epoch 443/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1881 - accuracy: 0.5234\n",
      "Epoch 443: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2132 - accuracy: 0.5763 - val_loss: 0.5958 - val_accuracy: 0.8723\n",
      "Epoch 444/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1214 - accuracy: 0.6016\n",
      "Epoch 444: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2400 - accuracy: 0.5683 - val_loss: 0.5910 - val_accuracy: 0.8762\n",
      "Epoch 445/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1915 - accuracy: 0.5703\n",
      "Epoch 445: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2039 - accuracy: 0.5956 - val_loss: 0.5902 - val_accuracy: 0.8782\n",
      "Epoch 446/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1565 - accuracy: 0.5938\n",
      "Epoch 446: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2266 - accuracy: 0.6049 - val_loss: 0.5896 - val_accuracy: 0.8902\n",
      "Epoch 447/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2502 - accuracy: 0.6328\n",
      "Epoch 447: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1943 - accuracy: 0.5896 - val_loss: 0.5822 - val_accuracy: 0.8723\n",
      "Epoch 448/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2724 - accuracy: 0.6250\n",
      "Epoch 448: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2397 - accuracy: 0.5736 - val_loss: 0.5846 - val_accuracy: 0.8862\n",
      "Epoch 449/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1371 - accuracy: 0.5703\n",
      "Epoch 449: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2358 - accuracy: 0.5836 - val_loss: 0.5877 - val_accuracy: 0.8723\n",
      "Epoch 450/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3046 - accuracy: 0.5312\n",
      "Epoch 450: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2016 - accuracy: 0.5763 - val_loss: 0.5881 - val_accuracy: 0.8603\n",
      "Epoch 451/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1321 - accuracy: 0.6562\n",
      "Epoch 451: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2430 - accuracy: 0.5789 - val_loss: 0.5883 - val_accuracy: 0.8743\n",
      "Epoch 452/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3672 - accuracy: 0.5469\n",
      "Epoch 452: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2364 - accuracy: 0.5909 - val_loss: 0.5867 - val_accuracy: 0.8743\n",
      "Epoch 453/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2054 - accuracy: 0.5391\n",
      "Epoch 453: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2068 - accuracy: 0.5896 - val_loss: 0.5861 - val_accuracy: 0.8523\n",
      "Epoch 454/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1166 - accuracy: 0.5703\n",
      "Epoch 454: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2015 - accuracy: 0.5896 - val_loss: 0.5813 - val_accuracy: 0.8743\n",
      "Epoch 455/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2905 - accuracy: 0.5156\n",
      "Epoch 455: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2297 - accuracy: 0.5670 - val_loss: 0.5855 - val_accuracy: 0.8743\n",
      "Epoch 456/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0528 - accuracy: 0.6172\n",
      "Epoch 456: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1967 - accuracy: 0.5876 - val_loss: 0.5859 - val_accuracy: 0.8683\n",
      "Epoch 457/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1419 - accuracy: 0.6094\n",
      "Epoch 457: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2492 - accuracy: 0.5903 - val_loss: 0.5859 - val_accuracy: 0.8862\n",
      "Epoch 458/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.9937 - accuracy: 0.6953\n",
      "Epoch 458: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2354 - accuracy: 0.5730 - val_loss: 0.5842 - val_accuracy: 0.8842\n",
      "Epoch 459/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0256 - accuracy: 0.6406\n",
      "Epoch 459: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2085 - accuracy: 0.5803 - val_loss: 0.5862 - val_accuracy: 0.8623\n",
      "Epoch 460/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2167 - accuracy: 0.5781\n",
      "Epoch 460: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2037 - accuracy: 0.5843 - val_loss: 0.5860 - val_accuracy: 0.8762\n",
      "Epoch 461/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3122 - accuracy: 0.5312\n",
      "Epoch 461: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2377 - accuracy: 0.5650 - val_loss: 0.5893 - val_accuracy: 0.8443\n",
      "Epoch 462/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0902 - accuracy: 0.6094\n",
      "Epoch 462: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2206 - accuracy: 0.5776 - val_loss: 0.5945 - val_accuracy: 0.8762\n",
      "Epoch 463/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1805 - accuracy: 0.6250\n",
      "Epoch 463: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2270 - accuracy: 0.5690 - val_loss: 0.5990 - val_accuracy: 0.8683\n",
      "Epoch 464/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2139 - accuracy: 0.5469\n",
      "Epoch 464: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2093 - accuracy: 0.5883 - val_loss: 0.6027 - val_accuracy: 0.8703\n",
      "Epoch 465/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0697 - accuracy: 0.6406\n",
      "Epoch 465: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2511 - accuracy: 0.5756 - val_loss: 0.5999 - val_accuracy: 0.8563\n",
      "Epoch 466/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2968 - accuracy: 0.5859\n",
      "Epoch 466: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1687 - accuracy: 0.5923 - val_loss: 0.5927 - val_accuracy: 0.8743\n",
      "Epoch 467/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3379 - accuracy: 0.5078\n",
      "Epoch 467: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2425 - accuracy: 0.5690 - val_loss: 0.5891 - val_accuracy: 0.8623\n",
      "Epoch 468/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3464 - accuracy: 0.5391\n",
      "Epoch 468: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2012 - accuracy: 0.5929 - val_loss: 0.5840 - val_accuracy: 0.8703\n",
      "Epoch 469/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1498 - accuracy: 0.5938\n",
      "Epoch 469: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2081 - accuracy: 0.5730 - val_loss: 0.5863 - val_accuracy: 0.8603\n",
      "Epoch 470/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3185 - accuracy: 0.6484\n",
      "Epoch 470: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1897 - accuracy: 0.5989 - val_loss: 0.5836 - val_accuracy: 0.8822\n",
      "Epoch 471/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1368 - accuracy: 0.5703\n",
      "Epoch 471: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1735 - accuracy: 0.5929 - val_loss: 0.5789 - val_accuracy: 0.8862\n",
      "Epoch 472/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5879 - accuracy: 0.5000\n",
      "Epoch 472: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1928 - accuracy: 0.5983 - val_loss: 0.5774 - val_accuracy: 0.8743\n",
      "Epoch 473/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2395 - accuracy: 0.5625\n",
      "Epoch 473: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1792 - accuracy: 0.6056 - val_loss: 0.5800 - val_accuracy: 0.8583\n",
      "Epoch 474/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1824 - accuracy: 0.5547\n",
      "Epoch 474: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1971 - accuracy: 0.5883 - val_loss: 0.5780 - val_accuracy: 0.8723\n",
      "Epoch 475/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3206 - accuracy: 0.5625\n",
      "Epoch 475: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2469 - accuracy: 0.5656 - val_loss: 0.5797 - val_accuracy: 0.8663\n",
      "Epoch 476/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2497 - accuracy: 0.5859\n",
      "Epoch 476: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2342 - accuracy: 0.5856 - val_loss: 0.5845 - val_accuracy: 0.8683\n",
      "Epoch 477/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2475 - accuracy: 0.5938\n",
      "Epoch 477: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2086 - accuracy: 0.5929 - val_loss: 0.5893 - val_accuracy: 0.8643\n",
      "Epoch 478/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0708 - accuracy: 0.5859\n",
      "Epoch 478: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1788 - accuracy: 0.5936 - val_loss: 0.5810 - val_accuracy: 0.8743\n",
      "Epoch 479/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3622 - accuracy: 0.5469\n",
      "Epoch 479: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2379 - accuracy: 0.5876 - val_loss: 0.5792 - val_accuracy: 0.8882\n",
      "Epoch 480/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0783 - accuracy: 0.6328\n",
      "Epoch 480: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.1719 - accuracy: 0.5956 - val_loss: 0.5775 - val_accuracy: 0.8723\n",
      "Epoch 481/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1451 - accuracy: 0.6328\n",
      "Epoch 481: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2056 - accuracy: 0.5903 - val_loss: 0.5750 - val_accuracy: 0.8762\n",
      "Epoch 482/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1331 - accuracy: 0.6484\n",
      "Epoch 482: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2157 - accuracy: 0.5769 - val_loss: 0.5735 - val_accuracy: 0.8703\n",
      "Epoch 483/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2776 - accuracy: 0.5703\n",
      "Epoch 483: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2518 - accuracy: 0.5710 - val_loss: 0.5785 - val_accuracy: 0.8683\n",
      "Epoch 484/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2586 - accuracy: 0.6641\n",
      "Epoch 484: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2342 - accuracy: 0.6009 - val_loss: 0.5833 - val_accuracy: 0.8762\n",
      "Epoch 485/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2380 - accuracy: 0.5547\n",
      "Epoch 485: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2157 - accuracy: 0.5829 - val_loss: 0.5821 - val_accuracy: 0.8743\n",
      "Epoch 486/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2348 - accuracy: 0.6094\n",
      "Epoch 486: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2296 - accuracy: 0.5763 - val_loss: 0.5819 - val_accuracy: 0.8603\n",
      "Epoch 487/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5114 - accuracy: 0.5391\n",
      "Epoch 487: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2496 - accuracy: 0.5956 - val_loss: 0.5862 - val_accuracy: 0.8683\n",
      "Epoch 488/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0770 - accuracy: 0.6016\n",
      "Epoch 488: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1777 - accuracy: 0.5816 - val_loss: 0.5819 - val_accuracy: 0.8723\n",
      "Epoch 489/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0235 - accuracy: 0.6250\n",
      "Epoch 489: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2169 - accuracy: 0.6023 - val_loss: 0.5797 - val_accuracy: 0.8723\n",
      "Epoch 490/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2670 - accuracy: 0.6094\n",
      "Epoch 490: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2274 - accuracy: 0.5723 - val_loss: 0.5791 - val_accuracy: 0.8683\n",
      "Epoch 491/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3135 - accuracy: 0.5938\n",
      "Epoch 491: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2216 - accuracy: 0.6003 - val_loss: 0.5782 - val_accuracy: 0.8822\n",
      "Epoch 492/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1288 - accuracy: 0.6328\n",
      "Epoch 492: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1747 - accuracy: 0.6029 - val_loss: 0.5756 - val_accuracy: 0.8643\n",
      "Epoch 493/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0948 - accuracy: 0.6328\n",
      "Epoch 493: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2047 - accuracy: 0.5923 - val_loss: 0.5764 - val_accuracy: 0.8523\n",
      "Epoch 494/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2204 - accuracy: 0.5469\n",
      "Epoch 494: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2219 - accuracy: 0.5829 - val_loss: 0.5790 - val_accuracy: 0.8762\n",
      "Epoch 495/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1042 - accuracy: 0.6406\n",
      "Epoch 495: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1789 - accuracy: 0.5969 - val_loss: 0.5771 - val_accuracy: 0.8822\n",
      "Epoch 496/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3105 - accuracy: 0.5781\n",
      "Epoch 496: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1655 - accuracy: 0.5996 - val_loss: 0.5746 - val_accuracy: 0.8762\n",
      "Epoch 497/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1884 - accuracy: 0.5781\n",
      "Epoch 497: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.1748 - accuracy: 0.6069 - val_loss: 0.5789 - val_accuracy: 0.8563\n",
      "Epoch 498/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3168 - accuracy: 0.5469\n",
      "Epoch 498: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2266 - accuracy: 0.5823 - val_loss: 0.5785 - val_accuracy: 0.8723\n",
      "Epoch 499/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1770 - accuracy: 0.6016\n",
      "Epoch 499: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1834 - accuracy: 0.5929 - val_loss: 0.5732 - val_accuracy: 0.8703\n",
      "Epoch 500/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0538 - accuracy: 0.5938\n",
      "Epoch 500: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2173 - accuracy: 0.5923 - val_loss: 0.5729 - val_accuracy: 0.8842\n",
      "Epoch 501/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1558 - accuracy: 0.5781\n",
      "Epoch 501: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1973 - accuracy: 0.5816 - val_loss: 0.5732 - val_accuracy: 0.8782\n",
      "Epoch 502/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0643 - accuracy: 0.5938\n",
      "Epoch 502: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1733 - accuracy: 0.5989 - val_loss: 0.5723 - val_accuracy: 0.8683\n",
      "Epoch 503/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1870 - accuracy: 0.5625\n",
      "Epoch 503: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2026 - accuracy: 0.5849 - val_loss: 0.5748 - val_accuracy: 0.8802\n",
      "Epoch 504/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1816 - accuracy: 0.5391\n",
      "Epoch 504: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1865 - accuracy: 0.5929 - val_loss: 0.5750 - val_accuracy: 0.8683\n",
      "Epoch 505/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2113 - accuracy: 0.5781\n",
      "Epoch 505: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1723 - accuracy: 0.5736 - val_loss: 0.5739 - val_accuracy: 0.8583\n",
      "Epoch 506/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3815 - accuracy: 0.5234\n",
      "Epoch 506: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2311 - accuracy: 0.5876 - val_loss: 0.5738 - val_accuracy: 0.8842\n",
      "Epoch 507/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1012 - accuracy: 0.6562\n",
      "Epoch 507: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.1926 - accuracy: 0.5863 - val_loss: 0.5767 - val_accuracy: 0.8762\n",
      "Epoch 508/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0962 - accuracy: 0.6328\n",
      "Epoch 508: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2109 - accuracy: 0.5949 - val_loss: 0.5756 - val_accuracy: 0.8782\n",
      "Epoch 509/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2904 - accuracy: 0.5938\n",
      "Epoch 509: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2102 - accuracy: 0.5963 - val_loss: 0.5820 - val_accuracy: 0.8503\n",
      "Epoch 510/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1945 - accuracy: 0.6172\n",
      "Epoch 510: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1849 - accuracy: 0.5856 - val_loss: 0.5824 - val_accuracy: 0.8543\n",
      "Epoch 511/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1872 - accuracy: 0.5859\n",
      "Epoch 511: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1927 - accuracy: 0.5736 - val_loss: 0.5770 - val_accuracy: 0.8723\n",
      "Epoch 512/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2317 - accuracy: 0.5625\n",
      "Epoch 512: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2083 - accuracy: 0.5776 - val_loss: 0.5729 - val_accuracy: 0.8842\n",
      "Epoch 513/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0919 - accuracy: 0.6406\n",
      "Epoch 513: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2283 - accuracy: 0.5796 - val_loss: 0.5772 - val_accuracy: 0.8703\n",
      "Epoch 514/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3171 - accuracy: 0.5625\n",
      "Epoch 514: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1984 - accuracy: 0.6076 - val_loss: 0.5763 - val_accuracy: 0.8842\n",
      "Epoch 515/1000\n",
      "10/12 [========================>.....] - ETA: 0s - loss: 1.1631 - accuracy: 0.6008\n",
      "Epoch 515: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.1559 - accuracy: 0.6036 - val_loss: 0.5721 - val_accuracy: 0.8762\n",
      "Epoch 516/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1291 - accuracy: 0.5859\n",
      "Epoch 516: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1781 - accuracy: 0.5876 - val_loss: 0.5743 - val_accuracy: 0.8743\n",
      "Epoch 517/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6606 - accuracy: 0.5781\n",
      "Epoch 517: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2036 - accuracy: 0.6016 - val_loss: 0.5765 - val_accuracy: 0.8563\n",
      "Epoch 518/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1405 - accuracy: 0.6328\n",
      "Epoch 518: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2171 - accuracy: 0.5856 - val_loss: 0.5722 - val_accuracy: 0.8683\n",
      "Epoch 519/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0416 - accuracy: 0.6406\n",
      "Epoch 519: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1952 - accuracy: 0.5989 - val_loss: 0.5699 - val_accuracy: 0.8762\n",
      "Epoch 520/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1797 - accuracy: 0.6406\n",
      "Epoch 520: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2265 - accuracy: 0.5829 - val_loss: 0.5713 - val_accuracy: 0.8623\n",
      "Epoch 521/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1469 - accuracy: 0.5859\n",
      "Epoch 521: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2220 - accuracy: 0.5903 - val_loss: 0.5790 - val_accuracy: 0.8663\n",
      "Epoch 522/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3478 - accuracy: 0.5156\n",
      "Epoch 522: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1827 - accuracy: 0.6003 - val_loss: 0.5765 - val_accuracy: 0.8583\n",
      "Epoch 523/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1916 - accuracy: 0.5859\n",
      "Epoch 523: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1488 - accuracy: 0.6109 - val_loss: 0.5679 - val_accuracy: 0.8743\n",
      "Epoch 524/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1764 - accuracy: 0.6250\n",
      "Epoch 524: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2158 - accuracy: 0.5963 - val_loss: 0.5670 - val_accuracy: 0.8762\n",
      "Epoch 525/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2195 - accuracy: 0.6016\n",
      "Epoch 525: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1833 - accuracy: 0.5983 - val_loss: 0.5657 - val_accuracy: 0.8623\n",
      "Epoch 526/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2654 - accuracy: 0.5859\n",
      "Epoch 526: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2046 - accuracy: 0.5869 - val_loss: 0.5653 - val_accuracy: 0.8463\n",
      "Epoch 527/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3342 - accuracy: 0.5234\n",
      "Epoch 527: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2330 - accuracy: 0.5730 - val_loss: 0.5670 - val_accuracy: 0.8663\n",
      "Epoch 528/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2429 - accuracy: 0.5469\n",
      "Epoch 528: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1897 - accuracy: 0.5956 - val_loss: 0.5709 - val_accuracy: 0.8443\n",
      "Epoch 529/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0190 - accuracy: 0.6094\n",
      "Epoch 529: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2093 - accuracy: 0.5843 - val_loss: 0.5701 - val_accuracy: 0.8603\n",
      "Epoch 530/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3752 - accuracy: 0.5781\n",
      "Epoch 530: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2217 - accuracy: 0.5863 - val_loss: 0.5698 - val_accuracy: 0.8563\n",
      "Epoch 531/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2569 - accuracy: 0.5859\n",
      "Epoch 531: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1988 - accuracy: 0.5949 - val_loss: 0.5698 - val_accuracy: 0.8503\n",
      "Epoch 532/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1366 - accuracy: 0.5938\n",
      "Epoch 532: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2176 - accuracy: 0.5736 - val_loss: 0.5695 - val_accuracy: 0.8503\n",
      "Epoch 533/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1631 - accuracy: 0.5938\n",
      "Epoch 533: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2255 - accuracy: 0.5756 - val_loss: 0.5681 - val_accuracy: 0.8643\n",
      "Epoch 534/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1923 - accuracy: 0.5156\n",
      "Epoch 534: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2136 - accuracy: 0.5630 - val_loss: 0.5732 - val_accuracy: 0.8503\n",
      "Epoch 535/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3149 - accuracy: 0.5938\n",
      "Epoch 535: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1875 - accuracy: 0.6029 - val_loss: 0.5647 - val_accuracy: 0.8563\n",
      "Epoch 536/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2374 - accuracy: 0.5938\n",
      "Epoch 536: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2037 - accuracy: 0.6023 - val_loss: 0.5644 - val_accuracy: 0.8603\n",
      "Epoch 537/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0925 - accuracy: 0.6094\n",
      "Epoch 537: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1815 - accuracy: 0.5929 - val_loss: 0.5700 - val_accuracy: 0.8463\n",
      "Epoch 538/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1084 - accuracy: 0.6016\n",
      "Epoch 538: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1832 - accuracy: 0.5829 - val_loss: 0.5656 - val_accuracy: 0.8603\n",
      "Epoch 539/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2716 - accuracy: 0.5469\n",
      "Epoch 539: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2439 - accuracy: 0.5776 - val_loss: 0.5692 - val_accuracy: 0.8663\n",
      "Epoch 540/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0982 - accuracy: 0.5703\n",
      "Epoch 540: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1768 - accuracy: 0.5869 - val_loss: 0.5653 - val_accuracy: 0.8663\n",
      "Epoch 541/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1397 - accuracy: 0.5703\n",
      "Epoch 541: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2166 - accuracy: 0.5896 - val_loss: 0.5690 - val_accuracy: 0.8603\n",
      "Epoch 542/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2383 - accuracy: 0.5234\n",
      "Epoch 542: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2106 - accuracy: 0.5656 - val_loss: 0.5695 - val_accuracy: 0.8643\n",
      "Epoch 543/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0914 - accuracy: 0.5938\n",
      "Epoch 543: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1929 - accuracy: 0.5829 - val_loss: 0.5686 - val_accuracy: 0.8743\n",
      "Epoch 544/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0220 - accuracy: 0.5781\n",
      "Epoch 544: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1886 - accuracy: 0.5863 - val_loss: 0.5704 - val_accuracy: 0.8663\n",
      "Epoch 545/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5511 - accuracy: 0.4688\n",
      "Epoch 545: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2348 - accuracy: 0.5530 - val_loss: 0.5694 - val_accuracy: 0.8743\n",
      "Epoch 546/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3239 - accuracy: 0.5547\n",
      "Epoch 546: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2033 - accuracy: 0.5849 - val_loss: 0.5717 - val_accuracy: 0.8623\n",
      "Epoch 547/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4636 - accuracy: 0.5156\n",
      "Epoch 547: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2271 - accuracy: 0.5836 - val_loss: 0.5765 - val_accuracy: 0.8623\n",
      "Epoch 548/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0603 - accuracy: 0.6250\n",
      "Epoch 548: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1873 - accuracy: 0.6009 - val_loss: 0.5688 - val_accuracy: 0.8723\n",
      "Epoch 549/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2265 - accuracy: 0.5938\n",
      "Epoch 549: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1927 - accuracy: 0.5883 - val_loss: 0.5717 - val_accuracy: 0.8563\n",
      "Epoch 550/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0877 - accuracy: 0.5859\n",
      "Epoch 550: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.2117 - accuracy: 0.5743 - val_loss: 0.5699 - val_accuracy: 0.8583\n",
      "Epoch 551/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.8856 - accuracy: 0.7188\n",
      "Epoch 551: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1971 - accuracy: 0.5963 - val_loss: 0.5666 - val_accuracy: 0.8703\n",
      "Epoch 552/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2979 - accuracy: 0.5391\n",
      "Epoch 552: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1825 - accuracy: 0.6009 - val_loss: 0.5649 - val_accuracy: 0.8782\n",
      "Epoch 553/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1760 - accuracy: 0.5703\n",
      "Epoch 553: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1736 - accuracy: 0.6063 - val_loss: 0.5646 - val_accuracy: 0.8743\n",
      "Epoch 554/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2954 - accuracy: 0.5469\n",
      "Epoch 554: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1985 - accuracy: 0.5909 - val_loss: 0.5639 - val_accuracy: 0.8683\n",
      "Epoch 555/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2079 - accuracy: 0.5625\n",
      "Epoch 555: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2046 - accuracy: 0.5690 - val_loss: 0.5619 - val_accuracy: 0.8683\n",
      "Epoch 556/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2594 - accuracy: 0.6016\n",
      "Epoch 556: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1800 - accuracy: 0.5976 - val_loss: 0.5590 - val_accuracy: 0.8683\n",
      "Epoch 557/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3092 - accuracy: 0.5391\n",
      "Epoch 557: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1958 - accuracy: 0.5996 - val_loss: 0.5578 - val_accuracy: 0.8463\n",
      "Epoch 558/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0733 - accuracy: 0.5703\n",
      "Epoch 558: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1553 - accuracy: 0.6036 - val_loss: 0.5563 - val_accuracy: 0.8523\n",
      "Epoch 559/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1454 - accuracy: 0.6406\n",
      "Epoch 559: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1699 - accuracy: 0.5869 - val_loss: 0.5559 - val_accuracy: 0.8643\n",
      "Epoch 560/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2380 - accuracy: 0.5312\n",
      "Epoch 560: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1957 - accuracy: 0.5823 - val_loss: 0.5559 - val_accuracy: 0.8523\n",
      "Epoch 561/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1972 - accuracy: 0.5703\n",
      "Epoch 561: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1721 - accuracy: 0.5969 - val_loss: 0.5605 - val_accuracy: 0.8623\n",
      "Epoch 562/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.9877 - accuracy: 0.6172\n",
      "Epoch 562: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1768 - accuracy: 0.6089 - val_loss: 0.5613 - val_accuracy: 0.8723\n",
      "Epoch 563/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1722 - accuracy: 0.6641\n",
      "Epoch 563: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.1699 - accuracy: 0.6009 - val_loss: 0.5621 - val_accuracy: 0.8623\n",
      "Epoch 564/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2640 - accuracy: 0.5781\n",
      "Epoch 564: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1863 - accuracy: 0.6083 - val_loss: 0.5646 - val_accuracy: 0.8583\n",
      "Epoch 565/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2581 - accuracy: 0.5469\n",
      "Epoch 565: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1975 - accuracy: 0.5883 - val_loss: 0.5664 - val_accuracy: 0.8683\n",
      "Epoch 566/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1365 - accuracy: 0.6172\n",
      "Epoch 566: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1721 - accuracy: 0.5876 - val_loss: 0.5655 - val_accuracy: 0.8723\n",
      "Epoch 567/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1477 - accuracy: 0.6016\n",
      "Epoch 567: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2392 - accuracy: 0.5703 - val_loss: 0.5641 - val_accuracy: 0.8762\n",
      "Epoch 568/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2560 - accuracy: 0.5078\n",
      "Epoch 568: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1501 - accuracy: 0.6009 - val_loss: 0.5594 - val_accuracy: 0.8583\n",
      "Epoch 569/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1468 - accuracy: 0.5859\n",
      "Epoch 569: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1685 - accuracy: 0.5976 - val_loss: 0.5572 - val_accuracy: 0.8583\n",
      "Epoch 570/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1903 - accuracy: 0.5781\n",
      "Epoch 570: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1756 - accuracy: 0.5863 - val_loss: 0.5543 - val_accuracy: 0.8743\n",
      "Epoch 571/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2179 - accuracy: 0.6094\n",
      "Epoch 571: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1758 - accuracy: 0.5949 - val_loss: 0.5551 - val_accuracy: 0.8543\n",
      "Epoch 572/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2740 - accuracy: 0.5547\n",
      "Epoch 572: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1593 - accuracy: 0.5876 - val_loss: 0.5582 - val_accuracy: 0.8802\n",
      "Epoch 573/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1201 - accuracy: 0.6094\n",
      "Epoch 573: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2073 - accuracy: 0.5816 - val_loss: 0.5612 - val_accuracy: 0.8643\n",
      "Epoch 574/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3368 - accuracy: 0.5312\n",
      "Epoch 574: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2330 - accuracy: 0.5836 - val_loss: 0.5581 - val_accuracy: 0.8862\n",
      "Epoch 575/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1552 - accuracy: 0.5625\n",
      "Epoch 575: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1710 - accuracy: 0.5949 - val_loss: 0.5554 - val_accuracy: 0.8703\n",
      "Epoch 576/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.9880 - accuracy: 0.6250\n",
      "Epoch 576: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2201 - accuracy: 0.5863 - val_loss: 0.5540 - val_accuracy: 0.8663\n",
      "Epoch 577/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1804 - accuracy: 0.5938\n",
      "Epoch 577: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1331 - accuracy: 0.6123 - val_loss: 0.5545 - val_accuracy: 0.8762\n",
      "Epoch 578/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2416 - accuracy: 0.5938\n",
      "Epoch 578: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1337 - accuracy: 0.6003 - val_loss: 0.5563 - val_accuracy: 0.8703\n",
      "Epoch 579/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1219 - accuracy: 0.6406\n",
      "Epoch 579: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1705 - accuracy: 0.5883 - val_loss: 0.5597 - val_accuracy: 0.8683\n",
      "Epoch 580/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0303 - accuracy: 0.6484\n",
      "Epoch 580: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1623 - accuracy: 0.5916 - val_loss: 0.5635 - val_accuracy: 0.8703\n",
      "Epoch 581/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1364 - accuracy: 0.6094\n",
      "Epoch 581: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1951 - accuracy: 0.5883 - val_loss: 0.5646 - val_accuracy: 0.8523\n",
      "Epoch 582/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.9524 - accuracy: 0.6953\n",
      "Epoch 582: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1737 - accuracy: 0.5883 - val_loss: 0.5636 - val_accuracy: 0.8782\n",
      "Epoch 583/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0170 - accuracy: 0.6484\n",
      "Epoch 583: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1905 - accuracy: 0.5829 - val_loss: 0.5652 - val_accuracy: 0.8703\n",
      "Epoch 584/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1001 - accuracy: 0.5312\n",
      "Epoch 584: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.2126 - accuracy: 0.5883 - val_loss: 0.5720 - val_accuracy: 0.8603\n",
      "Epoch 585/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0771 - accuracy: 0.6250\n",
      "Epoch 585: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1726 - accuracy: 0.6029 - val_loss: 0.5631 - val_accuracy: 0.8643\n",
      "Epoch 586/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0725 - accuracy: 0.6250\n",
      "Epoch 586: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1984 - accuracy: 0.5876 - val_loss: 0.5623 - val_accuracy: 0.8543\n",
      "Epoch 587/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0550 - accuracy: 0.6094\n",
      "Epoch 587: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1612 - accuracy: 0.5969 - val_loss: 0.5548 - val_accuracy: 0.8862\n",
      "Epoch 588/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2724 - accuracy: 0.5938\n",
      "Epoch 588: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1856 - accuracy: 0.5929 - val_loss: 0.5551 - val_accuracy: 0.8683\n",
      "Epoch 589/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.9827 - accuracy: 0.6562\n",
      "Epoch 589: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1457 - accuracy: 0.5923 - val_loss: 0.5568 - val_accuracy: 0.8483\n",
      "Epoch 590/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2192 - accuracy: 0.5938\n",
      "Epoch 590: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1804 - accuracy: 0.6036 - val_loss: 0.5580 - val_accuracy: 0.8463\n",
      "Epoch 591/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.0150 - accuracy: 0.6406\n",
      "Epoch 591: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1932 - accuracy: 0.5936 - val_loss: 0.5556 - val_accuracy: 0.8743\n",
      "Epoch 592/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2096 - accuracy: 0.5859\n",
      "Epoch 592: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1781 - accuracy: 0.5836 - val_loss: 0.5591 - val_accuracy: 0.8603\n",
      "Epoch 593/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1983 - accuracy: 0.5703\n",
      "Epoch 593: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1557 - accuracy: 0.5989 - val_loss: 0.5611 - val_accuracy: 0.8463\n",
      "Epoch 594/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2345 - accuracy: 0.6328\n",
      "Epoch 594: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.1855 - accuracy: 0.5956 - val_loss: 0.5570 - val_accuracy: 0.8683\n",
      "Epoch 595/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2425 - accuracy: 0.6484\n",
      "Epoch 595: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1986 - accuracy: 0.5969 - val_loss: 0.5599 - val_accuracy: 0.8543\n",
      "Epoch 596/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.1322 - accuracy: 0.6016\n",
      "Epoch 596: val_accuracy did not improve from 0.89621\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2699 - accuracy: 0.5690 - val_loss: 0.5625 - val_accuracy: 0.8802\n",
      "Epoch 596: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2aee59f9d50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.8802\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0098663   0.956963    0.0150986   0.21915998  0.7659997  -0.1676402\n",
      "   0.17950019  0.46948805 -0.40810955 -0.09960476  0.16587618 -0.5145438\n",
      "  -0.38834637 -0.05185343 -0.36437097  0.2299003  -0.03018218 -0.10236242\n",
      "   0.29828066 -0.33802134 -0.19002743  0.3285288  -0.588163   -0.24315976\n",
      "   0.33095878 -0.8111216  -0.34757483  0.02448051 -0.08161858  0.05007136\n",
      "   0.00879964 -0.49580288  0.05305604  0.00406761 -0.7452922  -0.05046683\n",
      "  -0.0179214  -1.         -0.2633567  -0.1619485   0.01194787  0.06245303\n",
      "  -0.30150482 -0.32077593 -0.06538994 -0.34234872 -0.5413856  -0.268708\n",
      "  -0.4024452  -0.7566281  -0.49470243 -0.35141537  0.19640154  0.08709393\n",
      "  -0.29578707  0.0897393  -0.0870325  -0.22131918  0.18603012 -0.2634412\n",
      "  -0.18556915  0.32598954 -0.36380878]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([x_test[24]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step\n",
      "[2.11022680e-07 2.42909332e-10 2.41467473e-03 4.56409702e-11\n",
      " 4.32692422e-03 3.78331861e-05 4.74308997e-01 2.44904004e-06\n",
      " 7.03862213e-08 2.29243710e-07 2.61498068e-09 9.21803340e-03\n",
      " 1.09319299e-05 6.84196588e-10 1.73475128e-05 6.79478092e-07\n",
      " 4.45434716e-06 2.61270528e-04 5.95142945e-12 1.66451797e-09\n",
      " 2.35975021e-05 8.28178926e-15 1.21481335e-05 1.12498221e-07\n",
      " 2.46837749e-06 1.48055897e-05 2.76823044e-07 5.02909813e-03\n",
      " 1.94412042e-08 3.24051683e-08 7.22212996e-03 1.39584765e-03\n",
      " 4.95689511e-01 1.96596989e-06 9.04534114e-10 3.83570796e-06]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "predict_result = model.predict(np.array([x_test[24]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.43      0.43         7\n",
      "           1       0.95      1.00      0.97        18\n",
      "           2       0.62      0.80      0.70        10\n",
      "           3       1.00      1.00      1.00        19\n",
      "           4       0.96      0.96      0.96        24\n",
      "           5       0.96      0.96      0.96        23\n",
      "           6       0.85      0.85      0.85        20\n",
      "           7       1.00      1.00      1.00        16\n",
      "           8       0.85      1.00      0.92        17\n",
      "           9       1.00      1.00      1.00        25\n",
      "          10       0.80      0.57      0.67         7\n",
      "          11       1.00      1.00      1.00        15\n",
      "          12       0.75      0.86      0.80        14\n",
      "          13       0.93      1.00      0.96        13\n",
      "          14       1.00      0.50      0.67        10\n",
      "          15       1.00      1.00      1.00        20\n",
      "          16       1.00      1.00      1.00        16\n",
      "          17       1.00      1.00      1.00        12\n",
      "          18       1.00      1.00      1.00        10\n",
      "          19       1.00      0.86      0.92         7\n",
      "          20       1.00      1.00      1.00        18\n",
      "          21       1.00      1.00      1.00        19\n",
      "          22       0.86      0.86      0.86         7\n",
      "          23       0.62      1.00      0.77         5\n",
      "          24       0.60      0.46      0.52        13\n",
      "          25       0.94      1.00      0.97        16\n",
      "          26       0.83      0.83      0.83         6\n",
      "          27       0.89      0.94      0.92        18\n",
      "          28       0.86      0.75      0.80         8\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.92      0.67      0.77        18\n",
      "          31       0.67      0.71      0.69        14\n",
      "          32       0.71      0.77      0.74        13\n",
      "          33       0.79      0.92      0.85        12\n",
      "          34       0.89      0.94      0.92        18\n",
      "          35       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.90       501\n",
      "   macro avg       0.85      0.85      0.84       501\n",
      "weighted avg       0.90      0.90      0.89       501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
