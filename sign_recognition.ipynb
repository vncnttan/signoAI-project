{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"./model/kp_classifier.csv\"\n",
    "model_save_path = \"./model/kp_classifier.hdf5\"\n",
    "tflite_save_path = \"./model/kp_classifier.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt(dataset, delimiter=\",\", dtype='float32', skiprows=1, usecols=list(range(1, (21 * 2) + 1)))\n",
    "y = np.loadtxt(dataset, delimiter=\",\", dtype='int32', skiprows=1, usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.75, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_2 (Dropout)         (None, 42)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 20)                860       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                210       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 36)                396       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1466 (5.73 KB)\n",
      "Trainable params: 1466 (5.73 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False, save_best_only=True, monitor='val_accuracy', mode='max'\n",
    ")\n",
    "\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/12 [=>............................] - ETA: 11s - loss: 3.5764 - accuracy: 0.0625\n",
      "Epoch 1: val_accuracy improved from -inf to 0.04391, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 1s 26ms/step - loss: 3.5888 - accuracy: 0.0366 - val_loss: 3.5422 - val_accuracy: 0.0439\n",
      "Epoch 2/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.5355 - accuracy: 0.0625\n",
      "Epoch 2: val_accuracy improved from 0.04391 to 0.07385, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.5640 - accuracy: 0.0493 - val_loss: 3.5163 - val_accuracy: 0.0739\n",
      "Epoch 3/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.5734 - accuracy: 0.0156\n",
      "Epoch 3: val_accuracy improved from 0.07385 to 0.08782, saving model to ./model\\kp_classifier.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 7ms/step - loss: 3.5420 - accuracy: 0.0526 - val_loss: 3.4941 - val_accuracy: 0.0878\n",
      "Epoch 4/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.4777 - accuracy: 0.0703\n",
      "Epoch 4: val_accuracy improved from 0.08782 to 0.09381, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3.5133 - accuracy: 0.0553 - val_loss: 3.4711 - val_accuracy: 0.0938\n",
      "Epoch 5/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.5236 - accuracy: 0.0859\n",
      "Epoch 5: val_accuracy improved from 0.09381 to 0.11377, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3.4838 - accuracy: 0.0806 - val_loss: 3.4426 - val_accuracy: 0.1138\n",
      "Epoch 6/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.4720 - accuracy: 0.0859\n",
      "Epoch 6: val_accuracy improved from 0.11377 to 0.13373, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.4530 - accuracy: 0.0913 - val_loss: 3.4098 - val_accuracy: 0.1337\n",
      "Epoch 7/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.4290 - accuracy: 0.1094\n",
      "Epoch 7: val_accuracy improved from 0.13373 to 0.14371, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.4333 - accuracy: 0.0846 - val_loss: 3.3744 - val_accuracy: 0.1437\n",
      "Epoch 8/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.4000 - accuracy: 0.0938\n",
      "Epoch 8: val_accuracy improved from 0.14371 to 0.15369, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3.3963 - accuracy: 0.0913 - val_loss: 3.3330 - val_accuracy: 0.1537\n",
      "Epoch 9/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.3572 - accuracy: 0.1484\n",
      "Epoch 9: val_accuracy improved from 0.15369 to 0.16966, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.3545 - accuracy: 0.1013 - val_loss: 3.2850 - val_accuracy: 0.1697\n",
      "Epoch 10/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.3574 - accuracy: 0.0859\n",
      "Epoch 10: val_accuracy did not improve from 0.16966\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3.3020 - accuracy: 0.1166 - val_loss: 3.2326 - val_accuracy: 0.1657\n",
      "Epoch 11/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.3120 - accuracy: 0.0859\n",
      "Epoch 11: val_accuracy improved from 0.16966 to 0.19960, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3.2524 - accuracy: 0.1166 - val_loss: 3.1784 - val_accuracy: 0.1996\n",
      "Epoch 12/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.2107 - accuracy: 0.1562\n",
      "Epoch 12: val_accuracy improved from 0.19960 to 0.21956, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.2001 - accuracy: 0.1233 - val_loss: 3.1178 - val_accuracy: 0.2196\n",
      "Epoch 13/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.0765 - accuracy: 0.1172\n",
      "Epoch 13: val_accuracy improved from 0.21956 to 0.22555, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.1519 - accuracy: 0.1193 - val_loss: 3.0580 - val_accuracy: 0.2255\n",
      "Epoch 14/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.2166 - accuracy: 0.1016\n",
      "Epoch 14: val_accuracy improved from 0.22555 to 0.22954, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.1302 - accuracy: 0.1292 - val_loss: 3.0040 - val_accuracy: 0.2295\n",
      "Epoch 15/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.1955 - accuracy: 0.1172\n",
      "Epoch 15: val_accuracy improved from 0.22954 to 0.23553, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3.0726 - accuracy: 0.1519 - val_loss: 2.9480 - val_accuracy: 0.2355\n",
      "Epoch 16/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 3.0408 - accuracy: 0.1641\n",
      "Epoch 16: val_accuracy improved from 0.23553 to 0.24551, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3.0211 - accuracy: 0.1592 - val_loss: 2.8880 - val_accuracy: 0.2455\n",
      "Epoch 17/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.9992 - accuracy: 0.1641\n",
      "Epoch 17: val_accuracy improved from 0.24551 to 0.26148, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.9498 - accuracy: 0.1732 - val_loss: 2.8251 - val_accuracy: 0.2615\n",
      "Epoch 18/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.8705 - accuracy: 0.2031\n",
      "Epoch 18: val_accuracy improved from 0.26148 to 0.28343, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.9323 - accuracy: 0.1679 - val_loss: 2.7593 - val_accuracy: 0.2834\n",
      "Epoch 19/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.9311 - accuracy: 0.1641\n",
      "Epoch 19: val_accuracy improved from 0.28343 to 0.29741, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 2.8806 - accuracy: 0.1812 - val_loss: 2.6973 - val_accuracy: 0.2974\n",
      "Epoch 20/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.8625 - accuracy: 0.2344\n",
      "Epoch 20: val_accuracy improved from 0.29741 to 0.31936, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.8171 - accuracy: 0.1899 - val_loss: 2.6350 - val_accuracy: 0.3194\n",
      "Epoch 21/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.8780 - accuracy: 0.1172\n",
      "Epoch 21: val_accuracy improved from 0.31936 to 0.32335, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.7968 - accuracy: 0.1825 - val_loss: 2.5743 - val_accuracy: 0.3234\n",
      "Epoch 22/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.7646 - accuracy: 0.1641\n",
      "Epoch 22: val_accuracy improved from 0.32335 to 0.32735, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.7318 - accuracy: 0.1905 - val_loss: 2.5152 - val_accuracy: 0.3273\n",
      "Epoch 23/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.7313 - accuracy: 0.1953\n",
      "Epoch 23: val_accuracy improved from 0.32735 to 0.33333, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2.7024 - accuracy: 0.2085 - val_loss: 2.4583 - val_accuracy: 0.3333\n",
      "Epoch 24/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.7120 - accuracy: 0.1797\n",
      "Epoch 24: val_accuracy improved from 0.33333 to 0.35529, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 2.6467 - accuracy: 0.2072 - val_loss: 2.4002 - val_accuracy: 0.3553\n",
      "Epoch 25/1000\n",
      "10/12 [========================>.....] - ETA: 0s - loss: 2.6264 - accuracy: 0.2094\n",
      "Epoch 25: val_accuracy improved from 0.35529 to 0.35729, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 2.6093 - accuracy: 0.2205 - val_loss: 2.3448 - val_accuracy: 0.3573\n",
      "Epoch 26/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.6713 - accuracy: 0.1875\n",
      "Epoch 26: val_accuracy did not improve from 0.35729\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.6061 - accuracy: 0.2112 - val_loss: 2.2937 - val_accuracy: 0.3533\n",
      "Epoch 27/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.4794 - accuracy: 0.3047\n",
      "Epoch 27: val_accuracy improved from 0.35729 to 0.35928, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.5268 - accuracy: 0.2405 - val_loss: 2.2396 - val_accuracy: 0.3593\n",
      "Epoch 28/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.5512 - accuracy: 0.2500\n",
      "Epoch 28: val_accuracy improved from 0.35928 to 0.36128, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.4942 - accuracy: 0.2418 - val_loss: 2.1921 - val_accuracy: 0.3613\n",
      "Epoch 29/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.5871 - accuracy: 0.1641\n",
      "Epoch 29: val_accuracy did not improve from 0.36128\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.4560 - accuracy: 0.2552 - val_loss: 2.1487 - val_accuracy: 0.3613\n",
      "Epoch 30/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.3167 - accuracy: 0.2891\n",
      "Epoch 30: val_accuracy improved from 0.36128 to 0.36926, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.4570 - accuracy: 0.2298 - val_loss: 2.1073 - val_accuracy: 0.3693\n",
      "Epoch 31/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2755 - accuracy: 0.3359\n",
      "Epoch 31: val_accuracy improved from 0.36926 to 0.37126, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.4004 - accuracy: 0.2525 - val_loss: 2.0639 - val_accuracy: 0.3713\n",
      "Epoch 32/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.5512 - accuracy: 0.1641\n",
      "Epoch 32: val_accuracy improved from 0.37126 to 0.37725, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.3952 - accuracy: 0.2352 - val_loss: 2.0273 - val_accuracy: 0.3772\n",
      "Epoch 33/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.4589 - accuracy: 0.2422\n",
      "Epoch 33: val_accuracy improved from 0.37725 to 0.38323, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.3670 - accuracy: 0.2558 - val_loss: 1.9926 - val_accuracy: 0.3832\n",
      "Epoch 34/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.3602 - accuracy: 0.2578\n",
      "Epoch 34: val_accuracy improved from 0.38323 to 0.43713, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.3120 - accuracy: 0.2712 - val_loss: 1.9567 - val_accuracy: 0.4371\n",
      "Epoch 35/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2834 - accuracy: 0.3359\n",
      "Epoch 35: val_accuracy did not improve from 0.43713\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.3077 - accuracy: 0.2905 - val_loss: 1.9198 - val_accuracy: 0.4291\n",
      "Epoch 36/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.3136 - accuracy: 0.2969\n",
      "Epoch 36: val_accuracy did not improve from 0.43713\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.2908 - accuracy: 0.2825 - val_loss: 1.8901 - val_accuracy: 0.4212\n",
      "Epoch 37/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.3362 - accuracy: 0.2109\n",
      "Epoch 37: val_accuracy did not improve from 0.43713\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.3104 - accuracy: 0.2658 - val_loss: 1.8672 - val_accuracy: 0.4172\n",
      "Epoch 38/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.3893 - accuracy: 0.2266\n",
      "Epoch 38: val_accuracy improved from 0.43713 to 0.44311, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.2465 - accuracy: 0.2738 - val_loss: 1.8435 - val_accuracy: 0.4431\n",
      "Epoch 39/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2363 - accuracy: 0.3047\n",
      "Epoch 39: val_accuracy improved from 0.44311 to 0.45908, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.2432 - accuracy: 0.2818 - val_loss: 1.8158 - val_accuracy: 0.4591\n",
      "Epoch 40/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2216 - accuracy: 0.2891\n",
      "Epoch 40: val_accuracy improved from 0.45908 to 0.46108, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.2710 - accuracy: 0.2958 - val_loss: 1.7937 - val_accuracy: 0.4611\n",
      "Epoch 41/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2122 - accuracy: 0.2969\n",
      "Epoch 41: val_accuracy improved from 0.46108 to 0.46906, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.2117 - accuracy: 0.2945 - val_loss: 1.7737 - val_accuracy: 0.4691\n",
      "Epoch 42/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2392 - accuracy: 0.2891\n",
      "Epoch 42: val_accuracy improved from 0.46906 to 0.48902, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.1932 - accuracy: 0.3025 - val_loss: 1.7506 - val_accuracy: 0.4890\n",
      "Epoch 43/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1570 - accuracy: 0.3047\n",
      "Epoch 43: val_accuracy did not improve from 0.48902\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.2017 - accuracy: 0.2938 - val_loss: 1.7381 - val_accuracy: 0.4671\n",
      "Epoch 44/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2649 - accuracy: 0.2891\n",
      "Epoch 44: val_accuracy improved from 0.48902 to 0.49102, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.1755 - accuracy: 0.3038 - val_loss: 1.7216 - val_accuracy: 0.4910\n",
      "Epoch 45/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0722 - accuracy: 0.3438\n",
      "Epoch 45: val_accuracy did not improve from 0.49102\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.1395 - accuracy: 0.3078 - val_loss: 1.7072 - val_accuracy: 0.4790\n",
      "Epoch 46/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0543 - accuracy: 0.3438\n",
      "Epoch 46: val_accuracy did not improve from 0.49102\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.1477 - accuracy: 0.3198 - val_loss: 1.6879 - val_accuracy: 0.4711\n",
      "Epoch 47/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1532 - accuracy: 0.3203\n",
      "Epoch 47: val_accuracy did not improve from 0.49102\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.1300 - accuracy: 0.3165 - val_loss: 1.6741 - val_accuracy: 0.4850\n",
      "Epoch 48/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0217 - accuracy: 0.2969\n",
      "Epoch 48: val_accuracy improved from 0.49102 to 0.49900, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.1069 - accuracy: 0.3158 - val_loss: 1.6589 - val_accuracy: 0.4990\n",
      "Epoch 49/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9048 - accuracy: 0.3516\n",
      "Epoch 49: val_accuracy did not improve from 0.49900\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.0565 - accuracy: 0.3371 - val_loss: 1.6374 - val_accuracy: 0.4950\n",
      "Epoch 50/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2474 - accuracy: 0.2578\n",
      "Epoch 50: val_accuracy improved from 0.49900 to 0.51297, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.1078 - accuracy: 0.3205 - val_loss: 1.6233 - val_accuracy: 0.5130\n",
      "Epoch 51/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9856 - accuracy: 0.4062\n",
      "Epoch 51: val_accuracy did not improve from 0.51297\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.0783 - accuracy: 0.3225 - val_loss: 1.6146 - val_accuracy: 0.5050\n",
      "Epoch 52/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0420 - accuracy: 0.3438\n",
      "Epoch 52: val_accuracy did not improve from 0.51297\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.1116 - accuracy: 0.3105 - val_loss: 1.6071 - val_accuracy: 0.5070\n",
      "Epoch 53/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9328 - accuracy: 0.3984\n",
      "Epoch 53: val_accuracy improved from 0.51297 to 0.53293, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.0544 - accuracy: 0.3404 - val_loss: 1.5926 - val_accuracy: 0.5329\n",
      "Epoch 54/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0759 - accuracy: 0.3359\n",
      "Epoch 54: val_accuracy did not improve from 0.53293\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.0604 - accuracy: 0.3225 - val_loss: 1.5814 - val_accuracy: 0.5210\n",
      "Epoch 55/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0482 - accuracy: 0.3359\n",
      "Epoch 55: val_accuracy did not improve from 0.53293\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.0650 - accuracy: 0.3271 - val_loss: 1.5752 - val_accuracy: 0.5250\n",
      "Epoch 56/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1295 - accuracy: 0.3047\n",
      "Epoch 56: val_accuracy improved from 0.53293 to 0.54092, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.0531 - accuracy: 0.3198 - val_loss: 1.5643 - val_accuracy: 0.5409\n",
      "Epoch 57/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.2562 - accuracy: 0.2734\n",
      "Epoch 57: val_accuracy did not improve from 0.54092\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 2.0144 - accuracy: 0.3551 - val_loss: 1.5498 - val_accuracy: 0.5349\n",
      "Epoch 58/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0022 - accuracy: 0.4062\n",
      "Epoch 58: val_accuracy did not improve from 0.54092\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.0086 - accuracy: 0.3491 - val_loss: 1.5354 - val_accuracy: 0.5369\n",
      "Epoch 59/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8594 - accuracy: 0.3750\n",
      "Epoch 59: val_accuracy improved from 0.54092 to 0.55289, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 2.0025 - accuracy: 0.3511 - val_loss: 1.5317 - val_accuracy: 0.5529\n",
      "Epoch 60/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0718 - accuracy: 0.3203\n",
      "Epoch 60: val_accuracy improved from 0.55289 to 0.56487, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 2.0251 - accuracy: 0.3318 - val_loss: 1.5237 - val_accuracy: 0.5649\n",
      "Epoch 61/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9751 - accuracy: 0.3359\n",
      "Epoch 61: val_accuracy did not improve from 0.56487\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.9952 - accuracy: 0.3424 - val_loss: 1.5149 - val_accuracy: 0.5569\n",
      "Epoch 62/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9804 - accuracy: 0.3516\n",
      "Epoch 62: val_accuracy did not improve from 0.56487\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.0185 - accuracy: 0.3484 - val_loss: 1.5024 - val_accuracy: 0.5649\n",
      "Epoch 63/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0781 - accuracy: 0.3906\n",
      "Epoch 63: val_accuracy did not improve from 0.56487\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 2.0104 - accuracy: 0.3571 - val_loss: 1.5008 - val_accuracy: 0.5529\n",
      "Epoch 64/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9503 - accuracy: 0.3594\n",
      "Epoch 64: val_accuracy did not improve from 0.56487\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9658 - accuracy: 0.3644 - val_loss: 1.4978 - val_accuracy: 0.5529\n",
      "Epoch 65/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9112 - accuracy: 0.3594\n",
      "Epoch 65: val_accuracy improved from 0.56487 to 0.58283, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.9495 - accuracy: 0.3638 - val_loss: 1.4805 - val_accuracy: 0.5828\n",
      "Epoch 66/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9563 - accuracy: 0.3438\n",
      "Epoch 66: val_accuracy did not improve from 0.58283\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9445 - accuracy: 0.3611 - val_loss: 1.4647 - val_accuracy: 0.5709\n",
      "Epoch 67/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.1148 - accuracy: 0.4219\n",
      "Epoch 67: val_accuracy did not improve from 0.58283\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.9660 - accuracy: 0.3698 - val_loss: 1.4567 - val_accuracy: 0.5749\n",
      "Epoch 68/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0747 - accuracy: 0.3047\n",
      "Epoch 68: val_accuracy did not improve from 0.58283\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9851 - accuracy: 0.3604 - val_loss: 1.4559 - val_accuracy: 0.5669\n",
      "Epoch 69/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9509 - accuracy: 0.3906\n",
      "Epoch 69: val_accuracy did not improve from 0.58283\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.9367 - accuracy: 0.3631 - val_loss: 1.4511 - val_accuracy: 0.5788\n",
      "Epoch 70/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9203 - accuracy: 0.4141\n",
      "Epoch 70: val_accuracy did not improve from 0.58283\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9474 - accuracy: 0.3498 - val_loss: 1.4429 - val_accuracy: 0.5828\n",
      "Epoch 71/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9788 - accuracy: 0.3594\n",
      "Epoch 71: val_accuracy did not improve from 0.58283\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9121 - accuracy: 0.3771 - val_loss: 1.4349 - val_accuracy: 0.5669\n",
      "Epoch 72/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0830 - accuracy: 0.3750\n",
      "Epoch 72: val_accuracy improved from 0.58283 to 0.59880, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.9473 - accuracy: 0.3764 - val_loss: 1.4285 - val_accuracy: 0.5988\n",
      "Epoch 73/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8011 - accuracy: 0.4453\n",
      "Epoch 73: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.9438 - accuracy: 0.3777 - val_loss: 1.4267 - val_accuracy: 0.5908\n",
      "Epoch 74/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9796 - accuracy: 0.3047\n",
      "Epoch 74: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9829 - accuracy: 0.3624 - val_loss: 1.4216 - val_accuracy: 0.5768\n",
      "Epoch 75/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8645 - accuracy: 0.3594\n",
      "Epoch 75: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9113 - accuracy: 0.3724 - val_loss: 1.4147 - val_accuracy: 0.5788\n",
      "Epoch 76/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8654 - accuracy: 0.3906\n",
      "Epoch 76: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8935 - accuracy: 0.3837 - val_loss: 1.4076 - val_accuracy: 0.5788\n",
      "Epoch 77/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8204 - accuracy: 0.3984\n",
      "Epoch 77: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9732 - accuracy: 0.3451 - val_loss: 1.4014 - val_accuracy: 0.5908\n",
      "Epoch 78/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9651 - accuracy: 0.3594\n",
      "Epoch 78: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9038 - accuracy: 0.3691 - val_loss: 1.4002 - val_accuracy: 0.5848\n",
      "Epoch 79/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8714 - accuracy: 0.3906\n",
      "Epoch 79: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9040 - accuracy: 0.3738 - val_loss: 1.3962 - val_accuracy: 0.5828\n",
      "Epoch 80/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8167 - accuracy: 0.4297\n",
      "Epoch 80: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8922 - accuracy: 0.3831 - val_loss: 1.3889 - val_accuracy: 0.5948\n",
      "Epoch 81/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9393 - accuracy: 0.3672\n",
      "Epoch 81: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9329 - accuracy: 0.3578 - val_loss: 1.3775 - val_accuracy: 0.5848\n",
      "Epoch 82/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0296 - accuracy: 0.3359\n",
      "Epoch 82: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.9255 - accuracy: 0.3644 - val_loss: 1.3793 - val_accuracy: 0.5988\n",
      "Epoch 83/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7476 - accuracy: 0.4453\n",
      "Epoch 83: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8908 - accuracy: 0.3738 - val_loss: 1.3765 - val_accuracy: 0.5868\n",
      "Epoch 84/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7803 - accuracy: 0.3672\n",
      "Epoch 84: val_accuracy did not improve from 0.59880\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8830 - accuracy: 0.3877 - val_loss: 1.3693 - val_accuracy: 0.5908\n",
      "Epoch 85/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7306 - accuracy: 0.3984\n",
      "Epoch 85: val_accuracy improved from 0.59880 to 0.62675, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8906 - accuracy: 0.3731 - val_loss: 1.3643 - val_accuracy: 0.6267\n",
      "Epoch 86/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9317 - accuracy: 0.3906\n",
      "Epoch 86: val_accuracy did not improve from 0.62675\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8514 - accuracy: 0.4004 - val_loss: 1.3587 - val_accuracy: 0.5928\n",
      "Epoch 87/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0286 - accuracy: 0.3594\n",
      "Epoch 87: val_accuracy improved from 0.62675 to 0.63273, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8934 - accuracy: 0.3644 - val_loss: 1.3562 - val_accuracy: 0.6327\n",
      "Epoch 88/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9622 - accuracy: 0.3359\n",
      "Epoch 88: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.8785 - accuracy: 0.3611 - val_loss: 1.3547 - val_accuracy: 0.6088\n",
      "Epoch 89/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8639 - accuracy: 0.3594\n",
      "Epoch 89: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8704 - accuracy: 0.3698 - val_loss: 1.3522 - val_accuracy: 0.5928\n",
      "Epoch 90/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8510 - accuracy: 0.3672\n",
      "Epoch 90: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8746 - accuracy: 0.3704 - val_loss: 1.3428 - val_accuracy: 0.5868\n",
      "Epoch 91/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8490 - accuracy: 0.3594\n",
      "Epoch 91: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8464 - accuracy: 0.3804 - val_loss: 1.3371 - val_accuracy: 0.6068\n",
      "Epoch 92/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7610 - accuracy: 0.4531\n",
      "Epoch 92: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8766 - accuracy: 0.3817 - val_loss: 1.3396 - val_accuracy: 0.6028\n",
      "Epoch 93/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8191 - accuracy: 0.4375\n",
      "Epoch 93: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8374 - accuracy: 0.4031 - val_loss: 1.3335 - val_accuracy: 0.6068\n",
      "Epoch 94/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7419 - accuracy: 0.3750\n",
      "Epoch 94: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8387 - accuracy: 0.4044 - val_loss: 1.3212 - val_accuracy: 0.6267\n",
      "Epoch 95/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9512 - accuracy: 0.3906\n",
      "Epoch 95: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8495 - accuracy: 0.3877 - val_loss: 1.3168 - val_accuracy: 0.6048\n",
      "Epoch 96/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6582 - accuracy: 0.4609\n",
      "Epoch 96: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8271 - accuracy: 0.3924 - val_loss: 1.3109 - val_accuracy: 0.6307\n",
      "Epoch 97/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8582 - accuracy: 0.3438\n",
      "Epoch 97: val_accuracy did not improve from 0.63273\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8546 - accuracy: 0.3837 - val_loss: 1.3092 - val_accuracy: 0.6168\n",
      "Epoch 98/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0789 - accuracy: 0.3359\n",
      "Epoch 98: val_accuracy improved from 0.63273 to 0.64072, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.8517 - accuracy: 0.3937 - val_loss: 1.3070 - val_accuracy: 0.6407\n",
      "Epoch 99/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7020 - accuracy: 0.4297\n",
      "Epoch 99: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.8132 - accuracy: 0.3911 - val_loss: 1.3051 - val_accuracy: 0.6307\n",
      "Epoch 100/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8095 - accuracy: 0.4297\n",
      "Epoch 100: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8492 - accuracy: 0.3864 - val_loss: 1.3028 - val_accuracy: 0.6287\n",
      "Epoch 101/1000\n",
      "10/12 [========================>.....] - ETA: 0s - loss: 1.8550 - accuracy: 0.3789\n",
      "Epoch 101: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.8554 - accuracy: 0.3871 - val_loss: 1.3028 - val_accuracy: 0.6327\n",
      "Epoch 102/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9710 - accuracy: 0.3281\n",
      "Epoch 102: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8417 - accuracy: 0.3951 - val_loss: 1.2991 - val_accuracy: 0.6208\n",
      "Epoch 103/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7780 - accuracy: 0.3828\n",
      "Epoch 103: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7889 - accuracy: 0.4024 - val_loss: 1.2927 - val_accuracy: 0.6267\n",
      "Epoch 104/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7477 - accuracy: 0.4375\n",
      "Epoch 104: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8555 - accuracy: 0.3871 - val_loss: 1.2874 - val_accuracy: 0.6327\n",
      "Epoch 105/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9002 - accuracy: 0.3594\n",
      "Epoch 105: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.8074 - accuracy: 0.3877 - val_loss: 1.2823 - val_accuracy: 0.6287\n",
      "Epoch 106/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8042 - accuracy: 0.4219\n",
      "Epoch 106: val_accuracy did not improve from 0.64072\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7761 - accuracy: 0.4164 - val_loss: 1.2789 - val_accuracy: 0.6347\n",
      "Epoch 107/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7791 - accuracy: 0.4141\n",
      "Epoch 107: val_accuracy improved from 0.64072 to 0.64271, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8211 - accuracy: 0.4044 - val_loss: 1.2754 - val_accuracy: 0.6427\n",
      "Epoch 108/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9202 - accuracy: 0.3359\n",
      "Epoch 108: val_accuracy did not improve from 0.64271\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.8167 - accuracy: 0.4011 - val_loss: 1.2753 - val_accuracy: 0.6327\n",
      "Epoch 109/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9411 - accuracy: 0.3828\n",
      "Epoch 109: val_accuracy did not improve from 0.64271\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8219 - accuracy: 0.3937 - val_loss: 1.2694 - val_accuracy: 0.6407\n",
      "Epoch 110/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7758 - accuracy: 0.3594\n",
      "Epoch 110: val_accuracy did not improve from 0.64271\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8029 - accuracy: 0.4037 - val_loss: 1.2712 - val_accuracy: 0.6248\n",
      "Epoch 111/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7576 - accuracy: 0.4297\n",
      "Epoch 111: val_accuracy did not improve from 0.64271\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8194 - accuracy: 0.3884 - val_loss: 1.2740 - val_accuracy: 0.6307\n",
      "Epoch 112/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8192 - accuracy: 0.4141\n",
      "Epoch 112: val_accuracy did not improve from 0.64271\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8298 - accuracy: 0.3944 - val_loss: 1.2686 - val_accuracy: 0.6347\n",
      "Epoch 113/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9155 - accuracy: 0.3672\n",
      "Epoch 113: val_accuracy did not improve from 0.64271\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8199 - accuracy: 0.4104 - val_loss: 1.2648 - val_accuracy: 0.6188\n",
      "Epoch 114/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8198 - accuracy: 0.3828\n",
      "Epoch 114: val_accuracy improved from 0.64271 to 0.65269, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7867 - accuracy: 0.4031 - val_loss: 1.2551 - val_accuracy: 0.6527\n",
      "Epoch 115/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7546 - accuracy: 0.3828\n",
      "Epoch 115: val_accuracy did not improve from 0.65269\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8026 - accuracy: 0.3984 - val_loss: 1.2558 - val_accuracy: 0.6307\n",
      "Epoch 116/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9603 - accuracy: 0.3906\n",
      "Epoch 116: val_accuracy improved from 0.65269 to 0.65469, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8033 - accuracy: 0.4044 - val_loss: 1.2521 - val_accuracy: 0.6547\n",
      "Epoch 117/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6183 - accuracy: 0.4766\n",
      "Epoch 117: val_accuracy improved from 0.65469 to 0.65868, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.8009 - accuracy: 0.3931 - val_loss: 1.2478 - val_accuracy: 0.6587\n",
      "Epoch 118/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6779 - accuracy: 0.4609\n",
      "Epoch 118: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7660 - accuracy: 0.3997 - val_loss: 1.2466 - val_accuracy: 0.6487\n",
      "Epoch 119/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9557 - accuracy: 0.3750\n",
      "Epoch 119: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7738 - accuracy: 0.3971 - val_loss: 1.2475 - val_accuracy: 0.6367\n",
      "Epoch 120/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8905 - accuracy: 0.3281\n",
      "Epoch 120: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7869 - accuracy: 0.3984 - val_loss: 1.2428 - val_accuracy: 0.6287\n",
      "Epoch 121/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8289 - accuracy: 0.3984\n",
      "Epoch 121: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.8056 - accuracy: 0.3831 - val_loss: 1.2375 - val_accuracy: 0.6327\n",
      "Epoch 122/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7092 - accuracy: 0.4219\n",
      "Epoch 122: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7864 - accuracy: 0.4084 - val_loss: 1.2347 - val_accuracy: 0.6467\n",
      "Epoch 123/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7786 - accuracy: 0.4141\n",
      "Epoch 123: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7984 - accuracy: 0.3891 - val_loss: 1.2393 - val_accuracy: 0.6547\n",
      "Epoch 124/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8464 - accuracy: 0.3984\n",
      "Epoch 124: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7859 - accuracy: 0.4124 - val_loss: 1.2361 - val_accuracy: 0.6427\n",
      "Epoch 125/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7649 - accuracy: 0.4375\n",
      "Epoch 125: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7270 - accuracy: 0.4417 - val_loss: 1.2265 - val_accuracy: 0.6447\n",
      "Epoch 126/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9175 - accuracy: 0.4375\n",
      "Epoch 126: val_accuracy did not improve from 0.65868\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7746 - accuracy: 0.4144 - val_loss: 1.2241 - val_accuracy: 0.6467\n",
      "Epoch 127/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6641 - accuracy: 0.4609\n",
      "Epoch 127: val_accuracy improved from 0.65868 to 0.66068, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.7466 - accuracy: 0.4324 - val_loss: 1.2183 - val_accuracy: 0.6607\n",
      "Epoch 128/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7681 - accuracy: 0.4297\n",
      "Epoch 128: val_accuracy did not improve from 0.66068\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7621 - accuracy: 0.4244 - val_loss: 1.2142 - val_accuracy: 0.6347\n",
      "Epoch 129/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6324 - accuracy: 0.5078\n",
      "Epoch 129: val_accuracy did not improve from 0.66068\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7480 - accuracy: 0.4237 - val_loss: 1.2122 - val_accuracy: 0.6607\n",
      "Epoch 130/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7405 - accuracy: 0.4609\n",
      "Epoch 130: val_accuracy did not improve from 0.66068\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7433 - accuracy: 0.4151 - val_loss: 1.2161 - val_accuracy: 0.6547\n",
      "Epoch 131/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7561 - accuracy: 0.4219\n",
      "Epoch 131: val_accuracy did not improve from 0.66068\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7167 - accuracy: 0.4324 - val_loss: 1.2154 - val_accuracy: 0.6447\n",
      "Epoch 132/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9525 - accuracy: 0.3203\n",
      "Epoch 132: val_accuracy improved from 0.66068 to 0.66267, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7886 - accuracy: 0.4031 - val_loss: 1.2106 - val_accuracy: 0.6627\n",
      "Epoch 133/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5659 - accuracy: 0.4531\n",
      "Epoch 133: val_accuracy did not improve from 0.66267\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7098 - accuracy: 0.4410 - val_loss: 1.2139 - val_accuracy: 0.6567\n",
      "Epoch 134/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6833 - accuracy: 0.4297\n",
      "Epoch 134: val_accuracy did not improve from 0.66267\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7518 - accuracy: 0.4330 - val_loss: 1.2106 - val_accuracy: 0.6587\n",
      "Epoch 135/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7275 - accuracy: 0.4375\n",
      "Epoch 135: val_accuracy did not improve from 0.66267\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7079 - accuracy: 0.4370 - val_loss: 1.2032 - val_accuracy: 0.6547\n",
      "Epoch 136/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7183 - accuracy: 0.3906\n",
      "Epoch 136: val_accuracy improved from 0.66267 to 0.67066, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7879 - accuracy: 0.3977 - val_loss: 1.2004 - val_accuracy: 0.6707\n",
      "Epoch 137/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 2.0353 - accuracy: 0.3906\n",
      "Epoch 137: val_accuracy did not improve from 0.67066\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7560 - accuracy: 0.4364 - val_loss: 1.2048 - val_accuracy: 0.6627\n",
      "Epoch 138/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6713 - accuracy: 0.4375\n",
      "Epoch 138: val_accuracy did not improve from 0.67066\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7259 - accuracy: 0.4124 - val_loss: 1.2034 - val_accuracy: 0.6387\n",
      "Epoch 139/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6791 - accuracy: 0.4688\n",
      "Epoch 139: val_accuracy did not improve from 0.67066\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7077 - accuracy: 0.4264 - val_loss: 1.1967 - val_accuracy: 0.6447\n",
      "Epoch 140/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7563 - accuracy: 0.4375\n",
      "Epoch 140: val_accuracy did not improve from 0.67066\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7795 - accuracy: 0.4217 - val_loss: 1.1949 - val_accuracy: 0.6707\n",
      "Epoch 141/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5132 - accuracy: 0.5000\n",
      "Epoch 141: val_accuracy did not improve from 0.67066\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7930 - accuracy: 0.4177 - val_loss: 1.1954 - val_accuracy: 0.6707\n",
      "Epoch 142/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6454 - accuracy: 0.4766\n",
      "Epoch 142: val_accuracy improved from 0.67066 to 0.67465, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7498 - accuracy: 0.4177 - val_loss: 1.1937 - val_accuracy: 0.6747\n",
      "Epoch 143/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6196 - accuracy: 0.3906\n",
      "Epoch 143: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7405 - accuracy: 0.4144 - val_loss: 1.1970 - val_accuracy: 0.6607\n",
      "Epoch 144/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7322 - accuracy: 0.4688\n",
      "Epoch 144: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7565 - accuracy: 0.4164 - val_loss: 1.1930 - val_accuracy: 0.6627\n",
      "Epoch 145/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5494 - accuracy: 0.3984\n",
      "Epoch 145: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7289 - accuracy: 0.4144 - val_loss: 1.1898 - val_accuracy: 0.6667\n",
      "Epoch 146/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8269 - accuracy: 0.4141\n",
      "Epoch 146: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6996 - accuracy: 0.4397 - val_loss: 1.1844 - val_accuracy: 0.6647\n",
      "Epoch 147/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8998 - accuracy: 0.4062\n",
      "Epoch 147: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7335 - accuracy: 0.4197 - val_loss: 1.1852 - val_accuracy: 0.6527\n",
      "Epoch 148/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7331 - accuracy: 0.4219\n",
      "Epoch 148: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6965 - accuracy: 0.4390 - val_loss: 1.1819 - val_accuracy: 0.6567\n",
      "Epoch 149/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5801 - accuracy: 0.4375\n",
      "Epoch 149: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.7556 - accuracy: 0.4144 - val_loss: 1.1785 - val_accuracy: 0.6667\n",
      "Epoch 150/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5787 - accuracy: 0.5078\n",
      "Epoch 150: val_accuracy did not improve from 0.67465\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7691 - accuracy: 0.4204 - val_loss: 1.1816 - val_accuracy: 0.6667\n",
      "Epoch 151/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7684 - accuracy: 0.4531\n",
      "Epoch 151: val_accuracy improved from 0.67465 to 0.68064, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7181 - accuracy: 0.4171 - val_loss: 1.1802 - val_accuracy: 0.6806\n",
      "Epoch 152/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7089 - accuracy: 0.4766\n",
      "Epoch 152: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7183 - accuracy: 0.4297 - val_loss: 1.1805 - val_accuracy: 0.6547\n",
      "Epoch 153/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5854 - accuracy: 0.4531\n",
      "Epoch 153: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6947 - accuracy: 0.4337 - val_loss: 1.1749 - val_accuracy: 0.6627\n",
      "Epoch 154/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7697 - accuracy: 0.3750\n",
      "Epoch 154: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7212 - accuracy: 0.4131 - val_loss: 1.1728 - val_accuracy: 0.6806\n",
      "Epoch 155/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7844 - accuracy: 0.4375\n",
      "Epoch 155: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7149 - accuracy: 0.4350 - val_loss: 1.1721 - val_accuracy: 0.6687\n",
      "Epoch 156/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6602 - accuracy: 0.4531\n",
      "Epoch 156: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7530 - accuracy: 0.4211 - val_loss: 1.1728 - val_accuracy: 0.6627\n",
      "Epoch 157/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8029 - accuracy: 0.4219\n",
      "Epoch 157: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7049 - accuracy: 0.4257 - val_loss: 1.1711 - val_accuracy: 0.6707\n",
      "Epoch 158/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6770 - accuracy: 0.4297\n",
      "Epoch 158: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.7111 - accuracy: 0.4171 - val_loss: 1.1664 - val_accuracy: 0.6727\n",
      "Epoch 159/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5961 - accuracy: 0.4375\n",
      "Epoch 159: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7189 - accuracy: 0.4310 - val_loss: 1.1669 - val_accuracy: 0.6687\n",
      "Epoch 160/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7911 - accuracy: 0.3672\n",
      "Epoch 160: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7212 - accuracy: 0.4484 - val_loss: 1.1627 - val_accuracy: 0.6786\n",
      "Epoch 161/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7943 - accuracy: 0.4453\n",
      "Epoch 161: val_accuracy did not improve from 0.68064\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7212 - accuracy: 0.4377 - val_loss: 1.1623 - val_accuracy: 0.6607\n",
      "Epoch 162/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6682 - accuracy: 0.4141\n",
      "Epoch 162: val_accuracy improved from 0.68064 to 0.68663, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.7103 - accuracy: 0.4217 - val_loss: 1.1593 - val_accuracy: 0.6866\n",
      "Epoch 163/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6425 - accuracy: 0.4922\n",
      "Epoch 163: val_accuracy improved from 0.68663 to 0.69661, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.6779 - accuracy: 0.4477 - val_loss: 1.1569 - val_accuracy: 0.6966\n",
      "Epoch 164/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8841 - accuracy: 0.4297\n",
      "Epoch 164: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7168 - accuracy: 0.4244 - val_loss: 1.1556 - val_accuracy: 0.6687\n",
      "Epoch 165/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8242 - accuracy: 0.4375\n",
      "Epoch 165: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6903 - accuracy: 0.4370 - val_loss: 1.1537 - val_accuracy: 0.6766\n",
      "Epoch 166/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8835 - accuracy: 0.3828\n",
      "Epoch 166: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6758 - accuracy: 0.4564 - val_loss: 1.1502 - val_accuracy: 0.6806\n",
      "Epoch 167/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7802 - accuracy: 0.4219\n",
      "Epoch 167: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6752 - accuracy: 0.4490 - val_loss: 1.1437 - val_accuracy: 0.6846\n",
      "Epoch 168/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6689 - accuracy: 0.4141\n",
      "Epoch 168: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.7226 - accuracy: 0.4324 - val_loss: 1.1444 - val_accuracy: 0.6846\n",
      "Epoch 169/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6027 - accuracy: 0.4766\n",
      "Epoch 169: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7001 - accuracy: 0.4137 - val_loss: 1.1458 - val_accuracy: 0.6687\n",
      "Epoch 170/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7260 - accuracy: 0.3750\n",
      "Epoch 170: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6664 - accuracy: 0.4517 - val_loss: 1.1398 - val_accuracy: 0.6846\n",
      "Epoch 171/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6867 - accuracy: 0.4609\n",
      "Epoch 171: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6847 - accuracy: 0.4350 - val_loss: 1.1339 - val_accuracy: 0.6966\n",
      "Epoch 172/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5850 - accuracy: 0.4609\n",
      "Epoch 172: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6768 - accuracy: 0.4417 - val_loss: 1.1382 - val_accuracy: 0.6826\n",
      "Epoch 173/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6365 - accuracy: 0.4453\n",
      "Epoch 173: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6947 - accuracy: 0.4337 - val_loss: 1.1395 - val_accuracy: 0.6846\n",
      "Epoch 174/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6174 - accuracy: 0.4453\n",
      "Epoch 174: val_accuracy did not improve from 0.69661\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6593 - accuracy: 0.4430 - val_loss: 1.1332 - val_accuracy: 0.6866\n",
      "Epoch 175/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6886 - accuracy: 0.4453\n",
      "Epoch 175: val_accuracy improved from 0.69661 to 0.70060, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.6972 - accuracy: 0.4270 - val_loss: 1.1368 - val_accuracy: 0.7006\n",
      "Epoch 176/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7048 - accuracy: 0.4062\n",
      "Epoch 176: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7046 - accuracy: 0.4217 - val_loss: 1.1345 - val_accuracy: 0.6886\n",
      "Epoch 177/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5845 - accuracy: 0.4297\n",
      "Epoch 177: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6352 - accuracy: 0.4577 - val_loss: 1.1319 - val_accuracy: 0.6846\n",
      "Epoch 178/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7712 - accuracy: 0.4219\n",
      "Epoch 178: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6907 - accuracy: 0.4430 - val_loss: 1.1298 - val_accuracy: 0.6747\n",
      "Epoch 179/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4530 - accuracy: 0.4219\n",
      "Epoch 179: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6457 - accuracy: 0.4490 - val_loss: 1.1292 - val_accuracy: 0.6766\n",
      "Epoch 180/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.9750 - accuracy: 0.4062\n",
      "Epoch 180: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6669 - accuracy: 0.4510 - val_loss: 1.1272 - val_accuracy: 0.6806\n",
      "Epoch 181/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7216 - accuracy: 0.4141\n",
      "Epoch 181: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6703 - accuracy: 0.4330 - val_loss: 1.1282 - val_accuracy: 0.6747\n",
      "Epoch 182/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7486 - accuracy: 0.4062\n",
      "Epoch 182: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6725 - accuracy: 0.4390 - val_loss: 1.1295 - val_accuracy: 0.6766\n",
      "Epoch 183/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5419 - accuracy: 0.5000\n",
      "Epoch 183: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6973 - accuracy: 0.4277 - val_loss: 1.1316 - val_accuracy: 0.6926\n",
      "Epoch 184/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5407 - accuracy: 0.4453\n",
      "Epoch 184: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6427 - accuracy: 0.4617 - val_loss: 1.1274 - val_accuracy: 0.6786\n",
      "Epoch 185/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7540 - accuracy: 0.4141\n",
      "Epoch 185: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6315 - accuracy: 0.4444 - val_loss: 1.1199 - val_accuracy: 0.6766\n",
      "Epoch 186/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7113 - accuracy: 0.4297\n",
      "Epoch 186: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6601 - accuracy: 0.4410 - val_loss: 1.1140 - val_accuracy: 0.6826\n",
      "Epoch 187/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8952 - accuracy: 0.3750\n",
      "Epoch 187: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7073 - accuracy: 0.4410 - val_loss: 1.1149 - val_accuracy: 0.6886\n",
      "Epoch 188/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7320 - accuracy: 0.4375\n",
      "Epoch 188: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6774 - accuracy: 0.4310 - val_loss: 1.1183 - val_accuracy: 0.6806\n",
      "Epoch 189/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7321 - accuracy: 0.4453\n",
      "Epoch 189: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6777 - accuracy: 0.4477 - val_loss: 1.1163 - val_accuracy: 0.6786\n",
      "Epoch 190/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5237 - accuracy: 0.5078\n",
      "Epoch 190: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6664 - accuracy: 0.4444 - val_loss: 1.1178 - val_accuracy: 0.6906\n",
      "Epoch 191/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6645 - accuracy: 0.4688\n",
      "Epoch 191: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6802 - accuracy: 0.4444 - val_loss: 1.1162 - val_accuracy: 0.6846\n",
      "Epoch 192/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6118 - accuracy: 0.4453\n",
      "Epoch 192: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6782 - accuracy: 0.4404 - val_loss: 1.1170 - val_accuracy: 0.6866\n",
      "Epoch 193/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7242 - accuracy: 0.3906\n",
      "Epoch 193: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6818 - accuracy: 0.4390 - val_loss: 1.1217 - val_accuracy: 0.6687\n",
      "Epoch 194/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5444 - accuracy: 0.4297\n",
      "Epoch 194: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6578 - accuracy: 0.4577 - val_loss: 1.1203 - val_accuracy: 0.6707\n",
      "Epoch 195/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8057 - accuracy: 0.3984\n",
      "Epoch 195: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6813 - accuracy: 0.4497 - val_loss: 1.1163 - val_accuracy: 0.6806\n",
      "Epoch 196/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5441 - accuracy: 0.5078\n",
      "Epoch 196: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6657 - accuracy: 0.4510 - val_loss: 1.1138 - val_accuracy: 0.6826\n",
      "Epoch 197/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7768 - accuracy: 0.4297\n",
      "Epoch 197: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6551 - accuracy: 0.4410 - val_loss: 1.1089 - val_accuracy: 0.6886\n",
      "Epoch 198/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4601 - accuracy: 0.5000\n",
      "Epoch 198: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6572 - accuracy: 0.4504 - val_loss: 1.1109 - val_accuracy: 0.6806\n",
      "Epoch 199/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6926 - accuracy: 0.4453\n",
      "Epoch 199: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.7279 - accuracy: 0.4350 - val_loss: 1.1095 - val_accuracy: 0.6946\n",
      "Epoch 200/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6919 - accuracy: 0.3906\n",
      "Epoch 200: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6517 - accuracy: 0.4477 - val_loss: 1.1122 - val_accuracy: 0.6926\n",
      "Epoch 201/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5827 - accuracy: 0.4531\n",
      "Epoch 201: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6699 - accuracy: 0.4444 - val_loss: 1.1095 - val_accuracy: 0.6926\n",
      "Epoch 202/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6784 - accuracy: 0.4219\n",
      "Epoch 202: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6915 - accuracy: 0.4370 - val_loss: 1.1085 - val_accuracy: 0.6946\n",
      "Epoch 203/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4741 - accuracy: 0.4844\n",
      "Epoch 203: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5989 - accuracy: 0.4770 - val_loss: 1.1084 - val_accuracy: 0.6946\n",
      "Epoch 204/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6258 - accuracy: 0.4609\n",
      "Epoch 204: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6970 - accuracy: 0.4350 - val_loss: 1.1107 - val_accuracy: 0.6846\n",
      "Epoch 205/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5211 - accuracy: 0.5078\n",
      "Epoch 205: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6299 - accuracy: 0.4504 - val_loss: 1.1089 - val_accuracy: 0.6826\n",
      "Epoch 206/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8044 - accuracy: 0.3750\n",
      "Epoch 206: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6498 - accuracy: 0.4444 - val_loss: 1.1057 - val_accuracy: 0.6946\n",
      "Epoch 207/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5816 - accuracy: 0.4062\n",
      "Epoch 207: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6549 - accuracy: 0.4537 - val_loss: 1.1049 - val_accuracy: 0.7006\n",
      "Epoch 208/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6163 - accuracy: 0.4531\n",
      "Epoch 208: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6460 - accuracy: 0.4457 - val_loss: 1.1033 - val_accuracy: 0.6926\n",
      "Epoch 209/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6705 - accuracy: 0.4453\n",
      "Epoch 209: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6514 - accuracy: 0.4524 - val_loss: 1.1011 - val_accuracy: 0.7006\n",
      "Epoch 210/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5636 - accuracy: 0.4844\n",
      "Epoch 210: val_accuracy did not improve from 0.70060\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6598 - accuracy: 0.4277 - val_loss: 1.0993 - val_accuracy: 0.6926\n",
      "Epoch 211/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8355 - accuracy: 0.3750\n",
      "Epoch 211: val_accuracy improved from 0.70060 to 0.71657, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 1.6996 - accuracy: 0.4277 - val_loss: 1.0967 - val_accuracy: 0.7166\n",
      "Epoch 212/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7302 - accuracy: 0.4062\n",
      "Epoch 212: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6320 - accuracy: 0.4524 - val_loss: 1.0975 - val_accuracy: 0.7066\n",
      "Epoch 213/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5373 - accuracy: 0.5078\n",
      "Epoch 213: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6766 - accuracy: 0.4484 - val_loss: 1.0938 - val_accuracy: 0.7066\n",
      "Epoch 214/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4551 - accuracy: 0.5078\n",
      "Epoch 214: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6580 - accuracy: 0.4337 - val_loss: 1.0884 - val_accuracy: 0.7106\n",
      "Epoch 215/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8838 - accuracy: 0.3672\n",
      "Epoch 215: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6719 - accuracy: 0.4404 - val_loss: 1.0926 - val_accuracy: 0.6986\n",
      "Epoch 216/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4474 - accuracy: 0.5234\n",
      "Epoch 216: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6425 - accuracy: 0.4510 - val_loss: 1.0954 - val_accuracy: 0.6747\n",
      "Epoch 217/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6804 - accuracy: 0.4219\n",
      "Epoch 217: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6310 - accuracy: 0.4544 - val_loss: 1.0913 - val_accuracy: 0.7106\n",
      "Epoch 218/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4984 - accuracy: 0.4375\n",
      "Epoch 218: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6398 - accuracy: 0.4290 - val_loss: 1.0904 - val_accuracy: 0.6926\n",
      "Epoch 219/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5878 - accuracy: 0.4688\n",
      "Epoch 219: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6657 - accuracy: 0.4444 - val_loss: 1.0932 - val_accuracy: 0.6866\n",
      "Epoch 220/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6085 - accuracy: 0.5156\n",
      "Epoch 220: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6272 - accuracy: 0.4597 - val_loss: 1.0934 - val_accuracy: 0.7026\n",
      "Epoch 221/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4642 - accuracy: 0.5391\n",
      "Epoch 221: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6208 - accuracy: 0.4684 - val_loss: 1.0910 - val_accuracy: 0.7006\n",
      "Epoch 222/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6210 - accuracy: 0.4844\n",
      "Epoch 222: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.6176 - accuracy: 0.4577 - val_loss: 1.0906 - val_accuracy: 0.6966\n",
      "Epoch 223/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6703 - accuracy: 0.4141\n",
      "Epoch 223: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6455 - accuracy: 0.4344 - val_loss: 1.0891 - val_accuracy: 0.7006\n",
      "Epoch 224/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7862 - accuracy: 0.4219\n",
      "Epoch 224: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6865 - accuracy: 0.4297 - val_loss: 1.0893 - val_accuracy: 0.7066\n",
      "Epoch 225/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6972 - accuracy: 0.4688\n",
      "Epoch 225: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6309 - accuracy: 0.4584 - val_loss: 1.0880 - val_accuracy: 0.6926\n",
      "Epoch 226/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6395 - accuracy: 0.4141\n",
      "Epoch 226: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6141 - accuracy: 0.4590 - val_loss: 1.0832 - val_accuracy: 0.6986\n",
      "Epoch 227/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6173 - accuracy: 0.4219\n",
      "Epoch 227: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6330 - accuracy: 0.4604 - val_loss: 1.0818 - val_accuracy: 0.7026\n",
      "Epoch 228/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6666 - accuracy: 0.4688\n",
      "Epoch 228: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6438 - accuracy: 0.4464 - val_loss: 1.0778 - val_accuracy: 0.6986\n",
      "Epoch 229/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6411 - accuracy: 0.4062\n",
      "Epoch 229: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6226 - accuracy: 0.4637 - val_loss: 1.0773 - val_accuracy: 0.7126\n",
      "Epoch 230/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6369 - accuracy: 0.4844\n",
      "Epoch 230: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6127 - accuracy: 0.4457 - val_loss: 1.0822 - val_accuracy: 0.7026\n",
      "Epoch 231/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5556 - accuracy: 0.4766\n",
      "Epoch 231: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6281 - accuracy: 0.4484 - val_loss: 1.0818 - val_accuracy: 0.6926\n",
      "Epoch 232/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8829 - accuracy: 0.4141\n",
      "Epoch 232: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6196 - accuracy: 0.4557 - val_loss: 1.0776 - val_accuracy: 0.6986\n",
      "Epoch 233/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6253 - accuracy: 0.5078\n",
      "Epoch 233: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5997 - accuracy: 0.4730 - val_loss: 1.0787 - val_accuracy: 0.7146\n",
      "Epoch 234/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 1.6512 - accuracy: 0.4557\n",
      "Epoch 234: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.6512 - accuracy: 0.4557 - val_loss: 1.0795 - val_accuracy: 0.7046\n",
      "Epoch 235/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5586 - accuracy: 0.5156\n",
      "Epoch 235: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6353 - accuracy: 0.4610 - val_loss: 1.0851 - val_accuracy: 0.6946\n",
      "Epoch 236/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6555 - accuracy: 0.4453\n",
      "Epoch 236: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6448 - accuracy: 0.4497 - val_loss: 1.0833 - val_accuracy: 0.7046\n",
      "Epoch 237/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6045 - accuracy: 0.4219\n",
      "Epoch 237: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6216 - accuracy: 0.4537 - val_loss: 1.0846 - val_accuracy: 0.6946\n",
      "Epoch 238/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6239 - accuracy: 0.4219\n",
      "Epoch 238: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6061 - accuracy: 0.4584 - val_loss: 1.0835 - val_accuracy: 0.7046\n",
      "Epoch 239/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7794 - accuracy: 0.4141\n",
      "Epoch 239: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6194 - accuracy: 0.4657 - val_loss: 1.0780 - val_accuracy: 0.6966\n",
      "Epoch 240/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7382 - accuracy: 0.3828\n",
      "Epoch 240: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.6630 - accuracy: 0.4304 - val_loss: 1.0784 - val_accuracy: 0.7006\n",
      "Epoch 241/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6214 - accuracy: 0.4297\n",
      "Epoch 241: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6327 - accuracy: 0.4584 - val_loss: 1.0744 - val_accuracy: 0.7086\n",
      "Epoch 242/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6686 - accuracy: 0.4375\n",
      "Epoch 242: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.6354 - accuracy: 0.4604 - val_loss: 1.0743 - val_accuracy: 0.7026\n",
      "Epoch 243/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6213 - accuracy: 0.4844\n",
      "Epoch 243: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5855 - accuracy: 0.4430 - val_loss: 1.0728 - val_accuracy: 0.7106\n",
      "Epoch 244/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6344 - accuracy: 0.4062\n",
      "Epoch 244: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6762 - accuracy: 0.4524 - val_loss: 1.0715 - val_accuracy: 0.6946\n",
      "Epoch 245/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7094 - accuracy: 0.3828\n",
      "Epoch 245: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6495 - accuracy: 0.4484 - val_loss: 1.0759 - val_accuracy: 0.6926\n",
      "Epoch 246/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7906 - accuracy: 0.3672\n",
      "Epoch 246: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6283 - accuracy: 0.4597 - val_loss: 1.0724 - val_accuracy: 0.7046\n",
      "Epoch 247/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7374 - accuracy: 0.3672\n",
      "Epoch 247: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5967 - accuracy: 0.4477 - val_loss: 1.0738 - val_accuracy: 0.6946\n",
      "Epoch 248/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6715 - accuracy: 0.3906\n",
      "Epoch 248: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5991 - accuracy: 0.4577 - val_loss: 1.0674 - val_accuracy: 0.6986\n",
      "Epoch 249/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5952 - accuracy: 0.4219\n",
      "Epoch 249: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5965 - accuracy: 0.4670 - val_loss: 1.0680 - val_accuracy: 0.6966\n",
      "Epoch 250/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5377 - accuracy: 0.4844\n",
      "Epoch 250: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5742 - accuracy: 0.4783 - val_loss: 1.0655 - val_accuracy: 0.7006\n",
      "Epoch 251/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4684 - accuracy: 0.4766\n",
      "Epoch 251: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6362 - accuracy: 0.4577 - val_loss: 1.0628 - val_accuracy: 0.6986\n",
      "Epoch 252/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6199 - accuracy: 0.4453\n",
      "Epoch 252: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6285 - accuracy: 0.4584 - val_loss: 1.0656 - val_accuracy: 0.6986\n",
      "Epoch 253/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5751 - accuracy: 0.4922\n",
      "Epoch 253: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6206 - accuracy: 0.4657 - val_loss: 1.0642 - val_accuracy: 0.6966\n",
      "Epoch 254/1000\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 1.6270 - accuracy: 0.4531\n",
      "Epoch 254: val_accuracy did not improve from 0.71657\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.6280 - accuracy: 0.4544 - val_loss: 1.0630 - val_accuracy: 0.6946\n",
      "Epoch 255/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5418 - accuracy: 0.5078\n",
      "Epoch 255: val_accuracy improved from 0.71657 to 0.72056, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6535 - accuracy: 0.4510 - val_loss: 1.0577 - val_accuracy: 0.7206\n",
      "Epoch 256/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6016 - accuracy: 0.4531\n",
      "Epoch 256: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6123 - accuracy: 0.4510 - val_loss: 1.0508 - val_accuracy: 0.7186\n",
      "Epoch 257/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6404 - accuracy: 0.4766\n",
      "Epoch 257: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6247 - accuracy: 0.4737 - val_loss: 1.0553 - val_accuracy: 0.6986\n",
      "Epoch 258/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6546 - accuracy: 0.4922\n",
      "Epoch 258: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6104 - accuracy: 0.4670 - val_loss: 1.0546 - val_accuracy: 0.6906\n",
      "Epoch 259/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8726 - accuracy: 0.4062\n",
      "Epoch 259: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6233 - accuracy: 0.4524 - val_loss: 1.0530 - val_accuracy: 0.7006\n",
      "Epoch 260/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4563 - accuracy: 0.5469\n",
      "Epoch 260: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5587 - accuracy: 0.4803 - val_loss: 1.0530 - val_accuracy: 0.7206\n",
      "Epoch 261/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3631 - accuracy: 0.5625\n",
      "Epoch 261: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6203 - accuracy: 0.4564 - val_loss: 1.0562 - val_accuracy: 0.7006\n",
      "Epoch 262/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5373 - accuracy: 0.4688\n",
      "Epoch 262: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5832 - accuracy: 0.4497 - val_loss: 1.0590 - val_accuracy: 0.6806\n",
      "Epoch 263/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6155 - accuracy: 0.4531\n",
      "Epoch 263: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6174 - accuracy: 0.4584 - val_loss: 1.0513 - val_accuracy: 0.7166\n",
      "Epoch 264/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5644 - accuracy: 0.4922\n",
      "Epoch 264: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5962 - accuracy: 0.4557 - val_loss: 1.0526 - val_accuracy: 0.7186\n",
      "Epoch 265/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3262 - accuracy: 0.5781\n",
      "Epoch 265: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5917 - accuracy: 0.4724 - val_loss: 1.0546 - val_accuracy: 0.7166\n",
      "Epoch 266/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7350 - accuracy: 0.4531\n",
      "Epoch 266: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6111 - accuracy: 0.4584 - val_loss: 1.0579 - val_accuracy: 0.7026\n",
      "Epoch 267/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5787 - accuracy: 0.4609\n",
      "Epoch 267: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6381 - accuracy: 0.4530 - val_loss: 1.0571 - val_accuracy: 0.6966\n",
      "Epoch 268/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6387 - accuracy: 0.4297\n",
      "Epoch 268: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.6220 - accuracy: 0.4524 - val_loss: 1.0536 - val_accuracy: 0.7046\n",
      "Epoch 269/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5542 - accuracy: 0.5000\n",
      "Epoch 269: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5864 - accuracy: 0.4664 - val_loss: 1.0484 - val_accuracy: 0.6926\n",
      "Epoch 270/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5156 - accuracy: 0.4844\n",
      "Epoch 270: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6114 - accuracy: 0.4830 - val_loss: 1.0477 - val_accuracy: 0.7066\n",
      "Epoch 271/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3521 - accuracy: 0.5000\n",
      "Epoch 271: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5772 - accuracy: 0.4717 - val_loss: 1.0507 - val_accuracy: 0.7166\n",
      "Epoch 272/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6767 - accuracy: 0.4844\n",
      "Epoch 272: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5806 - accuracy: 0.4657 - val_loss: 1.0563 - val_accuracy: 0.6986\n",
      "Epoch 273/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5991 - accuracy: 0.4609\n",
      "Epoch 273: val_accuracy did not improve from 0.72056\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6507 - accuracy: 0.4577 - val_loss: 1.0552 - val_accuracy: 0.7106\n",
      "Epoch 274/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5800 - accuracy: 0.4297\n",
      "Epoch 274: val_accuracy improved from 0.72056 to 0.72655, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6261 - accuracy: 0.4557 - val_loss: 1.0527 - val_accuracy: 0.7265\n",
      "Epoch 275/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4018 - accuracy: 0.4609\n",
      "Epoch 275: val_accuracy did not improve from 0.72655\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6233 - accuracy: 0.4477 - val_loss: 1.0526 - val_accuracy: 0.7046\n",
      "Epoch 276/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5812 - accuracy: 0.4219\n",
      "Epoch 276: val_accuracy did not improve from 0.72655\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5807 - accuracy: 0.4604 - val_loss: 1.0485 - val_accuracy: 0.7046\n",
      "Epoch 277/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5540 - accuracy: 0.4531\n",
      "Epoch 277: val_accuracy did not improve from 0.72655\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6034 - accuracy: 0.4830 - val_loss: 1.0450 - val_accuracy: 0.7006\n",
      "Epoch 278/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5465 - accuracy: 0.4844\n",
      "Epoch 278: val_accuracy did not improve from 0.72655\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6181 - accuracy: 0.4750 - val_loss: 1.0433 - val_accuracy: 0.7006\n",
      "Epoch 279/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6191 - accuracy: 0.4531\n",
      "Epoch 279: val_accuracy did not improve from 0.72655\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5717 - accuracy: 0.4664 - val_loss: 1.0428 - val_accuracy: 0.7206\n",
      "Epoch 280/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6324 - accuracy: 0.4531\n",
      "Epoch 280: val_accuracy improved from 0.72655 to 0.73054, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.6197 - accuracy: 0.4417 - val_loss: 1.0390 - val_accuracy: 0.7305\n",
      "Epoch 281/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7233 - accuracy: 0.3828\n",
      "Epoch 281: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5860 - accuracy: 0.4697 - val_loss: 1.0402 - val_accuracy: 0.7106\n",
      "Epoch 282/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5754 - accuracy: 0.4922\n",
      "Epoch 282: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5699 - accuracy: 0.4677 - val_loss: 1.0393 - val_accuracy: 0.7106\n",
      "Epoch 283/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6486 - accuracy: 0.4844\n",
      "Epoch 283: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5828 - accuracy: 0.4843 - val_loss: 1.0399 - val_accuracy: 0.7066\n",
      "Epoch 284/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6488 - accuracy: 0.4844\n",
      "Epoch 284: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5797 - accuracy: 0.4570 - val_loss: 1.0402 - val_accuracy: 0.7086\n",
      "Epoch 285/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5089 - accuracy: 0.5078\n",
      "Epoch 285: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5677 - accuracy: 0.4803 - val_loss: 1.0392 - val_accuracy: 0.7106\n",
      "Epoch 286/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6814 - accuracy: 0.4219\n",
      "Epoch 286: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5919 - accuracy: 0.4597 - val_loss: 1.0383 - val_accuracy: 0.7106\n",
      "Epoch 287/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3791 - accuracy: 0.5234\n",
      "Epoch 287: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5992 - accuracy: 0.4770 - val_loss: 1.0387 - val_accuracy: 0.7006\n",
      "Epoch 288/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4755 - accuracy: 0.5234\n",
      "Epoch 288: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5899 - accuracy: 0.4617 - val_loss: 1.0369 - val_accuracy: 0.7046\n",
      "Epoch 289/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3523 - accuracy: 0.5469\n",
      "Epoch 289: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5732 - accuracy: 0.4724 - val_loss: 1.0332 - val_accuracy: 0.7186\n",
      "Epoch 290/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6279 - accuracy: 0.4766\n",
      "Epoch 290: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5687 - accuracy: 0.4697 - val_loss: 1.0343 - val_accuracy: 0.7166\n",
      "Epoch 291/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6183 - accuracy: 0.4609\n",
      "Epoch 291: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5981 - accuracy: 0.4677 - val_loss: 1.0331 - val_accuracy: 0.7146\n",
      "Epoch 292/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6650 - accuracy: 0.4062\n",
      "Epoch 292: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6230 - accuracy: 0.4524 - val_loss: 1.0413 - val_accuracy: 0.7106\n",
      "Epoch 293/1000\n",
      " 8/12 [===================>..........] - ETA: 0s - loss: 1.5849 - accuracy: 0.5068\n",
      "Epoch 293: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.5810 - accuracy: 0.4903 - val_loss: 1.0416 - val_accuracy: 0.7226\n",
      "Epoch 294/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7399 - accuracy: 0.4375\n",
      "Epoch 294: val_accuracy did not improve from 0.73054\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6122 - accuracy: 0.4704 - val_loss: 1.0394 - val_accuracy: 0.7146\n",
      "Epoch 295/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6086 - accuracy: 0.4766\n",
      "Epoch 295: val_accuracy improved from 0.73054 to 0.73253, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6081 - accuracy: 0.4477 - val_loss: 1.0372 - val_accuracy: 0.7325\n",
      "Epoch 296/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6528 - accuracy: 0.4453\n",
      "Epoch 296: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5948 - accuracy: 0.4737 - val_loss: 1.0439 - val_accuracy: 0.7026\n",
      "Epoch 297/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6776 - accuracy: 0.3594\n",
      "Epoch 297: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5864 - accuracy: 0.4677 - val_loss: 1.0456 - val_accuracy: 0.7086\n",
      "Epoch 298/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6466 - accuracy: 0.3984\n",
      "Epoch 298: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5743 - accuracy: 0.4710 - val_loss: 1.0392 - val_accuracy: 0.7305\n",
      "Epoch 299/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.8042 - accuracy: 0.4922\n",
      "Epoch 299: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6051 - accuracy: 0.4684 - val_loss: 1.0291 - val_accuracy: 0.7166\n",
      "Epoch 300/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4757 - accuracy: 0.4609\n",
      "Epoch 300: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5886 - accuracy: 0.4730 - val_loss: 1.0268 - val_accuracy: 0.7146\n",
      "Epoch 301/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5860 - accuracy: 0.4375\n",
      "Epoch 301: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5631 - accuracy: 0.4657 - val_loss: 1.0298 - val_accuracy: 0.7046\n",
      "Epoch 302/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6932 - accuracy: 0.3984\n",
      "Epoch 302: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5834 - accuracy: 0.4517 - val_loss: 1.0369 - val_accuracy: 0.7265\n",
      "Epoch 303/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5557 - accuracy: 0.5156\n",
      "Epoch 303: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5770 - accuracy: 0.4797 - val_loss: 1.0317 - val_accuracy: 0.7226\n",
      "Epoch 304/1000\n",
      "10/12 [========================>.....] - ETA: 0s - loss: 1.5375 - accuracy: 0.4773\n",
      "Epoch 304: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 1.5658 - accuracy: 0.4704 - val_loss: 1.0275 - val_accuracy: 0.7166\n",
      "Epoch 305/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6525 - accuracy: 0.4688\n",
      "Epoch 305: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5796 - accuracy: 0.4750 - val_loss: 1.0319 - val_accuracy: 0.7126\n",
      "Epoch 306/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5961 - accuracy: 0.3906\n",
      "Epoch 306: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6094 - accuracy: 0.4477 - val_loss: 1.0296 - val_accuracy: 0.7265\n",
      "Epoch 307/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6312 - accuracy: 0.4453\n",
      "Epoch 307: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5990 - accuracy: 0.4637 - val_loss: 1.0300 - val_accuracy: 0.7285\n",
      "Epoch 308/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5470 - accuracy: 0.4922\n",
      "Epoch 308: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5993 - accuracy: 0.4870 - val_loss: 1.0309 - val_accuracy: 0.7246\n",
      "Epoch 309/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3282 - accuracy: 0.5078\n",
      "Epoch 309: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5778 - accuracy: 0.4684 - val_loss: 1.0328 - val_accuracy: 0.6986\n",
      "Epoch 310/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5622 - accuracy: 0.4688\n",
      "Epoch 310: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5857 - accuracy: 0.4670 - val_loss: 1.0317 - val_accuracy: 0.7006\n",
      "Epoch 311/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6234 - accuracy: 0.4766\n",
      "Epoch 311: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5555 - accuracy: 0.4883 - val_loss: 1.0286 - val_accuracy: 0.7146\n",
      "Epoch 312/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7480 - accuracy: 0.4062\n",
      "Epoch 312: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5793 - accuracy: 0.4704 - val_loss: 1.0254 - val_accuracy: 0.7305\n",
      "Epoch 313/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5182 - accuracy: 0.4844\n",
      "Epoch 313: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5950 - accuracy: 0.4524 - val_loss: 1.0268 - val_accuracy: 0.7086\n",
      "Epoch 314/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7700 - accuracy: 0.3984\n",
      "Epoch 314: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6118 - accuracy: 0.4744 - val_loss: 1.0275 - val_accuracy: 0.6986\n",
      "Epoch 315/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4447 - accuracy: 0.4531\n",
      "Epoch 315: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5646 - accuracy: 0.4710 - val_loss: 1.0266 - val_accuracy: 0.6886\n",
      "Epoch 316/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4187 - accuracy: 0.5156\n",
      "Epoch 316: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5619 - accuracy: 0.4744 - val_loss: 1.0282 - val_accuracy: 0.6986\n",
      "Epoch 317/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6700 - accuracy: 0.3828\n",
      "Epoch 317: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6473 - accuracy: 0.4470 - val_loss: 1.0297 - val_accuracy: 0.7166\n",
      "Epoch 318/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3481 - accuracy: 0.5391\n",
      "Epoch 318: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5527 - accuracy: 0.4777 - val_loss: 1.0243 - val_accuracy: 0.7106\n",
      "Epoch 319/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6566 - accuracy: 0.4141\n",
      "Epoch 319: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5722 - accuracy: 0.4697 - val_loss: 1.0237 - val_accuracy: 0.7186\n",
      "Epoch 320/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5231 - accuracy: 0.4922\n",
      "Epoch 320: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5913 - accuracy: 0.4790 - val_loss: 1.0254 - val_accuracy: 0.7066\n",
      "Epoch 321/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4041 - accuracy: 0.5625\n",
      "Epoch 321: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5868 - accuracy: 0.4857 - val_loss: 1.0265 - val_accuracy: 0.6926\n",
      "Epoch 322/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7357 - accuracy: 0.4609\n",
      "Epoch 322: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5727 - accuracy: 0.4744 - val_loss: 1.0218 - val_accuracy: 0.7066\n",
      "Epoch 323/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4957 - accuracy: 0.4688\n",
      "Epoch 323: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5547 - accuracy: 0.4730 - val_loss: 1.0178 - val_accuracy: 0.7146\n",
      "Epoch 324/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6886 - accuracy: 0.4141\n",
      "Epoch 324: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.6220 - accuracy: 0.4597 - val_loss: 1.0203 - val_accuracy: 0.7166\n",
      "Epoch 325/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5255 - accuracy: 0.5000\n",
      "Epoch 325: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5650 - accuracy: 0.4797 - val_loss: 1.0165 - val_accuracy: 0.7126\n",
      "Epoch 326/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5229 - accuracy: 0.5312\n",
      "Epoch 326: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5388 - accuracy: 0.4857 - val_loss: 1.0188 - val_accuracy: 0.7166\n",
      "Epoch 327/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7196 - accuracy: 0.4297\n",
      "Epoch 327: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5643 - accuracy: 0.4710 - val_loss: 1.0166 - val_accuracy: 0.7186\n",
      "Epoch 328/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6562 - accuracy: 0.4609\n",
      "Epoch 328: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5875 - accuracy: 0.4624 - val_loss: 1.0183 - val_accuracy: 0.7166\n",
      "Epoch 329/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7461 - accuracy: 0.4453\n",
      "Epoch 329: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5781 - accuracy: 0.4737 - val_loss: 1.0189 - val_accuracy: 0.7106\n",
      "Epoch 330/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5958 - accuracy: 0.4531\n",
      "Epoch 330: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5839 - accuracy: 0.4810 - val_loss: 1.0201 - val_accuracy: 0.7146\n",
      "Epoch 331/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5181 - accuracy: 0.4375\n",
      "Epoch 331: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5876 - accuracy: 0.4737 - val_loss: 1.0190 - val_accuracy: 0.7186\n",
      "Epoch 332/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4900 - accuracy: 0.4922\n",
      "Epoch 332: val_accuracy did not improve from 0.73253\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5740 - accuracy: 0.4597 - val_loss: 1.0203 - val_accuracy: 0.7186\n",
      "Epoch 333/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4658 - accuracy: 0.5000\n",
      "Epoch 333: val_accuracy improved from 0.73253 to 0.73852, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6014 - accuracy: 0.4717 - val_loss: 1.0155 - val_accuracy: 0.7385\n",
      "Epoch 334/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4932 - accuracy: 0.5391\n",
      "Epoch 334: val_accuracy did not improve from 0.73852\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6121 - accuracy: 0.4577 - val_loss: 1.0145 - val_accuracy: 0.7046\n",
      "Epoch 335/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5146 - accuracy: 0.4844\n",
      "Epoch 335: val_accuracy did not improve from 0.73852\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5736 - accuracy: 0.4917 - val_loss: 1.0156 - val_accuracy: 0.7106\n",
      "Epoch 336/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6363 - accuracy: 0.4844\n",
      "Epoch 336: val_accuracy did not improve from 0.73852\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5925 - accuracy: 0.4763 - val_loss: 1.0161 - val_accuracy: 0.7325\n",
      "Epoch 337/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4940 - accuracy: 0.5312\n",
      "Epoch 337: val_accuracy did not improve from 0.73852\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5519 - accuracy: 0.4757 - val_loss: 1.0162 - val_accuracy: 0.7305\n",
      "Epoch 338/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6625 - accuracy: 0.4062\n",
      "Epoch 338: val_accuracy improved from 0.73852 to 0.74052, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5523 - accuracy: 0.4664 - val_loss: 1.0139 - val_accuracy: 0.7405\n",
      "Epoch 339/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4867 - accuracy: 0.4766\n",
      "Epoch 339: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5566 - accuracy: 0.4777 - val_loss: 1.0125 - val_accuracy: 0.7285\n",
      "Epoch 340/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4747 - accuracy: 0.5078\n",
      "Epoch 340: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5924 - accuracy: 0.4857 - val_loss: 1.0104 - val_accuracy: 0.7146\n",
      "Epoch 341/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6178 - accuracy: 0.4531\n",
      "Epoch 341: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.5857 - accuracy: 0.4717 - val_loss: 1.0158 - val_accuracy: 0.7146\n",
      "Epoch 342/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6776 - accuracy: 0.4844\n",
      "Epoch 342: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5493 - accuracy: 0.4957 - val_loss: 1.0113 - val_accuracy: 0.7305\n",
      "Epoch 343/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5857 - accuracy: 0.4609\n",
      "Epoch 343: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5594 - accuracy: 0.4744 - val_loss: 1.0068 - val_accuracy: 0.7385\n",
      "Epoch 344/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5493 - accuracy: 0.5156\n",
      "Epoch 344: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.4992 - accuracy: 0.4977 - val_loss: 1.0094 - val_accuracy: 0.7305\n",
      "Epoch 345/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6709 - accuracy: 0.4688\n",
      "Epoch 345: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5928 - accuracy: 0.4624 - val_loss: 1.0142 - val_accuracy: 0.6986\n",
      "Epoch 346/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5599 - accuracy: 0.4609\n",
      "Epoch 346: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5721 - accuracy: 0.4770 - val_loss: 1.0104 - val_accuracy: 0.7146\n",
      "Epoch 347/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5072 - accuracy: 0.4844\n",
      "Epoch 347: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.5176 - accuracy: 0.4930 - val_loss: 1.0035 - val_accuracy: 0.7186\n",
      "Epoch 348/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5033 - accuracy: 0.4609\n",
      "Epoch 348: val_accuracy did not improve from 0.74052\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5954 - accuracy: 0.4704 - val_loss: 0.9983 - val_accuracy: 0.7405\n",
      "Epoch 349/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6582 - accuracy: 0.3750\n",
      "Epoch 349: val_accuracy improved from 0.74052 to 0.74251, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5442 - accuracy: 0.4704 - val_loss: 0.9985 - val_accuracy: 0.7425\n",
      "Epoch 350/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6757 - accuracy: 0.5078\n",
      "Epoch 350: val_accuracy improved from 0.74251 to 0.74850, saving model to ./model\\kp_classifier.hdf5\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5568 - accuracy: 0.4750 - val_loss: 1.0028 - val_accuracy: 0.7485\n",
      "Epoch 351/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4594 - accuracy: 0.4922\n",
      "Epoch 351: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.5786 - accuracy: 0.4697 - val_loss: 1.0044 - val_accuracy: 0.7265\n",
      "Epoch 352/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5307 - accuracy: 0.4922\n",
      "Epoch 352: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5257 - accuracy: 0.4917 - val_loss: 0.9997 - val_accuracy: 0.7345\n",
      "Epoch 353/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3342 - accuracy: 0.5859\n",
      "Epoch 353: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.6010 - accuracy: 0.4644 - val_loss: 1.0031 - val_accuracy: 0.7106\n",
      "Epoch 354/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3621 - accuracy: 0.5391\n",
      "Epoch 354: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5256 - accuracy: 0.4737 - val_loss: 1.0023 - val_accuracy: 0.7086\n",
      "Epoch 355/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6091 - accuracy: 0.4844\n",
      "Epoch 355: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5339 - accuracy: 0.4710 - val_loss: 0.9981 - val_accuracy: 0.7206\n",
      "Epoch 356/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5663 - accuracy: 0.4688\n",
      "Epoch 356: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5351 - accuracy: 0.4744 - val_loss: 1.0003 - val_accuracy: 0.7305\n",
      "Epoch 357/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5092 - accuracy: 0.4688\n",
      "Epoch 357: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5618 - accuracy: 0.4963 - val_loss: 1.0001 - val_accuracy: 0.7265\n",
      "Epoch 358/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7047 - accuracy: 0.4531\n",
      "Epoch 358: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5354 - accuracy: 0.4943 - val_loss: 1.0054 - val_accuracy: 0.7285\n",
      "Epoch 359/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6346 - accuracy: 0.4922\n",
      "Epoch 359: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.6027 - accuracy: 0.4630 - val_loss: 1.0029 - val_accuracy: 0.7405\n",
      "Epoch 360/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3282 - accuracy: 0.5156\n",
      "Epoch 360: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5811 - accuracy: 0.4617 - val_loss: 1.0044 - val_accuracy: 0.7365\n",
      "Epoch 361/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6633 - accuracy: 0.5078\n",
      "Epoch 361: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5707 - accuracy: 0.4724 - val_loss: 1.0044 - val_accuracy: 0.7106\n",
      "Epoch 362/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4679 - accuracy: 0.5078\n",
      "Epoch 362: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5652 - accuracy: 0.4610 - val_loss: 1.0019 - val_accuracy: 0.7026\n",
      "Epoch 363/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4722 - accuracy: 0.4844\n",
      "Epoch 363: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.5315 - accuracy: 0.4970 - val_loss: 0.9990 - val_accuracy: 0.7285\n",
      "Epoch 364/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5153 - accuracy: 0.5078\n",
      "Epoch 364: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5484 - accuracy: 0.4857 - val_loss: 0.9996 - val_accuracy: 0.7126\n",
      "Epoch 365/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5179 - accuracy: 0.4688\n",
      "Epoch 365: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5517 - accuracy: 0.4817 - val_loss: 0.9978 - val_accuracy: 0.7445\n",
      "Epoch 366/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6978 - accuracy: 0.4844\n",
      "Epoch 366: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5738 - accuracy: 0.4757 - val_loss: 0.9997 - val_accuracy: 0.7305\n",
      "Epoch 367/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7827 - accuracy: 0.4688\n",
      "Epoch 367: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5398 - accuracy: 0.4857 - val_loss: 0.9975 - val_accuracy: 0.7265\n",
      "Epoch 368/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5261 - accuracy: 0.5156\n",
      "Epoch 368: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5932 - accuracy: 0.4730 - val_loss: 0.9968 - val_accuracy: 0.7166\n",
      "Epoch 369/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5400 - accuracy: 0.4297\n",
      "Epoch 369: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5473 - accuracy: 0.4730 - val_loss: 1.0026 - val_accuracy: 0.7046\n",
      "Epoch 370/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6074 - accuracy: 0.4297\n",
      "Epoch 370: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5552 - accuracy: 0.4770 - val_loss: 0.9971 - val_accuracy: 0.7166\n",
      "Epoch 371/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5372 - accuracy: 0.4375\n",
      "Epoch 371: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5532 - accuracy: 0.4617 - val_loss: 0.9989 - val_accuracy: 0.7006\n",
      "Epoch 372/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7025 - accuracy: 0.4141\n",
      "Epoch 372: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5837 - accuracy: 0.4610 - val_loss: 0.9982 - val_accuracy: 0.6926\n",
      "Epoch 373/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6542 - accuracy: 0.4141\n",
      "Epoch 373: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5131 - accuracy: 0.4870 - val_loss: 0.9978 - val_accuracy: 0.7126\n",
      "Epoch 374/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6649 - accuracy: 0.4453\n",
      "Epoch 374: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5453 - accuracy: 0.4843 - val_loss: 0.9984 - val_accuracy: 0.7206\n",
      "Epoch 375/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4935 - accuracy: 0.5312\n",
      "Epoch 375: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5630 - accuracy: 0.4710 - val_loss: 0.9990 - val_accuracy: 0.7186\n",
      "Epoch 376/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6941 - accuracy: 0.4609\n",
      "Epoch 376: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5537 - accuracy: 0.4877 - val_loss: 0.9992 - val_accuracy: 0.7166\n",
      "Epoch 377/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6087 - accuracy: 0.4766\n",
      "Epoch 377: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5150 - accuracy: 0.4783 - val_loss: 1.0004 - val_accuracy: 0.7026\n",
      "Epoch 378/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6578 - accuracy: 0.3906\n",
      "Epoch 378: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5755 - accuracy: 0.4677 - val_loss: 0.9980 - val_accuracy: 0.7186\n",
      "Epoch 379/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6514 - accuracy: 0.4688\n",
      "Epoch 379: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5470 - accuracy: 0.4870 - val_loss: 1.0038 - val_accuracy: 0.7206\n",
      "Epoch 380/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4361 - accuracy: 0.5469\n",
      "Epoch 380: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5639 - accuracy: 0.4577 - val_loss: 1.0075 - val_accuracy: 0.7086\n",
      "Epoch 381/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6072 - accuracy: 0.4141\n",
      "Epoch 381: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5176 - accuracy: 0.5003 - val_loss: 1.0076 - val_accuracy: 0.6886\n",
      "Epoch 382/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4436 - accuracy: 0.4609\n",
      "Epoch 382: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5463 - accuracy: 0.4757 - val_loss: 1.0014 - val_accuracy: 0.7246\n",
      "Epoch 383/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6180 - accuracy: 0.5312\n",
      "Epoch 383: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5986 - accuracy: 0.4690 - val_loss: 0.9957 - val_accuracy: 0.7246\n",
      "Epoch 384/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5102 - accuracy: 0.5078\n",
      "Epoch 384: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5443 - accuracy: 0.4937 - val_loss: 1.0006 - val_accuracy: 0.7265\n",
      "Epoch 385/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6722 - accuracy: 0.5078\n",
      "Epoch 385: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5727 - accuracy: 0.4763 - val_loss: 1.0037 - val_accuracy: 0.7285\n",
      "Epoch 386/1000\n",
      "10/12 [========================>.....] - ETA: 0s - loss: 1.5731 - accuracy: 0.4906\n",
      "Epoch 386: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1.5434 - accuracy: 0.4923 - val_loss: 1.0061 - val_accuracy: 0.7146\n",
      "Epoch 387/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5735 - accuracy: 0.4766\n",
      "Epoch 387: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5557 - accuracy: 0.4857 - val_loss: 0.9961 - val_accuracy: 0.7166\n",
      "Epoch 388/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6717 - accuracy: 0.4609\n",
      "Epoch 388: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5435 - accuracy: 0.4930 - val_loss: 0.9925 - val_accuracy: 0.6886\n",
      "Epoch 389/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5371 - accuracy: 0.5000\n",
      "Epoch 389: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5159 - accuracy: 0.4757 - val_loss: 0.9910 - val_accuracy: 0.7226\n",
      "Epoch 390/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6171 - accuracy: 0.4609\n",
      "Epoch 390: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5617 - accuracy: 0.4757 - val_loss: 0.9862 - val_accuracy: 0.7425\n",
      "Epoch 391/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5251 - accuracy: 0.4688\n",
      "Epoch 391: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5766 - accuracy: 0.4610 - val_loss: 0.9869 - val_accuracy: 0.7146\n",
      "Epoch 392/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5762 - accuracy: 0.4844\n",
      "Epoch 392: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5310 - accuracy: 0.4823 - val_loss: 0.9837 - val_accuracy: 0.7226\n",
      "Epoch 393/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5090 - accuracy: 0.5000\n",
      "Epoch 393: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5661 - accuracy: 0.4870 - val_loss: 0.9856 - val_accuracy: 0.7226\n",
      "Epoch 394/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4174 - accuracy: 0.5078\n",
      "Epoch 394: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5481 - accuracy: 0.4803 - val_loss: 0.9835 - val_accuracy: 0.7345\n",
      "Epoch 395/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5269 - accuracy: 0.4609\n",
      "Epoch 395: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5232 - accuracy: 0.4930 - val_loss: 0.9832 - val_accuracy: 0.7465\n",
      "Epoch 396/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3786 - accuracy: 0.5078\n",
      "Epoch 396: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5472 - accuracy: 0.4830 - val_loss: 0.9832 - val_accuracy: 0.7206\n",
      "Epoch 397/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6123 - accuracy: 0.5156\n",
      "Epoch 397: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5459 - accuracy: 0.4910 - val_loss: 0.9849 - val_accuracy: 0.7086\n",
      "Epoch 398/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7742 - accuracy: 0.4766\n",
      "Epoch 398: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5536 - accuracy: 0.4877 - val_loss: 0.9904 - val_accuracy: 0.7126\n",
      "Epoch 399/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4597 - accuracy: 0.5625\n",
      "Epoch 399: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.5660 - accuracy: 0.4863 - val_loss: 0.9905 - val_accuracy: 0.7206\n",
      "Epoch 400/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6176 - accuracy: 0.4531\n",
      "Epoch 400: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5136 - accuracy: 0.4903 - val_loss: 0.9903 - val_accuracy: 0.7146\n",
      "Epoch 401/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4774 - accuracy: 0.4844\n",
      "Epoch 401: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5429 - accuracy: 0.4890 - val_loss: 0.9848 - val_accuracy: 0.7206\n",
      "Epoch 402/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5596 - accuracy: 0.4531\n",
      "Epoch 402: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5778 - accuracy: 0.4684 - val_loss: 0.9779 - val_accuracy: 0.7425\n",
      "Epoch 403/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4395 - accuracy: 0.5703\n",
      "Epoch 403: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5234 - accuracy: 0.4923 - val_loss: 0.9769 - val_accuracy: 0.7265\n",
      "Epoch 404/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4159 - accuracy: 0.5312\n",
      "Epoch 404: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5645 - accuracy: 0.4770 - val_loss: 0.9811 - val_accuracy: 0.7305\n",
      "Epoch 405/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5203 - accuracy: 0.4688\n",
      "Epoch 405: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5285 - accuracy: 0.4744 - val_loss: 0.9802 - val_accuracy: 0.7006\n",
      "Epoch 406/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5292 - accuracy: 0.5625\n",
      "Epoch 406: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5243 - accuracy: 0.5003 - val_loss: 0.9788 - val_accuracy: 0.7246\n",
      "Epoch 407/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6550 - accuracy: 0.3672\n",
      "Epoch 407: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 1.5220 - accuracy: 0.4790 - val_loss: 0.9752 - val_accuracy: 0.7465\n",
      "Epoch 408/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5830 - accuracy: 0.4531\n",
      "Epoch 408: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5497 - accuracy: 0.4657 - val_loss: 0.9725 - val_accuracy: 0.7305\n",
      "Epoch 409/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7816 - accuracy: 0.4219\n",
      "Epoch 409: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5051 - accuracy: 0.4943 - val_loss: 0.9788 - val_accuracy: 0.7246\n",
      "Epoch 410/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4345 - accuracy: 0.5312\n",
      "Epoch 410: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5491 - accuracy: 0.4890 - val_loss: 0.9834 - val_accuracy: 0.7146\n",
      "Epoch 411/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4616 - accuracy: 0.4531\n",
      "Epoch 411: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5475 - accuracy: 0.4584 - val_loss: 0.9822 - val_accuracy: 0.7046\n",
      "Epoch 412/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5572 - accuracy: 0.5000\n",
      "Epoch 412: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.6075 - accuracy: 0.4617 - val_loss: 0.9800 - val_accuracy: 0.7345\n",
      "Epoch 413/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7358 - accuracy: 0.4531\n",
      "Epoch 413: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5394 - accuracy: 0.4763 - val_loss: 0.9839 - val_accuracy: 0.7265\n",
      "Epoch 414/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6701 - accuracy: 0.4375\n",
      "Epoch 414: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5370 - accuracy: 0.4963 - val_loss: 0.9882 - val_accuracy: 0.6966\n",
      "Epoch 415/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4937 - accuracy: 0.5469\n",
      "Epoch 415: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 1.5261 - accuracy: 0.4997 - val_loss: 0.9869 - val_accuracy: 0.7166\n",
      "Epoch 416/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5344 - accuracy: 0.4297\n",
      "Epoch 416: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5635 - accuracy: 0.4570 - val_loss: 0.9818 - val_accuracy: 0.7305\n",
      "Epoch 417/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3938 - accuracy: 0.5391\n",
      "Epoch 417: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5293 - accuracy: 0.4704 - val_loss: 0.9867 - val_accuracy: 0.7106\n",
      "Epoch 418/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5311 - accuracy: 0.5078\n",
      "Epoch 418: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5487 - accuracy: 0.4790 - val_loss: 0.9924 - val_accuracy: 0.7066\n",
      "Epoch 419/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4019 - accuracy: 0.5781\n",
      "Epoch 419: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5491 - accuracy: 0.4790 - val_loss: 0.9824 - val_accuracy: 0.7186\n",
      "Epoch 420/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4532 - accuracy: 0.5078\n",
      "Epoch 420: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.5301 - accuracy: 0.4830 - val_loss: 0.9816 - val_accuracy: 0.7265\n",
      "Epoch 421/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4750 - accuracy: 0.4766\n",
      "Epoch 421: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5165 - accuracy: 0.4917 - val_loss: 0.9778 - val_accuracy: 0.7146\n",
      "Epoch 422/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.6049 - accuracy: 0.4375\n",
      "Epoch 422: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5465 - accuracy: 0.4783 - val_loss: 0.9755 - val_accuracy: 0.7206\n",
      "Epoch 423/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.2638 - accuracy: 0.6016\n",
      "Epoch 423: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5171 - accuracy: 0.4863 - val_loss: 0.9754 - val_accuracy: 0.7226\n",
      "Epoch 424/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4063 - accuracy: 0.5312\n",
      "Epoch 424: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5524 - accuracy: 0.4717 - val_loss: 0.9754 - val_accuracy: 0.7285\n",
      "Epoch 425/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.3642 - accuracy: 0.5625\n",
      "Epoch 425: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 1.5268 - accuracy: 0.4837 - val_loss: 0.9739 - val_accuracy: 0.7186\n",
      "Epoch 426/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.5175 - accuracy: 0.4453\n",
      "Epoch 426: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5271 - accuracy: 0.4697 - val_loss: 0.9744 - val_accuracy: 0.7305\n",
      "Epoch 427/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.4238 - accuracy: 0.5000\n",
      "Epoch 427: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.5486 - accuracy: 0.4897 - val_loss: 0.9762 - val_accuracy: 0.7166\n",
      "Epoch 428/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 1.7126 - accuracy: 0.3594\n",
      "Epoch 428: val_accuracy did not improve from 0.74850\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.5155 - accuracy: 0.4783 - val_loss: 0.9793 - val_accuracy: 0.7066\n",
      "Epoch 428: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x287584d5a50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9793 - accuracy: 0.7066\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50         7\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.58      0.70      0.64        10\n",
      "           3       0.90      1.00      0.95        19\n",
      "           4       0.50      0.21      0.29        24\n",
      "           5       0.95      0.91      0.93        23\n",
      "           6       0.37      0.50      0.43        20\n",
      "           7       0.46      0.69      0.55        16\n",
      "           8       1.00      1.00      1.00        17\n",
      "           9       0.88      0.56      0.68        25\n",
      "          10       0.80      0.57      0.67         7\n",
      "          11       0.71      0.80      0.75        15\n",
      "          12       0.64      0.50      0.56        14\n",
      "          13       0.93      1.00      0.96        13\n",
      "          14       0.67      1.00      0.80        10\n",
      "          15       0.62      0.90      0.73        20\n",
      "          16       1.00      0.94      0.97        16\n",
      "          17       0.92      1.00      0.96        12\n",
      "          18       0.48      1.00      0.65        10\n",
      "          19       1.00      0.86      0.92         7\n",
      "          20       0.95      1.00      0.97        18\n",
      "          21       1.00      1.00      1.00        19\n",
      "          22       1.00      0.29      0.44         7\n",
      "          23       1.00      0.40      0.57         5\n",
      "          24       0.50      0.15      0.24        13\n",
      "          25       0.94      1.00      0.97        16\n",
      "          26       0.83      0.83      0.83         6\n",
      "          27       1.00      0.89      0.94        18\n",
      "          28       0.00      0.00      0.00         8\n",
      "          29       0.00      0.00      0.00         2\n",
      "          30       0.78      0.39      0.52        18\n",
      "          31       0.50      0.29      0.36        14\n",
      "          32       0.40      0.77      0.53        13\n",
      "          33       0.92      0.92      0.92        12\n",
      "          34       0.94      0.94      0.94        18\n",
      "          35       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           0.75       501\n",
      "   macro avg       0.74      0.72      0.70       501\n",
      "weighted avg       0.77      0.75      0.73       501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\BINUS\\Assignments\\Semester 3\\AI\\SignoAI\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(x_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
